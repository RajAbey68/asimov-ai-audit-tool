[
    {
        "control_name": "Anomaly Detection Techniques",
        "category": "Defensive Model Strengthening",
        "description": "Develop and implement anomaly detection techniques to detect unusual or unexpected patterns, behaviors, or datapoints in the AI system. Document and analyze the anomaly detection results.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15 - Accuracy, robustness and cybersecurity\nRecital 53: Emphasizes the AI System ability to detect decision making pattern deviations., NIST 800-53: SI-3 Malicious Code Protection\nSI-4 System Monitoring\nRA-5 Vulnerability Monitoring and Scanning\nSI-7 Software, Firmware, and Information Integrity\nIR-4 Incident Handling, SCF: PRI-10.2: Data Analytics Bias\nIAO-04: Threat Analysis & Flaw Remediation During Development\nSEA-05: Information In Shared Resources\nSEA-09: Information Output Filtering\nSAT-03.2: Suspicious Communications & Anomalous System Behavior\nOPS-04: Security Operations Center (SOC)\nTDA-06.2: Threat Modeling\nTDA-09.4: Malformed Input Testing\nTHR-01: Threat Intelligence Program\nTHR-07: Threat Hunting\nVPM-06: Vulnerability Scanning",
        "explainability": "Anomaly Detection Techniques are implemented in an AI system to identify unusual or unexpected patterns, behaviors, or data points. The rationale for this control is to ensure that the AI system can make informed decisions by detecting and flagging anomalies, which may be indicative of errors or malicious activity. This explanation should be delivered in a nontechnical and accessible way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document that provides a high-level overview of the need for anomaly detection techniques in the AI system and their role in ensuring data integrity and decision-making. facilitating decision making.\n\nVisual Representation: Diagrams or infographics that illustrate how anomaly detection works in the context of the AI system, making it easier for nontechnical stakeholders to understand. | Responsibility Explanation: Anomaly Detection Team Roster: A document listing all team members involved with the anomaly detection process, their roles, and contact information.\n\nAnomaly Response Protocols: Formal documentation of the procedures for when anomalies are detected, including the escalation process and responsible parties. \n\nTraining Data Audit Trails: Detailed logs that track all actions taken on training data, linking each action to a responsible party.\n\nFalse Positive Identification: Review the number of false positives identified by the system  to help determine the need for robust anomaly techniques.\n\nAnomaly Technique Review: Review of the anomaly techniques at regular intervals to ensure that they are up to date and no changes are required based on the technological advancement.  | Data Explanation: Anomaly Detection Reports: Document outlining the anomalies detected and potential data quality issues.\n\nData Quality Assessments: Evaluations of data quality before and after cleaning.\n\nData Audit Logs: Chronological records of the data examined and the anomalies found.\n\nDocumentation of Remediation Steps: Descriptions of the procedures taken to address the identified anomalies.\n\nUpdated Datasets: The final, cleaned datasets used for model training after anomaly correction. | Fairness Explanation: Comprehensive Anomaly Assessment Report: Documents the nature of each anomaly, potential for bias, corrective measures, and changes in decision patterns. \n\nAnomaly Resolution Log: Captures the decision trail from detection to resolution of biases. \n\nEquity Impact Statements: Summarizes how the correction of anomalies contributes to equitable outcomes. | Safety & Performance  Explanation: Anomaly Detection Reports: Detailed documentation of detected anomalies, the potential impact on AI performance, and remediation actions taken. \n\nData Quality Assessment: A thorough report evaluating the quality of training data before and after anomaly detection interventions. \n\nModel Performance Records: Pre- and post-deployment performance metrics of the AI model demonstrating the efficacy of anomaly detection controls.\n | Impact Explanation: Anomaly Impact Report: A comprehensive document outlining potential and actual impacts identified through anomaly detection, including case studies where interventions prevented harm.\n\nAlgorithmic Impact Assessments (AIA): Assessments that evaluate the potential impacts of anomalies on individuals and society, often required by regulations or industry standards.\n\nData Quality and Integrity Framework: A framework outlining the procedures and standards for maintaining data quality, mitigating the impact of anomalies on the AI's decision-making process."
    },
    {
        "control_name": "Adversarial Training for Model Robustness",
        "category": "Defensive Model Strengthening",
        "description": "Implement regular adversarial training to enhance and review model robustness to ensure the performance of the AI system against adversarial attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15 (4-5) - Accuracy, robustness and cybersecurity\nArticle   55 - Obligations for provides of general purpose AI models with systemic risk, NIST 800-53: SA-11: Developer Testing and Evaluation\nSC-31: Covert Channel Analysis\nSI-3: Malicious Code Protection\nRA-3: Risk Assessment, SCF: SEA-01: Secure Engineering Principles\nSEA-03: Defense-In-Depth (DiD) Architecture\nSEA-07: Predictable Failure Analysis\nSAT-02: Cybersecurity & Data Privacy Awareness Training\nSAT-03: Role-Based Cybersecurity & Data Privacy Training\nOPS-01: Operations Security\nTDA-06: Secure Coding\nTDA-06.2: Threat Modeling\nTDA-06.3: Software Assurance Maturity Model (SAMM)\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development\nTHR-01: Threat Intelligence Program\nTHR-07: Threat Hunting",
        "explainability": "Adversarial Training for Model Robustness involves the process of training AI models to withstand and perform well against adversarial attacks, which are attempts to manipulate or deceive the AI system. The rationale for this control is to enhance the AI system's security and robustness by preparing it to handle malicious input effectively and learn from mistakes (e.g., false-positive). This explanation should be delivered in an accessible and nontechnical way for stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of adversarial training in the context of model robustness and security. It should detail the potential risk and benefits. \n\nModel Robustness Plan: A document outlining the strategies and steps taken to implement adversarial training in the AI system. \n\nCase Studies: Real-world examples or case studies illustrating the impact of adversarial training on model performance and security. | Responsibility Explanation: Adversarial Training Assignment Document: A detailed outline of all parties involved in the adversarial training process, their specific roles and responsibilities, and how they contribute to model robustness. \n\nAdversarial Training Procedures Manual: A comprehensive manual that describes the step-by-step procedures for conducting adversarial training, including protocols for documenting iterations and changes to the model. \n\nModel Robustness Accountability Log: Logs that record each action taken to improve model robustness, clearly indicating which individual or team was responsible.\n | Data Explanation: Adversarial Training Dataset Specification: A document detailing the characteristics of the adversarial examples added to the training dataset, including the techniques used to generate them (e.g., FGSM, PGD).\n\nModel Training Records: Logs and reports that record the process of model training with adversarial data, including parameters adjusted and the impact on model performance.\n\nData Source and Provenance Information: Documentation providing insight into the sources of both original and adversarial training data, including any transformations or augmentations applied. | Fairness Explanation: Adversarial Training Fairness Report: Documents the adversarial training process with a focus on maintaining decision fairness. \n\nFair Robustness Certification: A certification that the model's robustness contributes to fair outcomes. \n\nTraining Bias Impact Log: Records any biases identified during adversarial training and the steps taken to mitigate them. | Safety & Performance  Explanation: Adversarial Training Protocols: Step-by-step procedures and guidelines used for generating adversarial examples and retraining the AI model. \n\nRobustness Evaluation Reports: Analysis of model performance against adversarial examples before and after the training. \n\nAttack Vector Analysis: Documentation of potential attack scenarios and how the model's robustness has been enhanced to mitigate them. | Impact Explanation: Model Robustness Report: A document that outlines the improvements in model robustness as a result of adversarial training, detailing the types of attacks the model is now better equipped to handle.\n\nTrust and Reliability Index: A metric or set of metrics developed to quantify the trustworthiness and reliability of AI decisions after adversarial training, potentially including user trust surveys.\n\nRobustness-Against-Adversaries Framework: A framework that defines the standards and processes for implementing adversarial training and measuring its impact on model robustness."
    },
    {
        "control_name": "Model Ensembles for Reduced Impact of Attacks",
        "category": "Defensive Model Strengthening",
        "description": "Develop, implement, train, and test multiple AI models and use an ensemble to reduce the impact of data poisoning attacks and adversarial inputs. Document the development, implementation, training, and test results to enhance and understand the AI systems' security and resilience.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 67: Emphasizes high-quality data free of errors to the extent possible., NIST 800-53: CP-2: Contingency Plan\nSA-10: Developer Configuration Management\nSI-3: Malicious Code Protection\nSI-7: Software, Firmware, and Information Integrity, SCF: IAC-13: Adaptive Identification & Authentication - Ensuring AI models used in authentication systems are resilient to data poisoning.\nRSK-03: Risk Identification - Identifying risk such as data poisoning in AI systems and mitigating them through ensemble approaches.\nRSK-04: Risk Assessment - Assessing the risk of data poisoning in AI models and the effectiveness of ensemble methods as a mitigation strategy.",
        "explainability": "Model Ensembles for Reduced Impact of Attacks involve using multiple AI models in combination to make decisions, reducing the impact of potential attacks or adversarial inputs. The rationale for this control is to enhance the AI system's security and resilience by leveraging diversity in decision making. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of model ensembles for security and the reduction of impact from attacks. It should outline the benefits and potential risk. \n\nModel Ensemble Strategy: A document detailing the strategy and approach used to implement model ensembles in the AI system. \n\nPerformance Metrics: Reports or data showing the improved performance and resilience achieved through model ensembles. | Responsibility Explanation: Ensemble Strategy Documentation: Details of the ensemble approach, individuals involved in the model training, and their specific roles. \n\nIntegration and Oversight Procedures: Processes for integrating multiple models and monitoring their performance. \n\nIncident Response Plan: A plan that outlines steps and identifies individuals responsible for responding to detected attacks. | Data Explanation: Ensemble Strategy Report: A comprehensive document explaining the rationale for selecting specific models for the ensemble, the datasets used for training each model, and the methodology for integrating their outputs.\n\nTraining Data Catalog: A detailed inventory of datasets used for each model, including data schemas, sources, and any preprocessing steps taken.\n\nModel Integration Framework: Technical documentation outlining the process and logic for how the individual model outputs are combined to make a final decision. | Fairness Explanation: Ensemble Fairness Overview Document: Explains the ensemble approach's contribution to fair decision making.\n\nData Poisoning and Fairness Response Strategy: Strategy documentation that links data poisoning detection with fairness maintenance. \n\nEnsemble Decision Equity Report: Evaluates the fairness of decisions made by the ensemble models. | Safety & Performance  Explanation: Ensemble Strategy Documentation: A comprehensive plan detailing the selection and combination of diverse models into an ensemble. \n\nPoisoning Attack Impact Reports: Assessments comparing the impact of data poisoning on single models versus the ensemble. \n\nEnsemble Performance Records: Records tracking the decision accuracy and reliability of the ensemble approach under various scenarios. | Impact Explanation: Comprehensive document detailing the ensemble approach's effectiveness in countering data poisoning, including case studies and statistical evidence of improved decision integrity.\n\nData Poisoning Resistance Framework: A set of guidelines and standards that outline best practices for training and maintaining model ensembles to withstand data poisoning attacks.\n\nDecision Stability Metrics: Quantitative measures developed to assess the stability and reliability of decisions made by the ensemble of models under various conditions, including attack scenarios."
    },
    {
        "control_name": "Prevent Electoral Influence",
        "category": "AI Model Poisoning Defense",
        "description": "Develop and implement safeguards to prevent AI systems from unduly influencing electoral processes, ensuring the integrity of democratic systems.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 114: Emphasizes provider need to perform model evaluations., NIST 800-53: AC-6: Least Privilege\nPL-2: System Security and Privacy Plans \nPM-11: Mission and Business Process Definition\nRA-5: Vulnerability Monitoring and Scanning, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI systems used in electoral processes are trustworthy and free from manipulative influences.\nAAT-06: AI & Autonomous Technologies Fairness & Bias - Controls to ensure AI systems are free from bias that could influence electoral processes.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of AI systems for compliance with data privacy and security, crucial for electoral integrity.\nGOV-01: Cybersecurity & Data Protection Governance Program - Governance programs that include guidelines on the ethical use of AI in electoral processes.\nGOV-14: Business As Usual (BAU) Secure Practices - Regular secure practices in AI operations to prevent misuse in sensitive areas like elections.\nPRI-03: Choice & Consent - Ensuring that AI systems respect the principles of individual choice and consent, critical in electoral contexts.\nRSK-03: Risk Identification - Identifying risk associated with AI influencing elections and implementing controls to mitigate these risk.\nRSK-04: Risk Assessment - Assessing the potential risk AI systems pose to the integrity of electoral processes.",
        "explainability": "Prevent Electoral Influence is a control aimed at ensuring that AI systems, especially those used in the electoral process, do not have undue influence on election outcomes or voters. The rationale for this control is to safeguard the integrity of elections and maintain public trust in the electoral process by preventing AI systems from unduly manipulating or biasing voter decisions. This explanation should be delivered in an accessible and nontechnical way for a wide range of stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the significance of preventing electoral influence by AI systems in elections. It should outline the potential risk and the importance of maintaining election integrity. \n\nElection Integrity Plan: A document detailing the strategies and measures in place to prevent electoral influence through AI systems. \n\nPublic Awareness Campaigns: Information campaigns, brochures, or videos to inform the public about the measures in place to prevent AI-related electoral influence. | Responsibility Explanation: Electoral Integrity Assurance Policy: A document that lays out the standards and practices to prevent AI influence on elections, including roles and responsibilities. \n\nCompliance Monitoring Logs: Records of monitoring activities that ensure adherence to the policy. \n\nAudit Reports: Periodic reports from internal or external audits that review compliance with electoral integrity guidelines. | Data Explanation: Documented policies and procedures for data management and use in AI systems that are specifically designed to prevent electoral influence. This includes details on data sourcing, data handling, data processing, and data storage. The deliverables should also cover protocols for data quality assurance, data privacy, and data security specific to the electoral context. | Fairness Explanation: Electoral Fairness Assurance Protocol: A comprehensive plan to prevent AI from influencing electoral outcomes. \n\nDemocracy Protection Impact Assessment: An assessment of the AI's impact on democratic processes. \n\nElectoral Integrity and Fairness Report: A report that documents efforts and measures taken to safeguard electoral fairness. | Safety & Performance  Explanation: Electoral Integrity Policies: Detailed policies outlining the use of AI in the context of electoral processes and the measures to prevent undue influence. \n\nAI Ethics Compliance Certificates: Documents certifying the AI system has been audited for compliance with electoral integrity standards. \n\nTransparency Logs: Detailed logs showing AI activities related to electoral processes, including decision-making pathways and content dissemination records. | Impact Explanation: Electoral Influence Prevention Policy: A document that outlines the policies and criteria for AI systems to ensure they do not interfere with electoral processes, including transparency logs and algorithmic auditing procedures.\n\nAI Ethics Charter: A declaration of ethical principles guiding the use of AI in the context of elections, committing to neutrality and noninterference.\n\nElection Integrity Impact Reports: Periodic reports evaluating the effectiveness of AI safeguards in protecting electoral integrity, including case studies and statistical analyses of AI behavior during election periods."
    },
    {
        "control_name": "Adversarial Example Detection",
        "category": "Adversarial Attack Mitigation Techniques",
        "description": "Develop and implement measures to ensure that AI systems models can detect and handle adversarial examples to avoid being misled.",
        "risk_level": "General",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behavior that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behavior, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-3: Malicious Code Protection\nSI-7: Software, Firmware, and Information Integrity\nRA-5: Vulnerability Monitoring and Scanning, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models remain trustworthy and effective in the presence of adversarial examples.\nRSK-03: Risk Identification - Identifying risk such as adversarial attacks in AI systems and implementing controls to mitigate these risk.\nRSK-04: Risk Assessment - Assessing the potential impact of adversarial examples on AI models and the effectiveness of detection mechanisms.\nOPS-04: Security Operations Center (SOC) Capabilities - Enhancing SOC capabilities to monitor and respond to adversarial attacks on AI systems.",
        "explainability": "Adversarial Example Detection is a control designed to identify and mitigate the impact of adversarial examples on AI systems. The rationale for this control is to enhance the AI system's security and reliability by detecting and responding to inputs that are crafted to deceive the model. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of adversarial example detection and its role in maintaining the security and robustness of AI systems. \n\nDetection Algorithm Overview: A nontechnical summary of the detection algorithms and techniques used in the AI system. \n\nDetection Performance Metrics: Reports on the effectiveness of adversarial example detection in real-world scenarios. | Responsibility Explanation: Adversarial Detection Framework: A document outlining the strategy and methodologies for adversarial example detection, including responsible parties. \n\nDetection Protocol Manuals: Step-by-step guides for operational procedures in detecting adversarial inputs. \n\nResponse Team Contact List: A list of individuals and teams to contact when adversarial examples are detected. | Data Explanation: Comprehensive documentation of adversarial detection techniques used in the machine learning model. This should include a clear explanation of the methods employed to identify potential adversarial inputs, the training data used, and the steps taken to update the model in response to detected adversarial examples. The deliverables must also include a version history of model updates related to adversarial example handling. | Fairness Explanation: Fairness-Driven Adversarial Detection Report: Documentation on how adversarial detection preserves fairness in AI decisions. \n\nAdversarial Impact on Fairness Logs: Logs detailing the detection of adversarial examples and their potential impact on fair outcomes. \n\nDetection Method Fairness Verification: Verification that the detection methods do not introduce bias themselves. | Safety & Performance  Explanation: Adversarial Detection Framework: A documented approach outlining the processes for identifying adversarial examples. \n\nDetection System Efficacy Reports: Detailed analysis of the detection system's success rates, challenges faced, and areas for improvement. \n\nIncident Response Protocols: Guidelines and procedures for responding to detected adversarial examples, including mitigation strategies. | Impact Explanation: Adversarial Detection Framework Documentation: Detailed documentation on the adversarial example detection methods implemented, their effectiveness, and guidelines on how they should be updated or refined.\n\nAdversarial Attack Case Studies: A collection of case studies detailing past adversarial attacks, the detection process, and the responses to these attacks.\n\nModel Robustness Reports: Reports that analyze the resilience of machine learning models to adversarial examples, including performance metrics and improvement logs."
    },
    {
        "control_name": "Adversarial Training Implementation",
        "category": "Adversarial Attack Mitigation Techniques",
        "description": "Develop, implement, train, and document the incorporation of adversarial training methods to train the AI models to recognize and mitigate adversarial inputs effectively and be resilient against malicious inputs.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15 – paragraph 1: High-risk AI systems shall be designed and developed following the principle of security by design and by default.\nArticle 15 – paragraph 4: AI systems must be resilient against errors, faults, or inconsistencies., NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-3: Malicious Code Protection\nRA-5: Vulnerability Monitoring and Scanning\nSI-7: Software, Firmware, and Information Integrity, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models trained with adversarial methods remain trustworthy and effective.\nRSK-03: Risk Identification - Identifying risk in AI systems that adversarial training can mitigate.\nRSK-04: Risk Assessment - Assessing risk that adversarial examples pose to AI systems and the effectiveness of adversarial training as a mitigation strategy.\nOPS-04: Security Operations Center (SOC) Capabilities - Utilizing SOC capabilities to monitor and respond to threats against AI systems that adversarial training can address.",
        "explainability": "Adversarial Training Implementation is the process of incorporating adversarial training techniques into an AI system to enhance its resilience against adversarial attacks. The rationale for this control is to improve the AI system's security and robustness by training it to recognize and mitigate adversarial inputs effectively. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of adversarial training implementation in bolstering AI system security and the reduction of vulnerabilities to adversarial attacks. \n\nImplementation Plan: A document outlining the steps, tools, and methods used for incorporating adversarial training into the AI system. \n\nSecurity Performance Metrics: Reports (e.g., KPIs, trends) showing the improved security and robustness achieved through adversarial training. | Responsibility Explanation: Adversarial Training Plan: Comprehensive planning document listing personnel responsible for each stage of adversarial training. \n\nTraining Execution Records: Detailed records of the adversarial training sessions, including methodologies used and individuals involved. \n\nModel Resilience Report: A report documenting the model's performance before and after adversarial training, with insights from the responsible team. | Data Explanation: A set of adversarial training protocols and guidelines, along with records of training sessions that detail the adversarial examples used, the adjustments made to the model, and the training outcomes. This may include technical reports, model training logs, and updated risk assessment documentation reflecting the changes made through adversarial training. | Fairness Explanation: Adversarial Training Fairness Implementation Report: A report on implementing adversarial training with an emphasis on fairness. \n\nBias Mitigation and Training Logs: Detailed records of adversarial training sessions and actions taken to mitigate any induced biases. \n\nFairness-Centered Training Certification: A certification that attests to the fairness of the model after adversarial training has been implemented. | Safety & Performance  Explanation: Adversarial Training Integration Plan: A comprehensive document that outlines the approach for incorporating adversarial training into the model's development life cycle.\n\nModel Robustness Reports: Reports documenting the model's resistance to attacks pre- and post-adversarial training implementation. \n\nTraining Session Logs: Records of each adversarial training session, including data used, adversarial examples introduced, and model adjustments made. | Impact Explanation: Adversarial Training Process Documentation: A comprehensive guide that documents the adversarial training methodology, protocols followed, and the rationale behind the chosen methods.\n\nTraining Efficacy Reports: Detailed analysis and reports demonstrating the pre-\nand post-implementation effectiveness of adversarial training on model performance.\n\nChange Management Logs: Records of when and how adversarial training methods were updated, with associated impact analysis."
    },
    {
        "control_name": "Training Data Poisoning Detection",
        "category": "AI Model Poisoning Defense",
        "description": "Develop and implement regular checks to detect tampering or poisoning within the AI systems' model.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10 - Data and data governance\nArticle 15 - Accuracy, robustness and cybersecurity\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: SI-7: Software, Firmware, and Information Integrity\nRA-5: Vulnerability Monitoring and Scanning\nSI-3: Malicious Code Protection\nCM-8: System Component Inventory, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models, including their training datasets, are free from manipulation.\nRSK-03: Risk Identification - Identifying risk of data tampering or poisoning in AI training datasets and implementing controls.\nRSK-04: Risk Assessment - Assessing the impact and likelihood of data poisoning in AI training datasets and the efficacy of detection methods.\nOPS-04: Security Operations Center (SOC) Capabilities - Leveraging SOC capabilities to monitor and respond to threats against AI training datasets.",
        "explainability": "Training Data Poisoning Detection is a control aimed at identifying and mitigating the presence of poisoned or malicious data in the training dataset used to develop AI models. The rationale for this control is to ensure the integrity and trustworthiness of AI models by preventing them from being trained on tainted data that could lead to biased or harmful outcomes. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of training data poisoning detection and its role in maintaining the quality and fairness of AI models. \n\nDetection Algorithm Overview: A nontechnical summary of the detection algorithms and techniques used to identify poisoned data. \n\nData Integrity Metrics: Reports showing the effectiveness of data poisoning detection in preserving model integrity. | Responsibility Explanation: Poisoning Detection Protocol: A detailed description of the procedures and responsible parties for identifying data poisoning. \n\nTraining Data Custodian Logs: Logs that show who accessed and modified training data, with timestamps and actions taken. \n\nData Tampering Incident Reports: Documentation of any incidents of data poisoning, including how they were discovered and addressed. | Data Explanation: Protocols for data integrity checks, including checksums, data provenance documentation, and data validation reports. Deliverables should include a comprehensive log of data integrity assessments and a breakdown of any incidents detected, with details on the nature of the tampering or poisoning attempt. | Fairness Explanation: Fairness Impact of Poisoning Detection Report: Describes how detecting and correcting data poisoning supports fairness in model training. \n\nAntipoisoning Training Data Fairness Logs: Logs that track instances of data poisoning and remediation efforts with a focus on fairness. \n\nData Poisoning Fairness Impact Certification: Certification that the model's training data is both poison-free and fair. | Safety & Performance  Explanation: Data Poisoning Detection Strategy: A documented strategy that describes the methods for monitoring and detecting data poisoning attempts. \n\nData Integrity Reports: Regular reports that provide insights into the health and integrity of the training data, noting any suspected poisoning instances. \n\nAudit Trails for Data Handling: Logged details of all data handling processes, which can be used to trace the source and nature of any tampering. | Impact Explanation: Data Integrity Audit Reports: Detailed audit trails that record checks for anomalies or inconsistencies in data, indicating potential poisoning.\n\nPoisoning Incident Response Plans: Prepared protocols that outline the steps to be taken when data poisoning is detected, including impact mitigation strategies.\n\nModel Decision Pathway Analysis: Analytical reports that trace how poisoned data could influence model decisions and the associated risk."
    },
    {
        "control_name": "Backdoor Detection Mechanism",
        "category": "AI Model Poisoning Defense",
        "description": "Implement regular checks to detect potential backdoors, triggers, or hidden features that might have been inserted maliciously into the models.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nRecital 133: Emphasizes the need to embed technical solution to reduce risk of manipulation, fraud, impersonation and consumer deception., NIST 800-53: SI-7: Software, Firmware, and Information Integrity\nRA-5: Vulnerability Monitoring and Scanning\nSI-3: Malicious Code Protection\nSA-11: Developer Testing and Evaluation, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Maintaining the trustworthiness of AI models by ensuring they are free from malicious backdoors.\nRSK-03: Risk Identification - Identifying risk related to backdoor insertions in AI models and planning controls to mitigate them.\nRSK-04: Risk Assessment - Assessing the likelihood and potential impact of backdoors in AI models and the effectiveness of detection methods.\nOPS-04: Security Operations Center (SOC) Capabilities - Utilizing SOC capabilities to monitor for and respond to backdoors in AI systems.",
        "explainability": "Backdoor Detection Mechanism is a control designed to identify and prevent the presence of backdoors or hidden vulnerabilities in AI systems. The rationale for this control is to enhance the security and trustworthiness of AI systems by ensuring that they are not susceptible to unauthorized access or malicious manipulation through hidden entry points. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of backdoor detection mechanisms and their role in securing AI systems against unauthorized access or tampering. \n\nDetection Algorithm Overview: A nontechnical summary of the detection algorithms and techniques used to identify backdoors. \n\nSecurity Assurance Metrics: Reports showing the effectiveness of backdoor detection mechanisms in maintaining system security. | Responsibility Explanation: Backdoor Detection Protocol: A document outlining the strategies and assigned roles for detecting backdoors. \n\nSecurity Team Roster: A list of security personnel with responsibilities for each phase of backdoor detection. \n\nIncident Handling Procedures: Detailed response procedures for when potential backdoors are detected, including escalation paths. | Data Explanation: Development of a Backdoor Detection Protocol document that includes the methodology for scanning and identifying potential backdoors. This should also encompass the incident response plan detailing the steps to be taken upon detection of a backdoor. In addition, version-controlled records of all detection activities and findings should be maintained, as well as security assessment reports. | Fairness Explanation: Fairness-Oriented Backdoor Detection Report: Discusses the detection of backdoors and their potential impact on fairness. \n\nTrigger Response Analysis: Analyzes responses to detected triggers with an eye on fairness outcomes. \n\nFairness Assurance Statement: A declaration confirming the model's continued fairness despite backdoor threats. | Safety & Performance  Explanation: Backdoor Detection Protocols: Detailed documentation on procedures and tools used for identifying potential backdoors within models. \n\nModel Security Assessments: Comprehensive analyses of model architectures and training processes for vulnerabilities, along with any identified threats and remediation actions taken. \n\nSecurity Incident Logs: Chronological records of all security checks performed, any backdoors discovered, and the steps taken to address them. | Impact Explanation: Security Audit Reports: Comprehensive reviews detailing the findings of backdoor vulnerability assessments, outlining the potential impact if exploited.\n\nIncident Response Documentation: Detailed action plans and protocols outlining responses to the detection of a backdoor, including impact minimization strategies.\n\nBackdoor Mitigation Strategies: Strategic plans documenting approaches for backdoor isolation and system restoration to ensure continuity and integrity of the AI service."
    },
    {
        "control_name": "Model Tampering Detection",
        "category": "AI Model Poisoning Defense",
        "description": "Develop and implement Model Tampering Detection mechanisms to detect if a model's parameters and behavior have been tampered with.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SI-7: Software, Firmware, and Information Integrity\nCM-6: Configuration Settings\nRA-5: Vulnerability Monitoring and Scanning\nSI-3: Malicious Code Protection, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models remain trustworthy by verifying their parameters have not been tampered with.\nRSK-03: Risk Identification - Identifying risk associated with tampering of AI model parameters and implementing appropriate controls.\nRSK-04: Risk Assessment - Assessing the risk posed by potential tampering of AI model parameters and the effectiveness of detection controls.\nOPS-04: Security Operations Center (SOC) Capabilities - Utilizing SOC capabilities to monitor and respond to tampering incidents involving AI model parameters.",
        "explainability": "Model Tampering Detection is a control aimed at identifying and mitigating attempts to tamper with the AI model, its parameters, or its behavior. The rationale for this control is to enhance the security and integrity of AI systems by ensuring that they are protected against unauthorized modifications that could lead to harmful or undesirable outcomes. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of model tampering detection and its role in safeguarding the AI system's integrity and trustworthiness. \n\nDetection Algorithm Overview: A nontechnical summary of the detection algorithms and techniques used to identify model tampering. \nModel Integrity Metrics: Reports showing the effectiveness of model tampering detection in maintaining model integrity. | Responsibility Explanation: Tampering Detection Framework: Documentation of the processes and methodologies for tampering detection, and the roles responsible for each. \n\nModel Integrity Logs: Records that track changes to the model's parameters over time. \n\nTampering Incident Reports: Formal reports that document any instances of model tampering, how they were detected, and the response. | Data Explanation: Detailed Model Tampering Detection Guidelines, including the specific parameters to monitor and the indicators of tampering. Maintain a log of parameter checks and any tampering incidents, with a clear version history of the model's parameter sets. Additionally, ensure the availability of Model Parameter Audit Reports. | Fairness Explanation: Tampering Detection and Fairness Report: Documents how tampering could affect fairness and the steps taken to prevent this. \n\nParameter Change Fairness Log: Records all parameter changes and assesses their impact on fair outcomes. \n\nIntegrity and Fairness Audit Certificates: Certifies that after tampering detection, the model continues to make fair decisions. | Safety & Performance  Explanation: Tamper Detection Framework: A document outlining the processes and methodologies employed to monitor model parameters for tampering. \n\nIntegrity Monitoring Reports: Periodic reports detailing the status of the model parameters and any detected changes or alerts. \n\nChange Management Logs: Records of all authorized changes to the model, which can be crossreferenced with detected alterations to identify unauthorized tampering. | Impact Explanation: Model Integrity Reports: Detailed documents that outline the state of the model’s parameters at given checkpoints, highlighting any detected discrepancies or tampering attempts.\n\nTampering Incident Logs: Records of all detected tampering incidents, including the nature of the tampering, the suspected entry points, and the potential impact.\n\nModel Recovery Protocols: Guided procedures for restoring the model to a verified state in the event of tampering, including impact mitigation measures."
    },
    {
        "control_name": "Evasion Attack Prevention",
        "category": "Adversarial Attack Mitigation Techniques",
        "description": "Develop and implement Evasion Attack Prevention measures to detect inputs designed to cause model misclassification.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: SI-7: Software, Firmware, and Information Integrity\nSA-11: Developer Testing and Evaluation\nRA-5: Vulnerability Monitoring and Scanning\nSI-3: Malicious Code Protection, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Maintaining the trustworthiness of AI models in the face of evasion attack attempts.\nRSK-03: Risk Identification - Identifying risk associated with evasion attacks in AI systems and planning countermeasures.\nRSK-04: Risk Assessment - Assessing the likelihood and potential impact of evasion attacks on AI models.\nOPS-04: Security Operations Center (SOC) Capabilities - Leveraging SOC capabilities to detect and respond to evasion attacks on AI systems.",
        "explainability": "Evasion Attack Prevention is a control designed to identify and prevent evasion attempts on AI systems, which involve manipulating inputs to bypass the system's defenses or gain unauthorized access. The rationale for this control is to enhance the security and reliability of AI systems by ensuring that they are resistant to evasion attacks that could compromise their functionality or integrity. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of evasion attack prevention and its role in securing AI systems against manipulation and unauthorized access. \n\nPrevention Strategy Overview: A nontechnical summary of the strategies and techniques used to prevent evasion attacks. \n\nSecurity Resilience Metrics: Reports showing the effectiveness of evasion attack prevention in maintaining system security and resilience. | Responsibility Explanation: Evasion Prevention Strategy: A comprehensive plan detailing the prevention measures and responsible parties. \n\nModel Monitoring Protocols: Protocols for continuous monitoring of the model to detect attempts at evasion. \n\nEvasion Response Plan: A plan specifying the actions to be taken and by whom when evasion attempts are detected. | Data Explanation: A strategic framework document outlining the evasion attack prevention strategies, including the types of attacks anticipated, the detection mechanisms in place, and the response protocols. There should also be training and validation datasets used to test the model against evasion attempts, along with the performance metrics achieved. | Fairness Explanation: Evasion Prevention and Fairness Strategy: Details how evasion countermeasures are designed to protect fairness. \n\nEquitable Misclassification Response Records: Documents responses to evasion attempts and their fairness implications.\n \nFair Operation Protocols: Guidelines ensuring that evasion prevention does not compromise fairness. | Safety & Performance  Explanation: Evasion Prevention Strategy: A formal strategy outlining the specific techniques and processes for detecting and mitigating evasion attempts. \n\nModel Resilience Reports: Analysis of the model's response to known evasion techniques and a record of any successful or thwarted evasion attempts. \n\nTraining and Testing Protocols: Documentation of procedures for training the model to recognize evasion patterns and for testing the model's ability to withstand such attacks. | Impact Explanation: Evasion Attack Case Studies: Documented examples of evasion attempts, detailing the nature of the attack, the input involved, and the system's response.\n\nAttack Prevention Protocols: Documentation on the specific strategies and techniques used to prevent evasion attacks, including any updates or refinements made over time.\n\nSystem Resilience Reports: Analysis reports that assess the AI system's ability to resist evasion attacks and the effectiveness of implemented countermeasures."
    },
    {
        "control_name": "Data Poisoning Detection",
        "category": "AI Model Poisoning Defense",
        "description": "Develop and implement Data Poisoning Detection controls to detect abnormal or tainted data patterns indicating training data poisoning attempts.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10 - Data and data governance\nArticle 15 - Accuracy, robustness and cybersecurity\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: RA-5: Vulnerability Monitoring and Scanning\nSI-7: Software, Firmware, and Information Integrity\nSI-3: Malicious Code Protection\nCM-8: System Component Inventory, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models' trustworthiness by verifying that training data is free from poisoning.\nRSK-03: Risk Identification - Identifying risk of data poisoning in AI training datasets and planning appropriate controls.\nRSK-04: Risk Assessment - Assessing the impact of potential data poisoning on AI models and the efficacy of detection controls.\nOPS-04: Security Operations Center (SOC) Capabilities - Utilizing SOC capabilities to monitor and respond to data poisoning threats against AI systems.",
        "explainability": "Data Poisoning Detection is a control aimed at identifying and mitigating the presence of poisoned or malicious data in the datasets used to train and validate AI models. The rationale for this control is to ensure the integrity and reliability of AI models by preventing them from being trained on tainted data that could lead to biased or erroneous outcomes. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of data poisoning detection and its role in maintaining the quality and fairness of AI models. \n\nDetection Algorithm Overview: A nontechnical summary of the detection algorithms and techniques used to identify poisoned data. \n\nData Quality Assurance Metrics: Reports showing the effectiveness of data poisoning detection in preserving data quality and model performance. | Responsibility Explanation: Data Poisoning Detection Protocols: Documentation outlining the methods and personnel responsible for detecting data poisoning. \n\nData Custodian Documentation: Records detailing who is responsible for the custody and integrity of the data. \n\nData Poisoning Incident Logs: Logs that record all suspected or confirmed data poisoning incidents and the response. | Data Explanation: Development of a Data Poisoning Detection Framework, which includes a set of practices for monitoring, analyzing, and reporting on data integrity. It should also include a historical log of detected poisoning attempts and the strategies employed to mitigate them. | Fairness Explanation: Data Poisoning and Fairness Impact Report: Describes how detected data poisoning can affect fairness and the measures taken to rectify this. \n\nTraining Data Fairness Log: Chronicles interventions in data poisoning from a fairness perspective. \n\nAntipoisioning Fairness Certification: Certification that data poisoning detection aligns with fairness standards. | Safety & Performance  Explanation: Data Poisoning Detection Framework: Comprehensive documentation of the methods and technologies used to monitor and detect data poisoning. \n\nAnomaly Detection Reports: Regular analysis reports highlighting any abnormal data patterns and potential data poisoning flags. \n\nData Handling and Validation Procedures: Established procedures for data collection, handling, and validation that help prevent data poisoning. | Impact Explanation: Data Poisoning Incident Reports: Detailed reports of identified poisoning attempts, including the nature of the poisoned data, methods used to detect them, and the actions taken in response.\n\nData Quality Assurance Policies: Documentation outlining the standards and practices in place to maintain the integrity of the training data, including protocols for regular data audits.\n\nSystem Integrity Assessments: Periodic assessments of the AI system's resistance to data poisoning, highlighting the robustness of the detection mechanisms."
    },
    {
        "control_name": "Transfer Attack Testing",
        "category": "Adversarial Attack Mitigation Techniques",
        "description": "Develop and implement Transfer Attack Testing to determine and evaluate the AI model resilience against adversarial samples trained on a different but related model.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-7: Software, Firmware, and Information Integrity\nRA-5: Vulnerability Monitoring and Scanning\nCA-2: Control Assessments, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models are trustworthy and can handle adversarial samples from transfer attacks.\nRSK-03: Risk Identification - Identifying risk associated with transfer attacks on AI models and planning appropriate controls.\nRSK-04: Risk Assessment - Assessing the risk and potential impact of transfer attacks on AI models.\nOPS-04: Security Operations Center (SOC) Capabilities - Leveraging SOC capabilities to monitor and respond to threats like transfer attacks against AI systems.",
        "explainability": "Transfer Attack Testing is a control designed to assess and improve the robustness of AI models against transfer attacks, where adversarial examples crafted for one model can also fool a different but similar model. The rationale for this control is to enhance the security and reliability of AI models by ensuring that they are resistant to attacks that exploit model similarities. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of transfer attack testing and its role in securing AI models against adversarial threats. \n\nTesting Plan: A document outlining the strategies, methods, and tools used to perform transfer attack testing. \n\nSecurity Resilience Metrics: Reports showing the effectiveness of transfer attack testing in maintaining model security and robustness. | Responsibility Explanation: Transfer Attack Testing Plan: A detailed plan specifying the testing procedures and the roles involved in each step. \n\nModel Evaluation Records: Documentation of each evaluation performed, including methodologies used and the personnel responsible. \n\nVulnerability Response Guidelines: Guidelines outlining how to respond to identified vulnerabilities, including responsible parties. | Data Explanation: Protocols and outcomes of crossmodel adversarial testing exercises, including a clear definition of the related models used, the nature of the adversarial samples, and the criteria for resilience. Documentation should include detailed testing plans, executed tests, and an analysis of how the model responded to the transfer attacks. | Fairness Explanation: Transfer Attack and Fairness Evaluation Report: Documents the evaluation of transfer attacks for fairness implications. \n\nCrossmodel Fairness Analysis: Compares fairness outcomes across models subjected to transfer attacks. \n\nTransfer Attack Fairness Assurance Plan: A plan detailing how to maintain fairness when facing transfer attacks. | Safety & Performance  Explanation: Transfer Attack Test Plan: A document that lays out the strategies for conducting transfer attack testing, including a selection of related models for the generation of adversarial samples. \n\nResilience Analysis Report: Detailed findings from the transfer attack tests, outlining how the model performed and recommendations for enhancing robustness. \n\nCrossmodel Attack Response Procedures: Guidelines and protocols for responding to detected transfer attacks, designed to minimize their impact. | Impact Explanation: Crossmodel Attack Analysis Reports: Comprehensive reports analyzing the effectiveness of adversarial samples on different but related models, including potential vulnerabilities and mitigation strategies.\n\nResilience Assessment Protocols: Documentation of the standardized tests and procedures used to assess model resilience against transfer attacks, reflecting industry best practices.\n\nModel Compatibility Matrices: Matrices that map the relationships and compatibilities between different models to predict the likelihood of successful transfer attacks."
    },
    {
        "control_name": "Model Extraction Testing",
        "category": "Adversarial Attack Mitigation Techniques",
        "description": "Develop and implement Model Extraction Testing to determine if it is possible for attackers to create a functionally similar model of the original AI system by querying the original AI system.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: CA-8: Penetration Testing\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nCM-6: Configuration Settings, SCF: AAT-01.1: AI & Autonomous Technologies-Related Legal Requirements Definition - Establishing legal frameworks to protect AI models from unauthorized extraction.\nAAT-02.2: AI & Autonomous Technologies Internal Controls - Implementing internal controls to detect and prevent model extraction attempts.\nAAT-10.6: AI TEVV Transparency & Accountability Assessment - Assessing the transparency and accountability measures in place to safeguard against model extraction.\nAAT-11.4: AI & Autonomous Technologies Incident & Error Reporting - Reporting mechanisms for incidents related to AI model extraction attempts.\nIAC-06: Multi-Factor Authentication (MFA) - Using MFA to secure access to AI models, reducing the risk of unauthorized extraction.\nRSK-03: Risk Identification - Identifying the risk of model extraction as a potential threat to AI systems.\nRSK-04: Risk Assessment - Assessing the risk and potential impact of model extraction on the organization’s AI assets.",
        "explainability": "Model Extraction Testing is a control designed to assess and mitigate the risk of adversaries extracting sensitive information from an AI model. The rationale for this control is to enhance the security and confidentiality of AI models by ensuring that they are resistant to model extraction attacks that could compromise proprietary or confidential information. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of model extraction testing and its role in securing AI models against information theft. \n\nTesting Plan: A document outlining the strategies, methods, and tools used to perform model extraction testing. \n\nSecurity Resilience Metrics: Reports showing the effectiveness of model extraction testing in maintaining model security and confidentiality. | Responsibility Explanation: Extraction Testing Protocol: A comprehensive guideline on how to conduct model extraction tests, including the personnel involved. \n\nTest Execution Logs: Detailed records of every test performed, the methodologies used, and the personnel responsible. \n\nMitigation Strategy Plan: A document detailing strategies for preventing model extraction, including responsible parties for each action item. | Data Explanation: Detailed Model Extraction Testing Protocols, which outline the methodologies for simulating and detecting extraction attempts. It should include a record of simulated attacks, the model's response to the queries, and the effectiveness of protective measures in place. | Fairness Explanation: Model Extraction Fairness Report: Details how extraction testing is conducted with fairness considerations. \n\nDerivative Model Fairness Impact Analysis: Analyzes potential fairness impacts if a functionally similar model is created. \n\nFairness Safeguard Protocol: Outlines methods used to prevent the extraction of biased models. | Safety & Performance  Explanation: Model Extraction Testing Protocol: A step-by-step guide that specifies the testing procedures for evaluating the risk of model extraction. \n\nQuery Audit Logs: Detailed logs that track the frequency, type, and pattern of queries to the model, which can be analyzed for suspicious activity consistent with extraction attempts. \n\nModel Extraction Risk Assessments: Assessments that analyze the potential risk and impacts of model extraction based on the testing outcomes. | Impact Explanation: Model Extraction Risk Analysis Report: A document detailing the potential risk associated with model extraction, including IP loss and data privacy breaches.\n\nQuery Audit Logs: Logs that record all queries made to the model, which can be analyzed for patterns that suggest attempted extraction.\n\nExtraction Mitigation Strategy Document: A plan outlining measures taken to prevent model extraction, including both technical defenses and policy measures."
    },
    {
        "control_name": "Preventing Malicious Knowledge Transfer",
        "category": "Adversarial Countermeasures",
        "description": "Develop and implement standards and procedures to prevent malicious knowledge transfer by attackers to ensure that sensitive knowledge is not transferred and exploited.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity, NIST 800-53: SC-7: Boundary Protection\nAC-4: Information Flow Enforcement\nSI-4: System Monitoring\nRA-5: Vulnerability Monitoring and Scanning, SCF: AAT-01.1: AI & Autonomous Technologies-Related Legal Requirements Definition - Defining legal requirements to protect AI systems from malicious knowledge transfer.\nAAT-02.2: AI & Autonomous Technologies Internal Controls - Implementing internal controls to prevent malicious knowledge transfer in AI systems.\nAAT-10.6: AI TEVV Transparency & Accountability Assessment - Evaluating transparency and accountability measures to safeguard against malicious knowledge transfer.\nAAT-11.4: AI & Autonomous Technologies Incident & Error Reporting - Reporting mechanisms for incidents related to malicious knowledge transfer.\nRSK-03: Risk Identification - Identifying the risk of malicious knowledge transfer as a potential threat to AI systems.\nRSK-04: Risk Assessment - Assessing the risk and potential impact of malicious knowledge transfer on the organization’s AI assets.",
        "explainability": "Preventing Malicious Knowledge Transfer is a control designed to prevent adversaries from transferring malicious knowledge or insights from one AI model to another. The rationale for this control is to enhance the security and confidentiality of AI models by ensuring that sensitive knowledge is not exploited to train other models with harmful or unauthorized information. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of preventing malicious knowledge transfer and its role in securing AI models against knowledge leakage. \n\nPrevention Strategy Overview: A nontechnical summary of the strategies, methods, and tools used to prevent malicious knowledge transfer. \n\nSecurity Assurance Metrics: Reports showing the effectiveness of the control in maintaining model security and confidentiality. | Responsibility Explanation: Knowledge Transfer Prevention Policy: A formal policy that defines measures and identifies the roles involved in preventing malicious knowledge transfer. \n\nEnforcement Procedure Document: Detailed procedures on how to enforce the prevention measures. \n\nRole Responsibility Matrix: A matrix that clarifies the specific responsibilities of each role in the process. | Data Explanation: Strategies and protocols for identifying and mitigating the risk of knowledge transfer via adversarial machine learning techniques. This includes a detailed description of monitoring systems, the criteria used to detect malicious activities, and the response plans in case of an incident. | Fairness Explanation: Knowledge Transfer Prevention Plan: A detailed plan addressing how to prevent malicious knowledge transfer with a focus on fairness. \n\nBias Introduction via Knowledge Transfer Report: Documents instances where knowledge transfer could introduce biases. \n\nTransfer Prevention and Fairness Assurance Statement: Certifies that measures to prevent knowledge transfer do not compromise fairness. | Safety & Performance  Explanation: Knowledge Transfer Prevention Plan: A document detailing the approach and tactics for preventing the assimilation of malicious knowledge into the AI system.\n \nTraining Data Integrity Assessments: Reports evaluating the sources and contents of training data to ensure they are free from adversarial influences. \n\nUpdate and Revision Histories: Logs that track changes to the model, including data updates and algorithm revisions, which can be audited for signs of malicious knowledge incorporation. | Impact Explanation: Knowledge Transfer Risk Assessment Report: A comprehensive document that identifies potential areas where knowledge transfer could be exploited by attackers, along with the associated risk.\n\nModel Monitoring and Logging Policies: Detailed descriptions of the logging mechanisms and monitoring policies that track  usage of the model to identify potential knowledge extraction.\n\nMalicious Use Case Analysis: A set of scenarios that illustrate how extracted knowledge could be misused, aiding in understanding the full spectrum of threats."
    },
    {
        "control_name": "Development of a Standardized Procedure for Assessing High-Risk AI Systems",
        "category": "AI Fairness Certification & Standards",
        "description": "Develop a standardized procedure for assessing high-risk AI systems. This procedure should provide a consistent framework for evaluation, covering areas like performance, risk, fairness and compliance and include a communication plan for communicating information to nontechnical stakeholders.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 74: Emphasizes the need for high-risk AI systems to meet an appropriate level of accuracy, robustness and cybersecurity.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: CA-2: Control Assessments\nRA-3: Risk Assessment\nPM-9: Risk Management Strategy\nSA-10: Developer Configuration Management, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Governance controls for AI systems, including procedures for assessing high-risk systems.\nAAT-10.9: AI & Autonomous Technologies Model Validation - Controls for model validation, a critical aspect of assessing high-risk AI systems.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Reviewing cybersecurity and data privacy controls as part of the AI system assessment process.\nGOV-15.3: Assess Controls - Procedures for assessing controls in place within AI systems, especially those considered high-risk.\nRSK-01: Risk Management Program - Establishing a risk management program that includes standardized assessment of high-risk AI systems.\nRSK-04: Risk Assessment - Controls for conducting risk assessments, integral to the standardized procedure for high-risk AI systems.\nRSK-11: Risk Monitoring - Ongoing monitoring of risk in high-risk AI systems as part of the standardized assessment procedure.\nTDA-06.2: Threat Modeling - Threat modeling processes that are part of a standardized assessment for high-risk AI systems.",
        "explainability": "Developing a standardized procedure for assessing high-risk AI systems is essential to ensure that these systems are thoroughly evaluated for potential risk and implications. The rationale for this control is to enhance transparency, accountability, and rigor in the evaluation of high-risk AI systems. This standardized procedure aims to provide a structured approach for assessing the impact, fairness, safety, and compliance of these systems. It should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of the standardized procedure and its role in ensuring comprehensive assessments of high-risk AI systems. \n\nProcedure Documentation: A detailed document outlining the step-by-step standardized procedure for assessing high-risk AI systems, including criteria, methodologies, and risk thresholds. \n\nCommunication Plan: A plan for communicating the standardized procedure to relevant stakeholders, including the public, regulators, and internal teams. | Responsibility Explanation: Standardized Assessment Framework: A document outlining the standardized procedures for assessing high-risk AI systems, including role assignments. \n\n\nValidation Reports: Reports from the validation of the assessment framework detailing the effectiveness and coverage of the procedures. \n\nFramework Revision Logs: Logs that record any changes to the assessment framework, including who authorized and implemented the changes. | Data Explanation: A comprehensive set of guidelines and checklists that outline the standardized assessment procedure. This should include specific metrics for performance, a risk assessment framework, and a compliance checklist with regulatory requirements. | Fairness Explanation: Standardized Fairness Assessment Procedure: The established procedure for a fair assessment of high-risk AI systems. \n\nHigh-Risk AI Fairness Framework Documentation: A detailed framework for how fairness is to be maintained in high-risk AI assessments. \n\nRisk and Fairness Compliance Protocol: Ensures high-risk AI systems are compliant with fairness standards. | Safety & Performance  Explanation: Standardized Assessment Procedure Manual: A comprehensive manual that details the step-by-step process for evaluating high-risk AI systems. \n\nRisk and Compliance Checklist: A detailed checklist that encompasses various risk factors and compliance requirements specific to high-risk AI applications. \n\nPerformance Benchmarking Report: A report that provides performance metrics measured against industry standards or regulatory benchmarks for high-risk systems. | Impact Explanation: High-Risk AI Assessment Framework Document: A detailed description of the procedures and criteria for evaluating high-risk AI systems, which includes methodologies for impact assessment.\n\nRisk Impact Reports: Documentation of the potential impacts identified during the assessment process, which may include ethical impact assessments, data privacy impact assessments, and others relevant to the specific context of use.\n\nCompliance Checklist: A checklist that outlines the necessary compliance requirements for high-risk AI systems, including industry-specific regulations and standards."
    },
    {
        "control_name": "Unfairness Test Scope",
        "category": "Bias & Fairness Validation Tools",
        "description": "Develop and implement an unfairness test and ensure that the scope of the unfairness test is limited to elements related to high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 74: Emphasizes the need for high-risk AI systems to meet an appropriate level of accuracy, robustness and cybersecurity.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: RA-3: Risk Assessment\nCA-2: Control Assessments\nSA-11: Developer Testing and Evaluation\nPL-2: System Security and Privacy Plans, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI systems, particularly high-risk ones, are developed and used in a manner that upholds fairness.\nAAT-06: AI & Autonomous Technologies Fairness & Bias - Controls to ensure fairness in AI systems and prevent bias, especially in high-risk contexts.\nAAT-10.9: AI & Autonomous Technologies Model Validation - Validating AI models for fairness, especially those used in high-risk scenarios.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Reviewing controls related to fairness in high-risk AI systems.\nGOV-01: Cybersecurity & Data Protection Governance Program - Governance programs that incorporate fairness evaluations, particularly for high-risk AI systems.\nRSK-03: Risk Identification - Identifying risk related to unfairness in AI systems, with a focus on those classified as high-risk.\nRSK-04: Risk Assessment - Risk assessment procedures that consider the fairness aspects of high-risk AI systems.",
        "explainability": "Unfairness Test Scope control is implemented to define the scope and parameters for assessing unfairness in AI systems. The rationale for this control is to ensure that unfairness assessments are targeted, effective, and relevant. It helps in focusing the testing efforts on specific areas, characteristics, or use cases that are most likely to result in unfair outcomes. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of defining the scope for unfairness tests and its role in efficient and effective assessment. \n\nTest Scope Definition: A document outlining the specific scope, criteria, and considerations for conducting unfairness assessments in AI systems. \n\nAssessment Planning: A plan detailing how the defined scope will be implemented and the expected outcomes of the unfairness tests. | Responsibility Explanation: Unfairness Testing Guidelines: A document that outlines the specific areas to be covered in unfairness tests and the roles responsible for each aspect. \n\nTesting Scope Records: Documentation that records the rationale for scope definitions and any scope changes. \n\nScope Compliance Logs: Logs that ensure the unfairness tests are conducted within the defined scope. | Data Explanation: Development of a scoped Unfairness Test Plan that outlines the specific elements of high-risk AI systems to be evaluated. This plan should include the criteria for unfairness assessments, targeted areas within the AI system, and the benchmarks for fairness. | Fairness Explanation: Unfairness Test Scope Documentation: Outlines the scope of unfairness tests with an emphasis on high-risk systems. \n\nFairness Issue Identification and Mitigation Plan: Plan documenting identified fairness issues and mitigation strategies. \n\nHigh-Risk System Fairness Verification Report: A report verifying that the unfairness test covers all necessary fairness aspects of high-risk AI. | Safety & Performance  Explanation: Unfairness Testing Framework: A document that defines the specific criteria and measures used to test for unfairness in high-risk AI systems. \n\nTest Case Scenarios: A collection of scenarios that represent diverse populations and decision contexts to evaluate the system's fairness.\n \nUnfairness Impact Reports: Comprehensive reports summarizing the findings from unfairness tests and their potential impacts on different demographic groups. | Impact Explanation:  Fairness Testing Protocol: A document outlining the scope, methods, and criteria for fairness testing in high-risk AI systems, detailing the specific fairness metrics to be used.\n\nBias Impact Analysis Report: A report that documents the findings of the unfairness test, highlighting potential areas of bias and their implications for different groups affected by the AI system's decisions.\n\nMitigation Strategy Plan: A plan specifying actionable steps to address any detected biases or areas of unfairness, including algorithmic adjustments, data corrections, and monitoring strategies."
    },
    {
        "control_name": "AI Systems & Discrimination",
        "category": "Bias & Fairness Validation Tools",
        "description": "Ensure that the output of AI systems do not perpetuate historical discrimination patterns and that they respect the rights to data protection and privacy.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 74: Emphasizes the need for high-risk AI systems to meet an appropriate level of accuracy, robustness and cybersecurity.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: RA-5: Vulnerability Monitoring and Scanning\nAC-6: Least Privilege\nPT-1: PII Processing and Transparency Policy and Procedures\nPT-2: Authority to Process Personally Identifiable Information\nPT-3: Consent\nPT-7: Specific Categories of Personally Identifiable Information \nSA-11: Developer Testing and Evaluation, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring trustworthiness in AI systems, including their adherence to non-discrimination principles.\nAAT-06: AI & Autonomous Technologies Fairness & Bias - Controls to prevent AI systems from perpetuating historical discrimination and ensuring fairness.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls in AI systems to ensure non-discrimination.\nGOV-01: Cybersecurity & Data Protection Governance Program - Governance programs that include non-discrimination as a key principle in AI systems.\nDCH-01.2: Sensitive / Regulated Data Protection - Protecting sensitive data in AI systems, a key aspect of preventing discrimination.\nPRI-01.6: Security of Personal Data - Ensuring the security of personal data in AI systems to respect privacy rights.\nPRI-02: Data Privacy Notice - Ensuring that data privacy notices reflect commitments to non-discrimination and privacy protection in AI systems.\nRSK-03: Risk Identification - Identifying risk related to discrimination in AI systems and planning appropriate controls.",
        "explainability": "The AI Systems and & Discrimination control aims to address and mitigate discrimination in AI systems. The rationale for this control is to promote fairness, equity, and inclusivity in AI applications by preventing discriminatory outcomes. Discrimination can occur due to biases in training data, model design, or decision-making processes. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of addressing discrimination in AI systems and its role in promoting fairness and equity. \n\nNondiscrimination Guidelines: A set of guidelines outlining the principles and strategies for preventing discrimination in AI systems. \n\nDiversity and Inclusion Plan: A plan detailing how diversity and inclusion considerations are integrated into the AI system development process. | Responsibility Explanation: Nondiscrimination Policy: A formal policy that outlines the commitment to nondiscrimination and the roles involved in upholding it. \n\nPrivacy and Data Protection Procedures: Documented procedures that ensure AI systems respect data protection and privacy rights. \n\nCompliance Monitoring Framework: A framework for monitoring compliance with nondiscrimination, privacy, and data protection standards. | Data Explanation: An Antidiscrimination Policy specific to AI system development and deployment. This should include a detailed analysis of historical discrimination patterns relevant to the AI system's domain, and the measures in place to prevent such biases in the system's decision-making processes. | Fairness Explanation: AI Nondiscrimination Protocol: A protocol outlining measures to prevent AI systems from discriminating. \n\nHistorical Bias Analysis and Rectification Record: Records efforts to identify and rectify historical biases in AI systems. \n\nData Protection and Fairness Compliance Certificate: Certifies compliance with data protection laws and fairness standards. | Safety & Performance  Explanation: Nondiscrimination Policy: An official document outlining the organization’s commitment to preventing AI from perpetuating bias and discrimination. \n\nData Protection Impact Assessment (DPIA): A thorough assessment that evaluates how personal data is processed to ensure compliance with data protection laws and the AI system’s impact on privacy.\n \nHistorical Bias Review Reports: Analyses that examine past data and model decisions for patterns of discrimination. | Impact Explanation: Historical Discrimination Impact Report: An analytical report reviewing historical discrimination patterns and the potential risk of AI systems perpetuating them.\n\nPrivacy and Data Protection Compliance Documentation: Detailed documentation demonstrating the AI system’s compliance with relevant data protection and privacy regulations, such as GDPR or HIPAA.\n\nRights Impact Assessment (RIA): A comprehensive assessment evaluating the impact of AI systems on individual rights, particularly focusing on nondiscrimination and privacy."
    },
    {
        "control_name": "Consideration of Adversarial Machine Learning for Bias Measurement",
        "category": "Bias & Fairness Validation Tools",
        "description": "Develop and implement adversarial machine learning approaches for measuring bias and discrimination, including prompt engineering and adversarial models.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity, NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-7: Software, Firmware, and Information Integrity\nRA-5: Vulnerability Monitoring and Scanning\nPL-8: Security and Privacy Architectures, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models are trustworthy by using adversarial machine learning to measure and address bias.\nAAT-06: AI & Autonomous Technologies Fairness & Bias - Implementing controls to measure and mitigate bias in AI systems using adversarial machine learning techniques.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of controls, including those using adversarial machine learning, to ensure fairness and reduce bias in AI systems.\nRSK-03: Risk Identification - Identifying risk of bias in AI systems and considering adversarial machine learning as a tool for measurement and mitigation.\nRSK-04: Risk Assessment - Assessing the effectiveness of adversarial machine learning techniques in identifying and mitigating bias in AI systems.",
        "explainability": "The Consideration of Adversarial Machine Learning for Bias Measurement control involves the use of adversarial machine learning techniques to measure and mitigate biases in AI systems. The rationale for this control is to enhance the fairness, equity, and accountability of AI systems by employing advanced techniques to identify and address biases. Adversarial machine learning can help detect subtle forms of bias and discrimination in AI models. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of considering adversarial machine learning for bias measurement and its role in promoting fairness and equity in AI systems. \n\nBias Measurement Guidelines: A set of guidelines outlining how adversarial machine learning is used to measure bias in AI systems. \n\nBias Mitigation Plan: A plan detailing strategies and methods for addressing biases identified through adversarial machine learning. | Responsibility Explanation: Adversarial Testing Plan: A document detailing the plan for adversarial testing to measure bias, including personnel roles. \n\nBias Measurement Reports: Reports from adversarial tests that indicate levels of bias discovered. \n\nBias Adjustment Records: Documentation of adjustments made to models in response to bias measurements. | Data Explanation: Development of a Bias Measurement Protocol that utilizes adversarial machine learning techniques. The protocol should detail how prompt engineering and adversarial models are used to uncover and measure bias in AI systems. | Fairness Explanation: Adversarial Machine Learning Fairness Testing Protocol: Details how adversarial machine learning is used to measure and correct biases. \n\nBias Measurement and Mitigation Record: Documents the identification and mitigation of biases through adversarial techniques. \n\nFairness Improvement via Adversarial Machine Learning Report: Reports on improvements in fairness due to adversarial machine learning techniques. | Safety & Performance  Explanation: Adversarial Testing Guidelines: Detailed instructions on how to conduct adversarial tests to uncover bias within AI models. \n\nBias Measurement Reports: The findings from adversarial testing, including metrics and analyses, that indicate the presence and severity of biases. \n\nAdversarial Model Development Documentation: Technical documents that describe the creation and use of models designed to challenge and test the fairness of the primary AI system. | Impact Explanation: Bias Measurement Report: A report that details findings from adversarial testing, highlighting potential biases and recommendations for improvement.\n\nAdversarial Test Suite: A collection of tools and tests designed for prompting AI systems in order to uncover biases.\n\nModel Response Analysis Documentation: Documentation of how the AI model responds to adversarial inputs, which can be indicative of bias."
    },
    {
        "control_name": "Bias Detection & Correction",
        "category": "Bias & Fairness Validation Tools",
        "description": "Implement the identification and mitigation of biased processes and procedures to detect and correct biases in training data, ensuring fairness in model predictions and to document, analyze and follow up on unfairness that could result in a discriminatory  outcome.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity, NIST 800-53: RA-5: Vulnerability Monitoring and Scanning\nSI-7: Software, Firmware, and Information Integrity\nSA-11: Developer Testing and Evaluation\nPL-8: Security and Privacy Architectures, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Maintaining trustworthiness in AI systems by rigorously addressing biases in training data.\nAAT-06: AI & Autonomous Technologies Fairness & Bias - Controls to detect and correct biases in AI training data, promoting fairness in model outcomes.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of controls related to bias detection and correction in AI systems.\nGOV-01: Cybersecurity & Data Protection Governance Program - Governance programs that incorporate principles of fairness and bias correction in AI systems.\nRSK-03: Risk Identification - Identifying risk of bias in AI systems and implementing controls for detection and correction.\nRSK-04: Risk Assessment - Assessing the risk posed by biases in training data and the effectiveness of correction processes.",
        "explainability": "The Bias Detection and Correction control involves the identification and mitigation of biases in AI systems. The rationale for this control is to ensure fairness, equity, and ethical AI by proactively detecting and correcting biases that could result in discriminatory outcomes. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of bias detection and correction and its role in promoting fairness and equity in AI systems. \n\nBias Detection Guidelines: Guidelines outlining the methodologies and tools used for identifying biases in AI models and decision-making processes. \n\nBias Mitigation Plan: A plan detailing the strategies and methods for addressing biases once detected. | Responsibility Explanation: Bias Detection Framework: A comprehensive framework detailing procedures for detecting biases and assigning responsibilities. \n\nCorrection Implementation Logs: Logs that track the implementation of corrective actions. \n\nFairness Audit Reports: Periodic reports assessing the fairness of the model post correction. | Data Explanation: A comprehensive Bias Detection and Correction Framework, including a description of the methodologies used to detect biases in the data, as well as the techniques employed to correct these biases before they affect the model’s predictions. | Fairness Explanation: Bias Detection and Correction Protocol: A comprehensive guide detailing the methods for detecting and correcting biases in data. \n\nFairness Correction Action Reports: Documentation of actions taken to correct identified biases and their outcomes on fairness. \n\nBias Audit and Mitigation Logs: Ongoing logs that record instances of bias detection and the steps taken to address them. | Safety & Performance  Explanation: Bias Detection and Correction Protocol: A comprehensive guide that outlines the steps to identify and mitigate bias in AI datasets. \n\nTraining Data Audit Reports: Documents that record the findings of biases in training datasets, along with the steps taken to correct them. \n\nCorrective Action Plans: Strategic plans that specify the actions required to adjust biased data and improve model fairness. | Impact Explanation: Bias Audit Report: A comprehensive document detailing detected biases in training data and subsequent corrections applied.\n\nCorrective Action Plan: A strategic plan outlining steps for rectifying identified biases, including data resampling or algorithmic adjustments.\n\nFairness Metrics Dashboard: An interactive dashboard that monitors key fairness metrics over time, reflecting the impact of corrective measures."
    },
    {
        "control_name": "Enhanced Dataset Access",
        "category": "AI Data Access, Sharing, & Control",
        "description": "Ensure that authorized entities have enhanced access to high-trustworthy datasets (e.g., European common data spaces, US Federal Data Strategy, etc.) for the development and assessment of high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nRecital 62: Emphasizes ability to access and use high quality datasets.\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks., NIST 800-53: AC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-8: System Component Inventory, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining awareness of AI technologies' data requirements, including access to quality datasets.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Compliance with regulatory requirements regarding data access and privacy, particularly in European contexts.\nDCH-01: Data Protection - Implementing data protection controls to ensure authorized access to high-quality datasets for AI development.\nDCH-06: Media Storage - Ensuring secure storage of datasets, facilitating enhanced access for authorized entities.\nDCH-14: Information Sharing - Controls to facilitate safe and authorized information sharing, including dataset access for AI systems.\nDCH-18: Media & Data Retention - Managing the retention of datasets, ensuring their availability for authorized access in AI development.\nPRI-01: Data Privacy Program - Establishing a data privacy program that includes guidelines for enhanced dataset access for AI development.\nPRI-02: Data Privacy Notice - Providing clear notices about the use and access of datasets for AI development, respecting data privacy norms.\nTDA-04: Documentation Requirements - Documentation requirements for enhanced dataset access, ensuring compliance and quality.",
        "explainability": "The Enhanced Dataset Access control involves providing improved access to the datasets used in AI model training and decision-making processes. The rationale for this control is to enhance transparency, accountability, and fairness in AI systems by allowing stakeholders, including researchers and regulators, to access and review the data used in AI development. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of enhanced dataset access and its role in promoting transparency and fairness in AI systems. \n\nDataset Access Policy: A policy outlining the procedures and criteria for granting access to AI datasets. \n\nAccess Monitoring Plan: A plan for monitoring and managing dataset access to ensure it aligns with fairness and ethical considerations. | Responsibility Explanation: Access Management Policy: A policy document that outlines the rules and responsibilities for dataset access. \n\nDataset Access Logs: Detailed logs that record who accesses the datasets and for what purpose. \n\nData Quality Assurance Reports: Reports ensuring that the datasets accessed are of high quality and suitable for the intended purposes. | Data Explanation: Access protocols and agreements that detail the terms of use, data sharing mechanisms, and security measures for high-quality datasets within European common data spaces. This also includes a catalogue or inventory of datasets available for high-risk AI system development and assessment, including metadata on data quality and relevance. | Fairness Explanation: Dataset Access and Fairness Enhancement Plan: A plan that details how enhanced dataset access contributes to the fairness of AI systems. \n\nQuality and Fairness Dataset Certification: Certification for datasets that meet high-quality and fairness standards. \n\nFair Dataset Usage and Compliance Records: Records of how datasets are used to ensure fairness in AI system development and assessment. | Safety & Performance  Explanation: Dataset Access Policy: A document detailing the terms, conditions, and protocols for granting access to high-quality datasets from European common data spaces. \n\nData Quality Assurance Reports: Reports that document the quality, diversity, and relevance of datasets being accessed for the development and testing of high-risk AI systems. \n\nAccess Control Logs: Records that track which entities have accessed which datasets, ensuring only authorized use. | Impact Explanation: Data Access Policy: A document outlining rules and protocols for accessing datasets, ensuring compliance with European data governance standards.\n\nData Quality Report: A report assessing the quality of datasets, including accuracy, completeness, and balance, with insights into the methodology used for the assessment.\n\nData Use Agreement: Agreements between the data providers and users that stipulate the terms of use, privacy standards, and responsibilities of each party."
    },
    {
        "control_name": "Prioritize Human Rights in AI",
        "category": "AI Data Access, Sharing, & Control",
        "description": "Ensure that the fundamental human rights of individuals, including data protection rights, nondiscrimination, freedom from bias and the right to explanation are prioritized and upheld in AI deployments and documented.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 27: Fundamental Rights Impact Assessment for High-Risk AI Systems\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: AC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nPM-22: Personally Identifiable Information Quality Management\nPT-3: Personally Identifiable Information Processing Purposes\nAC-24: Access Control Decisions, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI technologies uphold the rights of individuals, including data protection and explanation rights.\nAAT-06: AI & Autonomous Technologies Fairness & Bias - Controls to ensure AI deployments do not infringe on individuals' rights and are free from bias.\nCPL-03: Cybersecurity & Data Privacy Assessments - Assessing AI systems for compliance with data privacy and individual rights requirements.\nDCH-01: Data Protection - Implementing data protection controls that ensure the rights of individuals are respected in AI systems.\nPRI-01: Data Privacy Program - Developing and implementing a data privacy program that prioritizes individuals' rights in AI deployments. \nPRI-02: Data Privacy Notice - Providing clear and comprehensive privacy notices that inform individuals of their rights in the context of AI deployments.\nPRI-07: Information Sharing With Third Parties - Managing information sharing in a way that prioritizes and respects individuals' rights in AI systems.\nRSK-03: Risk Identification - Identifying risk to individuals' rights in AI systems and implementing measures to mitigate these risk.",
        "explainability": "The Prioritize Rights in AI control aims to prioritize and protect the rights of individuals and groups when designing and implementing AI systems. The rationale for this control is to ensure that AI systems respect and uphold fundamental human rights, including privacy, non-discrimination, nondiscrimination, and freedom from bias. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Report: A document explaining the importance of prioritizing rights in AI and its role in upholding human rights and ethical considerations. \n\nRights Prioritization Guidelines: Guidelines outlining how AI development prioritizes human rights, privacy, and nondiscrimination.\n\nEthical Impact Assessment: A framework for assessing the ethical impact of AI systems and their alignment with human rights. | Responsibility Explanation: Rights Assurance Framework: A framework that outlines how individuals' rights are protected in AI systems and assigns responsibilities. \n\nRights Impact Assessments: Assessments that evaluate the impact of AI systems on individuals' rights. \n\nCompliance Documentation: Documentation of compliance with data protection regulations and explanation rights. | Data Explanation: A Rights Protection Framework that outlines the specific measures taken to safeguard individual rights in AI deployments. This should include policy documents, operational guidelines, and staff training materials that emphasize the importance of these rights. | Fairness Explanation: Rights Prioritization in AI Framework: A framework outlining how individual rights are safeguarded in AI deployments. \n\nData Protection and Right to Explanation Compliance Reports: Reports confirming compliance with data protection rights and the right to explanation. \n\nAI Rights Impact Assessments: Assessments that evaluate AI systems for their impact on individual rights. | Safety & Performance  Explanation: Rights Preservation Framework: A document outlining how the AI respects individual rights and the procedures in place for this purpose. \n\nData Protection Compliance Certificates: Certifications or attestations demonstrating compliance with data protection laws. \n\nRight to Explanation Guidelines: Protocols that ensure individuals can obtain understandable explanations for AI decisions. | Impact Explanation: Rights Impact Assessment (RIA): A comprehensive analysis that evaluates how the AI system affects individual rights, including privacy and the right to explanation.\n\nPrivacy-by-Design Documentation: Documentation showing how privacy has been embedded at each stage of the AI system's development.\n\nAI Transparency Report: A report that provides insight into how decisions are made by the AI system and explains the logic involved to support the right to explanation."
    },
    {
        "control_name": "Data Privacy & Handling Protocols",
        "category": "AI Data Anonymization, Deidentification, & Sensitive Data Handling",
        "description": "Implement and document protocols and practices for data handling, ensuring data encryption, anonymization, and access controls.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: SC-28: Protection of Information at Rest\nMP-4: Media Storage\nAC-3: Access Enforcement\nSC-13: Cryptographic Protection, SCF: AAT-05: AI & Autonomous Technologies Training - Training relevant personnel on strict data handling protocols, including encryption and anonymization.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, ensuring adherence to strict data handling protocols in AI deployments.\nCRY-05: Encrypting Data At Rest - Encrypting data at rest, an essential part of data privacy and handling protocols in AI systems.\nDCH-01: Data Protection - Implementing data protection controls, including encryption and anonymization protocols for AI data handling.\nDCH-06: Media Storage - Secure storage protocols for AI data, ensuring proper encryption and anonymization measures are applied.\nDCH-23: De-Identification (Anonymization) - Implementing de-identification and anonymization techniques as part of data handling protocols.\nPRI-01: Data Privacy Program - Developing a data privacy program that emphasizes strict data handling protocols, including anonymization and encryption.\nPRI-02: Data Privacy Notice - Providing clear privacy notices that reflect strict data handling protocols, including anonymization and encryption practices.\nIAC-20: Access Enforcement - Enforcing strict access controls as part of data handling protocols in AI systems.",
        "explainability": "The Data Privacy & Handling Protocols control involves the establishment of protocols and practices to ensure the privacy and secure handling of data in AI systems. The rationale for this control is to protect individuals' data, maintain compliance with data protection regulations, and build trust with users. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of data privacy and handling protocols in AI and their role in protecting individuals' data and ensuring compliance. \n\nData Handling Protocols: A document outlining the procedures and practices for collecting, storing, and processing data in AI systems with a focus on privacy and security. \n\nPrivacy Compliance Report: A report demonstrating the AI system's compliance with data privacy regulations and the effectiveness of the handling protocols. | Responsibility Explanation: Privacy Protocol Manual: A detailed manual of the protocols for data privacy and handling, including the roles responsible for each protocol. \n\nData Handling Logs: Logs that track data handling activities, ensuring protocols are followed. \n\nPrivacy Compliance Reports: Reports verifying that data privacy protocols are being maintained. | Data Explanation: Data Handling and Privacy Protocol documents, including encryption standards, anonymization techniques, and detailed access control procedures. These documents should also include logs of data access and handling incidents, with an emphasis on privacy breaches and the responses to such events. | Fairness Explanation: Data Privacy and Handling Standard Operating Procedures (SOPs): SOPs detailing the data privacy protocols and their role in ensuring fairness. \n\nAnonymization and Encryption Compliance Certificates: Certificates showing compliance with data anonymization and encryption standards. \n\nData Handling and Fairness Audit Logs: Logs that document data handling practices and their compliance with fairness standards. | Safety & Performance  Explanation: Data Handling and Privacy Protocol Document: A detailed manual that specifies the methods and practices for data encryption, anonymization, and access control. \n\nData Encryption Standards Compliance Report: A document certifying adherence to recognized data encryption standards. \n\nAnonymization Technique Verification Records: Verification logs that demonstrate the effectiveness of data anonymization methods used. | Impact Explanation: Data Handling Policy: A detailed document outlining the procedures and protocols for secure data handling.\n\nData Anonymization Procedures: Documented methods and practices for anonymizing data to protect individual privacy.\n\nAccess Control Policies and Logs: Documentation and records that detail the mechanisms for access control and track data access within the system."
    },
    {
        "control_name": "AI Data Retention & Encryption Protocols",
        "category": "AI Data Anonymization, Deidentification, & Sensitive Data Handling",
        "description": "Deploy encryption techniques and define and implement clear data retention timelines to bolster AI data security.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: SC-12: Cryptographic Key Establishment and Management\nSC-28: Protection of Information at Rest\nMP-5: Media Sanitization\nAU-11: Audit Record Retention, SCF: CPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, including data encryption and retention protocols.\nCRY-01: Use of Cryptographic Controls - Implementing state-of-the-art cryptographic controls to secure AI data.\nCRY-02: Cryptographic Module Authentication - Authentication mechanisms for cryptographic modules used in AI data encryption.\nCRY-05: Encrypting Data At Rest - Utilizing advanced encryption techniques for securing AI data at rest.\nDCH-06: Media Storage - Applying strict encryption protocols for AI data storage, ensuring data security and privacy.\nDCH-18: Media & Data Retention - Defining and managing data retention timelines for AI data to enhance security and compliance.\nPRI-01: Data Privacy Program - Developing a data privacy program that includes encryption and data retention protocols for AI data.\nIAC-20: Access Enforcement - Enforcing access controls in line with data encryption and retention protocols for AI data security.\nRSK-04: Risk Assessment - Assessing risk associated with AI data security and implementing appropriate encryption and retention measures.",
        "explainability": "The AI Data Retention & Encryption Protocols control involves the development of protocols and practices for the retention and encryption of data in AI systems. The rationale for this control is to ensure the secure and ethical handling of data by retaining it only as long as necessary and encrypting it to protect sensitive information. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of data retention and encryption protocols in AI and their role in data security and ethical handling. \n\nData Retention Policy: A document outlining the policies and guidelines for the retention and disposal of data in AI systems. \n\nEncryption Protocols Overview: A nontechnical summary of the encryption methods and techniques used to protect data in AI systems. | Responsibility Explanation: Data Encryption & Retention Policy: A formal document that specifies encryption standards, data retention schedules, and roles responsible for data security. \n\nEncryption Audit Trails: Records that detail encryption activities and responsible parties. \n\nData Retention Compliance Logs: Logs to track adherence to data retention timelines and responsible personnel. | Data Explanation: Encryption and Data Retention Policy detailing the encryption methods used, the criteria for data retention, and the processes for data deletion after the retention period expires. The policy should also document the roles and responsibilities for managing data retention and encryption. | Fairness Explanation: Data Retention and Encryption Policy Documents: Documents outlining the data retention and encryption policies and their importance for fairness.\n \nData Security and Fairness Compliance Certificates: Certificates confirming that data retention and encryption practices meet security and fairness standards. \n\nData Life Cycle Management and Fairness Reports: Reports on the management of data lifecycles, life cycles, focusing on the implications for fairness. | Safety & Performance  Explanation: Encryption Protocol Document: Detailed guidelines and standards for data encryption specific to AI systems, ensuring the use of modern encryption techniques. \n\nData Retention Policy: A clear outline of how long different types of data are to be retained, including the criteria for data deletion. \n\nCompliance with Data Protection Standards Certificate: Evidence of adherence to international and industry-specific data protection and encryption standards. | Impact Explanation: Encryption Standards Documentation: A comprehensive manual detailing the encryption standards and protocols used.\n\nData Retention Policy: A formal document outlining the specific timelines for data retention and the criteria for data deletion.\n\nData Life Cycle Management Plan: A plan that covers the complete life cycle of data, from creation to deletion, and the security measures applied at each stage."
    },
    {
        "control_name": "AI System & Data Protection - Training to Prevent Data Compromise",
        "category": "AI Data Anonymization, Deidentification, & Sensitive Data Handling",
        "description": "Leverage technical (e.g., security protocols) and nontechnical (e.g., policies) methods to minimize the risk of compromised training data.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: SC-28: Protection of Information at Rest\nMP-4: Media Storage \nRA-5: Vulnerability Monitoring and Scanning\nAC-3: Access Enforcement, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining situational awareness to protect AI training data from potential compromises.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, specifically concerning the protection of training data.\nCRY-05: Encrypting Data At Rest - Encrypting AI training data at rest to minimize the risk of unauthorized access or compromise.\nDCH-01: Data Protection - Implementing robust data protection controls to safeguard training data from compromise.\nDCH-06: Media Storage - Securely storing training data media to protect it from potential security breaches or compromise.\nDCH-23: De-Identification (Anonymization) - Utilizing de-identification and anonymization techniques to protect training data.\nPRI-01: Data Privacy Program - Establishing a data privacy program that includes specific protocols for protecting training data.\nRSK-03: Risk Identification - Identifying risk associated with the compromise of AI training data and planning appropriate controls.\nRSK-04: Risk Assessment - Assessing the potential impact and likelihood of training data compromise in AI systems.",
        "explainability": "The AI System and Data Protection - Training Data Compromise control focuses on safeguarding AI systems and their training data from compromise and unauthorized access. The rationale for this control is to ensure the integrity and security of AI systems by preventing breaches and data theft that can lead to biased or compromised models. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of protecting AI systems and training data from compromise and its role in maintaining model integrity and data security. \n\nSecurity Measures Overview: A nontechnical summary of the security measures and protocols used to protect AI systems and training data. \n\nData Security Assessment: Reports demonstrating the effectiveness of data protection measures and the absence of training data compromise. | Responsibility Explanation: Training Data Security Plan: A detailed plan outlining security measures for protecting training data, including responsible roles. \n\nRisk Mitigation Records: Documentation of all actions taken to mitigate risk to training data. \n\nTraining Data Breach Response Plan: A plan specifying steps and identifying responsible individuals to respond to any breaches in training data security. | Data Explanation: Training Data Protection Strategy, which should detail the security measures and protocols in place to protect training data from being compromised. This includes access controls, data encryption, and regular security training for personnel involved in handling the data. | Fairness Explanation: Training Data Protection Protocol: A protocol outlining measures to protect training data from compromise and its role in maintaining fairness. \n\nData Compromise Response and Fairness Assurance Plan: A plan detailing responses to data compromise that prioritizes fairness. \n\nProtection Measures and Bias Prevention Certifications: Certifications that the protection measures in place prevent bias and ensure fairness in training data. | Safety & Performance  Explanation: Training Data Security Protocol: A comprehensive document outlining the measures and practices in place to secure training data. \n\nIncident Response Plan: A predefined plan for responding to security incidents that may affect training data. \n\nData Access and Handling Training Certificates: Documentation showing that individuals with access to training data have been trained in data protection best practices. | Impact Explanation: Training Data Protection Plan: A document outlining the methods and processes used to secure training data against unauthorized access and compromise.\n\nIncident Response Plan: Detailed procedures for responding to security breaches involving training data.\n\nAccess Control Lists: Records detailing user permissions for accessing training data, ensuring only authorized individuals can access sensitive information."
    },
    {
        "control_name": "AI System & Data Protection - Training Data Protection Controls",
        "category": "AI Data Anonymization, Deidentification, & Sensitive Data Handling",
        "description": "Protect training data from unauthorized access or breaches.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: AC-3: Access Enforcement\nSC-28: Protection of Information at Rest\nMP-4: Media Storage\nSC-7: Boundary Protection, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Awareness of the security needs of AI training data to prevent unauthorized access or breaches.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls for AI training data protection.\nCRY-05: Encrypting Data At Rest - Encrypting AI training data at rest as a key control to prevent unauthorized access and breaches.\nDCH-01: Data Protection - Implementing measures to protect training data from unauthorized access or breaches within AI systems.\nDCH-06: Media Storage - Securely storing AI training data media to prevent unauthorized access or breaches.\nPRI-01: Data Privacy Program - Developing a data privacy program that includes specific measures for the protection of AI training data.\nIAC-20: Access Enforcement - Enforcing strict access controls to protect AI training data from unauthorized access or breaches.\nRSK-03: Risk Identification - Identifying risk to the security of AI training data and implementing controls to mitigate these risk.\nRSK-04: Risk Assessment - Assessing the risk and potential impacts of unauthorized access or breaches to AI training data.",
        "explainability": "The AI System and Data Protection - Training Data Protection Controls control is focused on implementing a set of controls and measures to protect training data used in AI systems. The rationale for this control is to ensure data security and integrity throughout the AI development process. It aims to prevent unauthorized access, breaches, and data compromise, which can lead to biased or unreliable models. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of training data protection controls and their role in ensuring data security, model integrity, and ethical AI. \n\nData Protection Control Framework: An overview of the control framework used to protect training data, including access restrictions, encryption, and monitoring. \n\nSecurity Assessment Reports: Reports showcasing the effectiveness of training data protection controls in safeguarding data and AI systems. | Responsibility Explanation: Access Control Policies: Policies defining who can access training data and under what circumstances. \n\nAuthorization Access Logs: Logs that record every access to training data, including the individual's identity and time of access. \n\nData Protection Training Records: Records of training provided to individuals on data protection controls and their responsibilities. | Data Explanation: Data Access and Breach Prevention Policies outline the control measures like multifactor authentication, role-based access, and intrusion detection systems. Documentation should include audit trails of access to training data and any breach attempts. | Fairness Explanation: Training Data Protection Framework: A comprehensive framework that outlines the protection controls for training data and their relevance to fairness. \n\nData Breach Impact on Fairness Report: Documents the impact a data breach could have on the fairness of AI outcomes. \n\nFair Data Access Guidelines: Guidelines ensuring that data access controls are in place to maintain fairness. | Safety & Performance  Explanation: Training Data Protection Policy: Formal policy detailing security controls for protecting training data. \n\nAccess Management Records: Documentation of who has access to training data, their roles, and access history. \n\nData Breach Response Plan: A clearly defined strategy for responding to potential breaches involving training data. | Impact Explanation: Data Protection Policy: A comprehensive document that outlines the organization's approach to protecting training data, including classification, handling, and storage protocols.\n\nEncryption Standards Documentation: Technical documents detailing the encryption methods used for protecting data at rest and in transit.\n\nTraining and Awareness Materials: Resources used to educate staff about data protection principles, responsibilities, and procedures."
    },
    {
        "control_name": "AI System & Data Protection",
        "category": "AI Data Anonymization, Deidentification, & Sensitive Data Handling",
        "description": "Prevent overfitting and reduce susceptibility to membership inference attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: SA-11: Developer Testing and Evaluation\nRA-5: Vulnerability Monitoring and Scanning\nSI-3: Malicious Code Protection\nSC-28: Protection of Information at Rest, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI models are developed with considerations for avoiding overfitting and reducing vulnerability to membership inference attacks.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining awareness of AI models' vulnerabilities, including risk of overfitting and membership inference attacks.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, including those addressing overfitting and susceptibility to attacks.\nDCH-01: Data Protection - Implementing data protection controls to secure training data, reducing the risk of overfitting and vulnerability to attacks.\nRSK-03: Risk Identification - Identifying risk associated with overfitting and membership inference attacks in AI models and planning appropriate controls.\nRSK-04: Risk Assessment - Assessing the likelihood and potential impact of overfitting and membership inference attacks on AI models.\nTDA-06: Secure Coding - Secure coding practices in AI development to prevent overfitting and protect against inference attacks.",
        "explainability": "The AI System and Data Protection control focuses on the overall protection of AI systems and their associated data. The rationale for this control is to ensure the security, integrity, and confidentiality of AI systems, their data, and their operation. It aims to prevent unauthorized access, breaches, and data compromise and maintain ethical AI. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of overall AI system and data protection and its role in safeguarding AI operations and ethical considerations. \n\nSecurity Protocols Overview: A nontechnical summary of the security protocols and measures used to protect AI systems and data. \n\nSecurity Compliance Reports: Reports demonstrating the AI system's adherence to security protocols and the absence of security breaches. | Responsibility Explanation: Overfitting Prevention Guidelines: Guidelines that describe measures to prevent overfitting and assign responsible roles. \n\nMembership Inference Protection Strategy: A strategy document for protecting against membership inference attacks, including assigned responsibilities. \n\nModel Validation and Testing Logs: Logs that document the validation and testing processes aimed at preventing overfitting and membership inference attacks. | Data Explanation: Overfitting Prevention and Membership Inference Protection Protocols that outline the methodologies and practices in place to prevent the model from overfitting and protect against membership inference attacks. This should include model validation techniques and data generalization strategies. | Fairness Explanation: Overfitting and Inference Attack Prevention Strategy: A strategy that outlines measures to prevent overfitting and inference attacks with a focus on fairness. \n\nFairness Assurance in Data Protection Report: A report on how data protection mechanisms ensure fairness by preventing overfitting and inference attacks. \n\nModel Fairness and Security Certification: A certification that attests to the model's fairness and security regarding data protection. | Safety & Performance  Explanation: Overfitting Prevention Policy: A document that outlines strategies for avoiding overfitting, such as regularization techniques or crossvalidation methods. \n\nModel Evaluation Reports: Regular reports that assess model performance and the likelihood of overfitting. \n\nMembership Inference Attack Preparedness Plan: A plan detailing defense mechanisms against membership inference attacks. | Impact Explanation: Overfitting Mitigation Report: Documentation on the strategies implemented to avoid overfitting, including techniques like crossvalidation, regularization, and dataset augmentation.\n\nMembership Inference Attack Analysis: A detailed analysis of potential vulnerabilities of the AI system to membership inference attacks and the steps taken to address them.\n\nPrivacy Impact Assessment (PIA): A report that assesses the privacy risk associated with the AI system, particularly in relation to data protection and mitigation measures against membership inference attacks."
    },
    {
        "control_name": "Privacy-First Data Handling",
        "category": "AI Data Anonymization, Deidentification, & Sensitive Data Handling",
        "description": "Ensure models handle data with privacy as a priority, especially when regulatory changes might impact operations.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: PM-18: Privacy Program Plan\nPM-20: Dissemination of Privacy Program Information\nPM-21: Accounting of Disclosures\nPM-22: Personally Identifiable Information Quality Management\nPM-25: Minimization of Personally Identifiable Information Used In Testing, Training, and Research\nPT-3: Personally Identifiable Information Processing Purposes\nPT-7:  Specific Categories of Personally Identifiable Information\nAC-4: Information Flow Enforcement\nSA-11: Developer Testing and Evaluation, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Developing AI models that are trustworthy and prioritize data privacy in their operations.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining situational awareness regarding privacy regulations and their impact on AI data handling.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Compliance with regulatory requirements, adapting data handling practices in AI models to align with changing regulations.\nCPL-03: Cybersecurity & Data Privacy Assessments - Regular assessments of AI models to ensure they continue to handle data with privacy as a priority, especially in the context of regulatory changes.\nDCH-23: De-Identification (Anonymization) - Utilizing de-identification and anonymization techniques in AI data handling to prioritize privacy.\nPRI-01: Data Privacy Program - Implementing a comprehensive data privacy program that ensures privacy-first data handling in AI models.\nPRI-02: Data Privacy Notice - Providing clear data privacy notices that reflect a commitment to privacy-first data handling in AI models.\nPRI-07: Information Sharing With Third Parties - Managing third-party information sharing in AI systems to uphold a privacy-first approach to data handling.\nRSK-03: Risk Identification - Identifying risk to data privacy in AI systems and ensuring models are designed to prioritize privacy.",
        "explainability": "Privacy-First Data Handling is a control that emphasizes the handling of data with a primary focus on privacy preservation. The rationale for this control is to prioritize individual privacy rights, minimize data exposure, and ensure that data is processed and stored in a manner that respects privacy regulations and ethical considerations. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of privacy-first data handling and its role in respecting privacy rights and ethical data management. \n\nPrivacy Data Handling Guidelines: Guidelines outlining how data is handled with a primary focus on privacy, including anonymization, minimal data collection, and secure storage. \n\nPrivacy Compliance Report: A report demonstrating the AI system's compliance with privacy regulations and ethical data handling practices. | Responsibility Explanation: Privacy-First Handling Policy: A policy document that mandates privacy-first principles in data handling and defines responsibilities. \n\nRegulatory Change Logs: Logs that track changes in privacy regulations and actions taken to comply. \n\nModel Privacy Compliance Reports: Reports evaluating the compliance of AI models with privacy regulations and principles. | Data Explanation: Privacy-First Data Handling Policy that outlines the data handling practices prioritizing user privacy. This policy should be aligned with current regulations and adaptable to changes, with clear procedures for compliance and data management. | Fairness Explanation: Privacy-Centric Data Handling Policy: A policy that prioritizes privacy in data handling and its implications for fairness. \n\nPrivacy Impact on Fairness Assessment: An assessment that evaluates the impact of privacy measures on the fairness of AI outcomes. \n\nPrivacy-First Operation Manuals: Operation manuals that detail procedures for handling data with privacy and fairness as core principles. | Safety & Performance  Explanation: Privacy-By-Design Framework: A framework document that embeds privacy into the design and architecture of AI systems. \n\nRegulatory Compliance Checklist: A checklist detailing privacy regulations applicable to AI operations and their implementation status. \n\nPrivacy Impact Assessment Report: A report that evaluates how personal data is protected and how privacy risk is managed. | Impact Explanation: Privacy-by-Design Framework: A comprehensive framework document that outlines the data handling processes, prioritizing privacy at each step of data collection, processing, and storage.\n\nRegulatory Compliance Reports: Detailed reports on how the AI systems adhere to current data protection regulations and any updates made in response to regulatory changes.\n\nData Flow Diagrams: Visual representations showing how data moves through the AI system, with privacy controls indicated at each stage."
    },
    {
        "control_name": "Differential Privacy",
        "category": "AI Data Anonymization, Deidentification, & Sensitive Data Handling",
        "description": "Ensure the model does not leak individual training data  information.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: SC-28: Protection of Information at Rest\nSA-11: Developer Testing and Evaluation\nRA-5: Vulnerability Monitoring and Scanning\nAC-3: Access Enforcement, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Developing trustworthy AI systems that incorporate differential privacy to protect individual data points.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining awareness of the importance of differential privacy in protecting individual data points in AI models.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, ensuring differential privacy is effectively implemented in AI systems.\nCRY-05: Encrypting Data At Rest - Using encryption for data at rest, complementing differential privacy measures in AI models.\nDCH-01: Data Protection - Robust data protection controls, including differential privacy, to safeguard individual data points in AI training datasets.\nDCH-23: De-Identification (Anonymization) - Implementing de-identification and anonymization techniques, including differential privacy, to protect individual data points in AI training datasets.\nPRI-01: Data Privacy Program - Developing and implementing a data privacy program that includes differential privacy measures to protect training data.\nPRI-07: Information Sharing With Third Parties - Controlling third-party information sharing to ensure that differential privacy standards are maintained in AI systems.\nRSK-03: Risk Identification - Identifying risk associated with data leaks in AI models and implementing differential privacy as a mitigation strategy.",
        "explainability": "Differential Privacy is a control that focuses on protecting the privacy of individual data contributors while still allowing useful insights to be extracted from aggregated data. The rationale for this control is to strike a balance between data utility and privacy, ensuring that individuals' sensitive information is not compromised during data analysis. This explanation should be delivered in an accessible and nontechnical  way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of differential privacy and its role in preserving individual privacy while enabling data analysis. \n\nDifferential Privacy Guidelines: Guidelines outlining the principles and techniques used to achieve differential privacy in data handling and analysis. \n\nPrivacy Preservation Reports: Reports demonstrating the effectiveness of differential privacy mechanisms in protecting individual data privacy. | Responsibility Explanation: Differential Privacy Implementation Plan: A document outlining the approach, tools, and responsible roles for implementing differential privacy. \n\nPrivacy Impact Assessments (PIA): Assessments conducted pre- and\npost-implementation to ensure no individual data leakage occurs. \n\nCompliance Monitoring Reports: Reports on the ongoing monitoring of differential privacy measures and the individuals responsible for this monitoring. | Data Explanation: Differential Privacy Implementation Plan, which includes the techniques and parameters used to apply differential privacy to the model. It should document the privacy budget and how it is allocated across queries and data points. | Fairness Explanation: Differential Privacy Implementation Guide: A guide detailing the implementation of differential privacy and its role in supporting fairness. \n\nDifferential Privacy and Fairness Assurance Report: A report that verifies the effectiveness of differential privacy in maintaining fairness. \n\nIndividual Data Point Leakage Prevention Protocol: A protocol that prevents individual data point leakage, ensuring fairness in data usage. | Safety & Performance  Explanation: Differential Privacy Implementation Plan: A comprehensive guide on how differential privacy is integrated into the AI system.\n \nPrivacy Guarantee Certificates: Documents that certify the privacy guarantees of the model based on differential privacy standards.\n \nData Anonymization Procedures: Detailed procedures on how data is anonymized before being used in training. | Impact Explanation: Differential Privacy Implementation Plan: A document outlining the techniques and measures taken to implement differential privacy in the AI system.\n\nPrivacy Guarantee Proofs: Mathematical proofs or statistical analyses that demonstrate the privacy guarantees of the differential privacy model.\n\nData Anonymization Records: Records of data processing activities showing how individual data points have been anonymized or aggregated to prevent identification."
    },
    {
        "control_name": "Privacy-Enhancing Technology Integration",
        "category": "Privacy-Enhancing Technologies",
        "description": "Integrate technologies that enhance user privacy into AI systems, ensuring that data is protected and privacy standards are met or exceeded.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: SC-28: Protection of Information at Rest\nAC-24: Access Control Decisions\nCA-9: Internal System Connections\nAC-4: Information Flow Enforcement\nCM-12: Information Location\nPL-8: Security and Privacy Architecture\nPM-17: Protecting Controlled Unclassified Information on External Systems\nPM-25: Minimization of Personally Identifiable Information Used In Testing, Training, and Research, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Developing trustworthy AI systems that incorporate privacy-enhancing technologies.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining situational awareness about the role and importance of privacy-enhancing technologies in AI systems.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, including privacy-enhancing technology integration.\nCRY-05: Encrypting Data At Rest - Encryption of data at rest as a key privacy-enhancing technology in AI systems.\nDCH-23: De-Identification (Anonymization) - Utilizing de-identification and anonymization as privacy-enhancing technologies in AI systems.\nPRI-01: Data Privacy Program - Implementing a data privacy program that integrates privacy-enhancing technologies into AI systems.\nPRI-07: Information Sharing With Third Parties - Managing third-party information sharing in alignment with privacy-enhancing technology standards in AI systems.\nRSK-03: Risk Identification - Identifying risk that can be mitigated through the integration of privacy-enhancing technologies in AI systems.\nTDA-04: Documentation Requirements - Documenting the integration and effectiveness of privacy-enhancing technologies in AI systems.",
        "explainability": "Privacy-Enhancing Technology Integration is a control that focuses on the integration of technologies and practices that enhance privacy within AI systems. The rationale for this control is to prioritize individual privacy, minimize data exposure, and ensure the ethical handling of data in AI systems. It aims to protect sensitive information and ensure compliance with privacy regulations. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of privacy-enhancing technology integration and its role in protecting individual privacy and ethical data handling. \n\nTechnology Integration Guidelines: Guidelines outlining the technologies and practices used to enhance privacy within AI systems. \n\nPrivacy Compliance Report: A report demonstrating the AI system's compliance with privacy regulations and the effectiveness of privacy-enhancing technology integration. | Responsibility Explanation: PET Integration Plan: A detailed plan for integrating PETs into AI systems, including roles and responsibilities. \n\nTechnology Audit Trails: Logs that record all PET integration activities, identifying who performed each action. \n\nPET Efficacy Reports: Reports assessing the effectiveness of the integrated PETs in protecting user privacy. | Data Explanation: Privacy-Enhancing Technology Strategy document that identifies the privacy-enhancing technologies (PETs) used, their implementation within the AI system, and how they contribute to meeting or exceeding privacy standards. | Fairness Explanation: Privacy Technology Integration Plan: A plan that outlines the integration of privacy-enhancing technologies and their significance for fairness. \n\nEnhanced Privacy and Fairness Protocol: A protocol that ensures enhanced privacy measures contribute to the fairness of AI systems.\n \nPrivacy Technology and Fairness Compliance Report: A report on compliance with privacy technology standards and their impact on fairness. | Safety & Performance  Explanation: Privacy-Enhancing Technology (PET) Strategy Document: A detailed plan outlining the integration of PETs in the AI system’s design.\n \nPET Implementation Reports: Regular reports on the status and effectiveness of the PETs in use. \n\nCompliance with Privacy Standards Documentation: Proof of meeting, or exceeding, privacy standards through the use of PETs. | Impact Explanation: PETs Integration Plan: A detailed strategy document that outlines how various PETs will be integrated into the AI system.\n\nTechnology Impact Assessments: Assessments that document how each PET improves privacy and the measures taken to ensure their effectiveness.\n\nCompliance Certifications: Certificates and documentation that prove the AI system meets or exceeds privacy regulations due to the integrated PETs."
    },
    {
        "control_name": "Joint Investigation for AI Infringements",
        "category": "Vulnerability & Threat Management",
        "description": "Initiate a joint investigation to assess the impact and determine corrective actions in the event of a suspected infringement by a high-risk AI system affecting a significant number of individuals across multiple member states.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 20: Corrective Actions and Duty of Information\nArticle 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models, NIST 800-53: IR-4: Incident Handling\nAU-6: Audit Record Review, Analysis, and Reporting\nRA-3: Risk Assessment\nCP-2: Contingency Plan, SCF: IRO-02: Incident Handling - Handling AI-related security incidents, including coordination for joint investigations in multi-jurisdictional contexts.\nIRO-04: Incident Response Plan (IRP) - Developing an IRP that includes protocols for initiating joint investigations in cases of AI infringements.\nIRO-07: Integrated Security Incident Response Team (ISIRT) - Establishing an ISIRT that can collaborate with entities across multiple Member States for AI incident investigations.\nIRO-09: Situational Awareness For Incidents - Maintaining situational awareness to effectively participate in joint investigations of AI infringements.\nIRO-11: Incident Reporting Assistance - Providing assistance and coordination for reporting and investigating AI-related incidents across different jurisdictions.\nIRO-12: Information Spillage Response - Responding to information spillage incidents in AI systems, which can be part of the joint investigation process.\nIRO-13: Root Cause Analysis (RCA) & Lessons Learned - Conducting RCA and sharing lessons learned as part of the joint investigation process in AI system infringements.\nIRO-14: Regulatory & Law Enforcement Contacts - Establishing contacts with regulatory and law enforcement agencies for collaboration in joint AI infringement investigations.\nRSK-04: Risk Assessment - Assessing the risk associated with AI systems to prepare for potential joint investigations of infringements.\nTHR-07: Threat Hunting - Proactively searching for potential AI system infringements that may trigger joint investigations.",
        "explainability": "Joint Investigation for AI Infringements is a control that emphasizes collaborative investigations for identifying and addressing infringements or violations related to AI systems. The rationale for this control is to enhance accountability, transparency, and regulatory compliance in AI, as it ensures that potential issues or infringements are thoroughly investigated by multiple stakeholders. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of joint investigations for AI infringements and their role in ensuring accountability, transparency, and compliance in AI systems.\n \nInvestigation Framework: A framework outlining the processes and procedures for conducting joint investigations for AI infringements. \n\nInvestigation Outcome Reports: Reports demonstrating the effectiveness of joint investigations and the resolution of AI infringements. | Responsibility Explanation: Joint Investigation Framework: A framework detailing the procedure for initiating a joint investigation, including roles and collaboration protocols between Member States. \n\nInvestigation Task Force Roster: A list of individuals and their roles within the joint investigation task force. \n\nInvestigation Reports: Comprehensive reports documenting the findings of the joint investigation and recommended corrective actions. | Data Explanation: Joint Investigation Procedures and Coordination Plan, detailing the steps for initiating, conducting, and concluding a joint investigation. This should include the roles and responsibilities of each member state's investigative body, communication protocols, and documentation standards. | Fairness Explanation: Joint Investigation Fairness Framework: A framework that outlines the approach for joint investigations with an emphasis on fairness. \n\nAI Infringement Impact and Fairness Report: A report detailing the investigation's findings on the infringement's impact on fairness. \n\nCrossborder Fairness Response Protocol: A protocol that guides the response to AI infringements affecting fairness in multiple member states. | Safety & Performance  Explanation: Deliverables would include a Joint Investigation Framework detailing the collaborative process among member states for investigating AI infringements, an Investigation Report summarizing the findings, an impact assessment, and recommended corrective actions. Additionally, a Corrective Action Plan would be developed based on the investigation's outcomes. | Impact Explanation: Joint Investigation Protocol: A formalized document outlining the steps for initiating and conducting a joint investigation across member states.\n\nImpact Assessment Report: A comprehensive analysis that details the infringement's effects on individuals and proposes mitigation strategies.\n\nCorrective Action Plan: A strategic plan developed to address identified issues and prevent future occurrences, including timelines and responsibilities."
    },
    {
        "control_name": "Prompt Notification of AI Infringements",
        "category": "Vulnerability & Threat Management",
        "description": "Ensure providers of high-risk AI systems are required to promptly notify distributors, importers, national competent authorities, and, when feasible, deployers in the event of an infringement that has a harmful impact on a significant number of individuals.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 20: Corrective Actions and Duty of Information\nArticle 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models, NIST 800-53: IR-6: Incident Reporting\nAU-6: Audit Record Review, Analysis, and Reporting\nCP-2: Contingency Plan\nRA-5: Vulnerability Monitoring and Scanning, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with regulations that mandate prompt notification of AI system infringements.\nIRO-01: Incident Response Operations - Operational practices for responding to AI incidents, including timely notification of relevant parties.\nIRO-02: Incident Handling - Handling incidents in AI systems with a focus on the prompt notification of all relevant parties.\nIRO-04: Incident Response Plan (IRP) - Developing an incident response plan that includes prompt notification procedures for AI infringements.\nIRO-10: Incident Stakeholder Reporting - Procedures for reporting AI system infringements to stakeholders including distributors, importers, and authorities.\nIRO-11: Incident Reporting Assistance - Providing assistance in the incident reporting process, ensuring prompt notification of AI infringements.\nIRO-14: Regulatory & Law Enforcement Contacts - Establishing necessary contacts for the prompt notification of AI infringements to regulatory and law enforcement agencies.\nRSK-03: Risk Identification - Identifying risk that may lead to AI system infringements requiring prompt notification.\nTHR-07: Threat Hunting - Proactively identifying potential infringements in high-risk AI systems that necessitate prompt notification.",
        "explainability": "Prompt Notification of AI Infringements is a control that emphasizes the timely notification of infringements, breaches, or issues related to AI systems. The rationale for this control is to ensure transparency, accountability, and rapid response to AI-related problems. It helps in addressing and rectifying issues in a timely manner to mitigate potential harm and uphold trust in AI. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of prompt notification of AI infringements and its role in ensuring transparency, accountability, and rapid issue resolution. \n\nNotification Protocol: A protocol outlining the procedures and timelines for notifying relevant stakeholders about AI infringements or breaches. \n\nNotification Compliance Reports: Reports demonstrating the effectiveness of the notification process and the resolution of AI infringements. | Responsibility Explanation: Infringement Notification Protocol: A protocol detailing the steps for promptly notifying all relevant parties in the event of an infringement. \n\nNotification Logs: Logs that record all notifications sent, when they were sent, and by whom. \n\nStakeholder Response Plan: A plan outlining how stakeholders should respond to notifications of AI infringements. | Data Explanation: Notification Protocol for AI Infringements, which specifies the procedures for issuing alerts about infringements. This should include templates for notification messages, timelines for notifications, and a list of stakeholders to be informed. | Fairness Explanation: Infringement Notification Procedure: A document outlining the steps for prompt notification in case of infringements, emphasizing the preservation of fairness. \n\nFairness Impact Notification Report: A report detailing the infringement's impact on fairness and the promptness of notifications. \n\nStakeholder Communication Logs: Logs indicating when and how stakeholders are notified about infringements, ensuring a fair and transparent process. | Safety & Performance  Explanation: Incident Response Plan: A document detailing the procedures for responding to AI-related infringements, including notification processes. \n\nNotification Records: Documented evidence of notifications sent to relevant parties following an infringement.\n \nRegulatory Report Forms: Standardized forms for reporting infringements to national competent authorities. | Impact Explanation: Infringement Notification Protocol: A document that details the procedure for alerting relevant parties about an infringement.\n\nCommunication Logs: Records of all notifications sent to stakeholders, including timestamps and the nature of the infringement.\n\nRegulatory Notification Compliance Report: Documentation demonstrating adherence to notification timelines and regulations."
    },
    {
        "control_name": "Incident Notification Transfer",
        "category": "Vulnerability & Threat Management",
        "description": "Communicate notifications related to serious incidents involving certain high-risk AI systems to the appropriate supervisory authority for further action and oversight.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 20: Corrective Actions and Duty of Information\nArticle 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models, NIST 800-53: IR-6: Incident Reporting\nAU-6: Audit Record Review, Analysis, and Reporting\nRA-5: Vulnerability Monitoring and Scanning\nPM-16: Threat Awareness Program, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Compliance with regulations that require the transfer of incident notifications to supervisory authorities.\nIRO-01: Incident Response Operations - Operational practices in incident response, including the process of notification transfer to national authorities.\nIRO-02: Incident Handling - Effective handling of AI system incidents, including the protocol for notification transfer to national authorities.\nIRO-04: Incident Response Plan (IRP) - Developing an incident response plan that includes procedures for transferring notifications of serious AI incidents to national authorities.\nIRO-10: Incident Stakeholder Reporting - Reporting mechanisms for serious AI incidents, including the transfer of notifications to national supervisory authorities.\nIRO-11: Incident Reporting Assistance - Providing assistance in the incident reporting process, particularly in transferring notifications to appropriate authorities.\nIRO-14: Regulatory & Law Enforcement Contacts - Establishing and maintaining contacts with national supervisory authorities for incident notification transfer.\nRSK-03: Risk Identification - Identifying risk that may lead to incidents requiring notification transfer to supervisory authorities.\nRSK-04: Risk Assessment - Assessing the impact of AI system incidents and determining when notification transfer to supervisory authorities is necessary.",
        "explainability": "Incident Notification Transfer is a control that emphasizes the efficient and accurate transfer of incident notifications related to AI systems. The rationale for this control is to ensure that incident information is promptly and reliably conveyed to relevant parties, enabling swift response and resolution. This control is crucial for maintaining transparency, accountability, and trust in AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of incident notification transfer and its role in ensuring transparency, accountability, and rapid incident response. \n\nNotification Transfer Protocol: A protocol outlining the procedures and methods for transferring incident notifications to relevant stakeholders. \n\nIncident Notification Assessment Reports: Reports demonstrating the effectiveness of the notification transfer process and the timely response to AI incidents. | Responsibility Explanation: Notification Transfer Procedure: A document outlining the procedure for transferring incident notifications to the national supervisory authority. \n\nTransfer Logs: Logs that track the details of the notification transfer, including the personnel responsible for the transfer. \n\nAuthority Response Tracking Records: Records that track the national supervisory authority's responses and actions following a notification transfer. | Data Explanation: Incident Notification Transfer Guidelines that outline the process for escalating notifications to the supervisory authority. This includes criteria for what constitutes a \"serious incident,\" the information to be transferred, and the protocol for communication. | Fairness Explanation: Incident Notification Transfer Guidelines: Guidelines to ensure the fair transfer of incident notifications to the appropriate authorities. \n\nAuthority Notification Fairness Compliance Report: A compliance report ensuring that notification transfers meet fairness and regulatory requirements. \n\nSupervisory Authority Communication Records: Records of communication with supervisory authorities regarding incident notifications, emphasizing fairness. | Safety & Performance  Explanation: Incident Notification Protocol: A document outlining the process for transferring incident notifications to the supervisory authority. \n\nSupervisory Authority Reporting Templates: Predefined templates for reporting incidents to the national supervisory authority efficiently. \n\nRecord of Incident Notifications: A log that records all the incident notifications transferred, including dates, times, and details of the incident. | Impact Explanation: Incident Report Form: A standardized form for documenting serious incidents, which includes details on the nature, impact, and initial response.\n\nIncident Logbook: A record of all serious incidents that have occurred, including dates, details, and status of the transfer to the national authority.\n\nTransfer Confirmation Receipts: Acknowledgments from the national supervisory authority that they have received the notification of the incident."
    },
    {
        "control_name": "Tamper-Evident Logs",
        "category": "Security Logging & Monitoring",
        "description": "Maintain tamper-evident logs of input and output interactions to detect and respond to output integrity attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nRecital 133: Emphasizes the need to embed technical solution to reduce risk of manipulation, fraud, impersonation and consumer deception., NIST 800-53: AU-9: Protection of Audit Information\nSI-7: Software, Firmware, and Information Integrity\nAU-6: Audit Record Review, Analysis, and Reporting\nSC-7: Boundary Protection, SCF: MON-01: Continuous Monitoring - Implementing continuous monitoring practices that include maintaining tamper-evident logs for AI systems.\nMON-01.7: File Integrity Monitoring (FIM) - Using FIM tools to ensure the integrity of logs and detect any tampering in AI systems.\nMON-02: Centralized Collection of Security Event Logs - Centralizing the collection of security event logs, ensuring they are tamper-evident and reliable.\nMON-03: Content of Event Logs - Ensuring the event logs for AI systems are tamper-evident, recording all critical input and output interactions.\nMON-04: Event Log Storage Capacity - Managing the storage capacity of event logs to ensure that they remain tamper-evident and effective for security monitoring.\nMON-05: Response To Event Log Processing Failures - Responding effectively to any failures in event log processing, maintaining their tamper-evident nature.\nMON-08: Protection of Event Logs - Protecting event logs from tampering to maintain the integrity of input and output interaction records.",
        "explainability": "Tamper-Evident Logs is a control that focuses on maintaining logs that are resistant to tampering or unauthorized alterations. The rationale for this control is to ensure the integrity and reliability of logs, which are critical for monitoring and auditing AI systems. Tamper-evident logs help in detecting and deterring unauthorized activities, thereby enhancing accountability and transparency. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of tamper-evident logs and their role in ensuring log integrity, accountability, and transparency. \n\nTamper-Evident Logging Guidelines: Guidelines outlining the principles and techniques used to create and maintain tamper-evident logs. \n\nLog Integrity Assessment Reports: Reports demonstrating the effectiveness of tamper-evident logging practices in maintaining log integrity. | Responsibility Explanation: Tamper-Evident Logging Policy: A policy that defines the creation, maintenance, and review of tamper-evident logs, including assigned responsibilities. \n\nLog Integrity Reports: Regular reports that document the integrity of the logs and any tampering incidents. \n\nTampering Incident Response Plan: A plan that specifies the steps and responsible individuals to take in response to detected tampering. | Data Explanation: Tamper-Evident Logging System specifications and maintenance protocols. This includes the implementation of logging mechanisms that can detect and flag any unauthorized alterations, with a detailed incident response plan for potential integrity attacks. | Fairness Explanation: Tamper-Evident Log Policy: A policy detailing the maintenance of tamper-evident logs to uphold fairness. \n\nOutput Integrity and Fairness Report: A report that connects the integrity of outputs with the fairness of AI system decisions. \n\nLog Tampering Incident Reports: Incident reports that document any tampering and its impact on fairness. | Safety & Performance  Explanation: Tamper-Evident Logging System Documentation: Describes the implemented system that ensures logs are tamper-evident. \n\nAudit Trail Reports: Generated reports that provide evidence of the integrity of logs over time. \n\nSystem Security Policies: Outlines the policies in place that enforce and maintain the security of logging systems. | Impact Explanation: Log Management Policy: A document detailing procedures for log collection, monitoring, and protection.\n\nTamper-Proof Log Software Documentation: Technical specifications and user guides for the software ensuring log integrity.\n\nAudit Trail Records: Records that capture all changes to logs, providing a traceable history of interactions and modifications."
    },
    {
        "control_name": "AI-Specific Cybersecurity Protocols",
        "category": "AI-Specific Network Security",
        "description": "Adopt a \"security by design and by default\" approach for high-risk AI systems. This involves integrating security measures from the initial design phase, ensuring accuracy, robustness, safety, and cybersecurity throughout the system's life cycle.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 11: Technical Documentation\nArticle 15: Accuracy, robustness and cybersecurity, NIST 800-53: SA-3: System Development Life Cycle\nSA-8: Security and Privacy Engineering Principles\nSC-3: Security Function Isolation\nSI-2: Flaw Remediation, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI systems are trustworthy, integrating security measures from the design phase.\nRSK-03: Risk Identification - Identifying risk specific to AI systems and addressing them from the design phase onward.\nSEA-01: Secure Engineering Principles - Applying secure engineering principles from the outset in the design of high-risk AI systems.\nSEA-03: Defense-In-Depth (DiD) Architecture - Implementing a defense-in-depth architecture in AI systems as part of a holistic security strategy.\nSEA-07: Predictable Failure Analysis - Analyzing and preparing for predictable failures in AI systems to enhance robustness and safety.\nTDA-04: Documentation Requirements - Documenting the integration of security measures in AI systems as part of the design and development process.\nTDA-06: Secure Coding - Emphasizing secure coding practices as part of the 'security by design' approach in AI system development.",
        "explainability": "AI-Specific Cybersecurity Protocols is a control that emphasizes the development and implementation of specialized cybersecurity protocols tailored to the unique vulnerabilities and threats associated with AI systems. The rationale for this control is to ensure the security and resilience of AI systems against cyber threats, cyberthreats, data breaches, and other security risk. risk. AI-specific protocols are essential for maintaining the integrity, availability, and confidentiality of AI operations. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of AI-specific cybersecurity protocols and their role in safeguarding AI systems from cyber threats and breaches. \n\nAI Cybersecurity Protocol Guidelines: Guidelines outlining the AI-specific cybersecurity measures and best practices to be implemented. \n\nCybersecurity Assessment Reports: Reports demonstrating the effectiveness of AI-specific cybersecurity protocols in protecting AI systems. | Responsibility Explanation: Cybersecurity Integration Plan: A detailed plan for incorporating security measures into the AI system from the start. \n\nLife Cycle Security Audit Trails: Logs that record security measures and checks throughout the AI system’s lifecycle. life cycle.\n\nSystem Security Certifications: Documentation of security certifications the AI system has received. | Data Explanation: AI Cybersecurity Policy Document that outlines the 'security by design' principles, detailing the security measures integrated into the AI system at each stage of its development and deployment. This should include risk assessments, threat modeling outcomes, and security feature descriptions. | Fairness Explanation: AI Cybersecurity and Fairness Framework: A framework that integrates cybersecurity measures with fairness principles. \n\nSecurity by Design Fairness Protocol: A protocol that ensures security measures contribute to rather than detract from fairness. \n\nLife Cycle Fairness and Security Report: A report that reviews the AI system's security features in relation to their impact on fairness throughout the system's lifecycle. life cycle. | Safety & Performance  Explanation: Cybersecurity Framework Document: A comprehensive strategy that defines how security is embedded in the design and default settings of the AI system. \n\nSecurity Design Review Reports: Documentation from reviews that assess the security measures integrated into the AI system at the design stage. \n\nLife Cycle Security Plan: A detailed plan that outlines ongoing security measures throughout the system’s lifecycle. life cycle. | Impact Explanation: Security Design Documentation: Detailed plans and blueprints illustrating how security is embedded in the system's architecture.\n\nSecurity Risk Assessment Reports: Comprehensive analysis of potential security risk and how they are mitigated within the AI system.\n\nCybersecurity Testing Results: Records of all cybersecurity tests conducted, including penetration testing, vulnerability assessments, and security audits."
    },
    {
        "control_name": "Dynamic Threat Modeling",
        "category": "AI-Specific Network Security",
        "description": "Implement rigorous testing protocols for high-risk AI systems before market release. This includes evaluating the system against predefined metrics and thresholds. Testing should account for both the system's intended use and potential foreseeable misuse scenarios.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-11: Developer Testing and Evaluation\nCA-2: Control Assessments\nRA-3: Risk Assessment\nSA-3: System Development Life Cycle, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensuring AI systems are developed with trustworthiness, incorporating dynamic threat modeling in pre-market testing.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining awareness of the evolving threat landscape for AI systems, informing dynamic threat modeling.\nRSK-03: Risk Identification - Identifying potential risk in AI systems, including those that may arise from foreseeable misuse scenarios.\nRSK-04: Risk Assessment - Assessing risk related to AI systems, including potential misuse, as part of the testing protocol before market release.\nSEA-07: Predictable Failure Analysis - Analyzing and preparing for predictable failures and misuse scenarios in AI systems through rigorous testing.\nTDA-06: Secure Coding - Implementing secure coding practices that are verified through rigorous testing and dynamic threat modeling.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Conducting comprehensive testing of AI systems during development, including dynamic threat modeling and evaluation against specific metrics.\nTDA-09.5: Application Penetration Testing - Conducting penetration testing to evaluate AI systems against threats and predefined metrics.",
        "explainability": "Dynamic Threat Modeling is a control that emphasizes the continuous assessment and modeling of threats to AI systems. The rationale for this control is to adapt and respond to evolving security threats, vulnerabilities, and risk that may affect AI systems. Dynamic threat modeling is crucial for maintaining the security and resilience of AI operations. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of dynamic threat modeling and its role in adapting to evolving security threats and vulnerabilities in AI systems. \n\nThreat Modeling Guidelines: Guidelines outlining the process of dynamic threat modeling and its implementation.\n \nThreat Assessment Reports: Reports demonstrating the effectiveness of dynamic threat modeling in identifying and mitigating threats to AI systems. | Responsibility Explanation: Threat Modeling Protocol: A protocol that outlines the threat modeling and testing procedures, including roles and responsibilities. \n\nPremarket Testing Reports: Reports documenting the results of premarket testing against dynamic threat models. \n\nMisuse Scenario Planning Documents: Documents outlining potential misuse scenarios and the corresponding tests conducted. | Data Explanation: Testing Protocol Documentation that details the procedures, metrics, and thresholds used in evaluating the AI system. It should also include misuse case analysis reports and the risk mitigation strategies developed from the testing outcomes. | Fairness Explanation: Threat Modeling and Fairness Assessment Plan: A plan that outlines how threat modeling incorporates fairness testing. \n\nDynamic Testing Fairness Report: A report on dynamic testing results and their implications for fairness. \n\nThreat Model Fairness Certification: Certification that the AI system's threat model has been tested and is compliant with fairness standards. | Safety & Performance  Explanation: Threat Modeling Reports: Detailed documents outlining potential security threats and the results of tests conducted to assess the system's responses to these threats. \n\nTesting Protocol Documentation: A description of the testing procedures, including metrics and thresholds used for evaluating the AI system’s security. \n\nForeseeable Misuse Cases: A collection of potential misuse scenarios and the measures taken to prevent them. | Impact Explanation: Threat Model Reports: Documents outlining identified threats, potential impacts, and mitigation strategies.\n\nPredefined Metrics and Thresholds Documentation: Clear definitions of the metrics and thresholds against which the AI system is evaluated.\n\nTest Plans and Results: Detailed descriptions of testing protocols and the results obtained, indicating the system's response to various threat scenarios."
    },
    {
        "control_name": "Automated Event Logging",
        "category": "Security Logging & Monitoring",
        "description": "Implement strict access controls for automatically generated logs of high-risk AI systems. Provide access upon reasoned requests, ensuring only authorized personnel can view or modify logs.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 12: Record-Keeping\nArticle 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AU-9: Protection of Audit Information\nAC-3: Access Enforcement\nAU-6: Audit Record Review, Analysis, and Reporting\nAU-11: Audit Record Retention, SCF: MON-01: Continuous Monitoring - Implementing continuous monitoring with automated event logging and strict access controls for high-risk AI systems.\nMON-02: Centralized Collection of Security Event Logs - Centralizing the collection of security event logs and implementing access controls for review and modification.\nMON-03: Content of Event Logs - Managing the content of event logs in AI systems and ensuring access is restricted to authorized individuals.\nMON-04: Event Log Storage Capacity - Managing the storage of event logs, with access controls to ensure data security and privacy.\nMON-05: Response To Event Log Processing Failures - Establishing procedures to respond to event log processing failures, ensuring log integrity and access control.\nMON-08: Protection of Event Logs - Protecting event logs from unauthorized access and modification, in line with strict access control policies.\nIAC-15: Account Management - Managing accounts with access to automated event logs, ensuring only authorized personnel have access.\nIAC-20: Access Enforcement - Enforcing access controls to ensure only authorized personnel can view or modify automated logs in AI systems.",
        "explainability": "Automated Event Logging is a control that emphasizes the automated recording of events and activities in AI systems. The rationale for this control is to maintain a comprehensive record of AI operations, actions, and interactions for monitoring, auditing, and troubleshooting. Automated event logging enhances transparency, accountability, and the ability to detect and respond to incidents and anomalies. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of automated event logging and its role in maintaining comprehensive records for monitoring and auditing AI systems. \n\nEvent Logging Guidelines: Guidelines outlining the processes and protocols for automated event logging in AI systems. \n\nLogging Effectiveness Reports: Reports demonstrating the effectiveness of automated event logging in maintaining comprehensive records and facilitating incident response. | Responsibility Explanation: Access Control Policy for Logs: A policy outlining who can access the automated logs and under what conditions. \n\nAccess Request & Grant Logs: Logs that record all requests for access to the automated logs and the decisions made. \n\nLog Management Training Materials: Materials used to train responsible individuals on managing and controlling access to logs. | Data Explanation: Access Control Policy for Event Logs, which outlines the procedures for granting access to the logs, the roles authorized to view or modify the logs, and the safeguards in place to prevent unauthorized access or modifications. | Fairness Explanation: Event Logging Access Control Policy: A policy that governs access to automated event logs with fairness in mind. \n\nAuthorized Access Fairness Log: A log that records who has accessed the logs and the fairness implications of such access. \n\nEvent Log Fairness Audit Trail: An audit trail that ensures any modifications to logs are fair and justified. | Safety & Performance  Explanation: Access Control Policy Document: A document detailing who has access to the logs, under what circumstances, and how access is controlled. \n\nEvent Logging System Documentation: Technical documentation of the logging system describing how events are logged automatically. \n\nAccess Logs: Records of who accessed the logs, when, and for what purpose. | Impact Explanation: Access Control Policies: Detailed documents specifying who is authorized to access logs, under what circumstances, and how access is controlled and logged.\n\nLog Management Procedures: Procedures outlining how event logs are captured, stored, protected, and reviewed.\n\nLog Access and Modification Records: Auditable records that capture every instance of log access or modification, by whom, and when."
    },
    {
        "control_name": "Event Log Access & Retention",
        "category": "Security Logging & Monitoring",
        "description": "Retain logs generated by high-risk AI systems for a minimum of six months. Implement automated archival solutions to ensure older logs are securely stored while enabling easy retrieval when needed.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 12: Record-Keeping\nArticle 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AU-11: Audit Record Retention\nAU-9: Protection of Audit Information\nSC-13: Cryptographic Protection\nAU-6: Audit Record Review, Analysis, and Reporting, SCF:  MON-01: Continuous Monitoring - Continuous monitoring practices that include the retention of logs for at least 6 months and secure archival of older logs.\nMON-02: Centralized Collection of Security Event Logs - Centralizing the collection of logs with mechanisms for retention and archival.\nMON-03: Content of Event Logs - Ensuring the appropriate content of event logs is retained for the required duration and securely archived.\nMON-04: Event Log Storage Capacity - Managing the storage capacity of logs and implementing automated solutions for archiving logs beyond 6 months.\nMON-05: Response To Event Log Processing Failures - Procedures to respond to event log processing failures, including during the archival process.\nMON-07: Time Stamps - Ensuring accurate time stamps on logs to support the 6-month retention requirement and facilitate effective archival.\nMON-08: Protection of Event Logs - Protecting event logs against unauthorized access, ensuring security during storage and archival.\nDCH-18: Media & Data Retention - Policies for retaining media and data, including logs, for at least 6 months and securely archiving them thereafter.",
        "explainability": "Event Log Access Control is a control that focuses on controlling and securing access to logs and event records in AI systems. The rationale for this control is to prevent unauthorized access to sensitive logs, maintain data privacy, and ensure that only authorized personnel can view and manage event records. Log access control enhances data security, privacy, and accountability. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of log access control and its role in preventing unauthorized access to sensitive logs and event records. \n\nLog Access Control Guidelines: Guidelines outlining the principles and practices for controlling and securing log access in AI systems. \n\nAccess Control Effectiveness Reports: Reports demonstrating the effectiveness of log access control in protecting log data and ensuring accountability. | Responsibility Explanation: Log Retention & Archival Policy: A policy detailing log retention timeframes, archival processes, and responsible roles. \n\nArchival System Records: Records of the automated archival system's activities and any access to older logs. \n\nLog Retrieval Procedures: Documented procedures for retrieving logs, with a clear delineation of roles and access rights. | Data Explanation: Log Retention Policy and Archival Process documents, which specify the duration of log retention, the technology used for secure storage, and the retrieval process. This should include details on the encryption of archived logs and access controls for retrieval. | Fairness Explanation: Log Retention and Fairness Policy: A policy that outlines log retention practices and their significance for maintaining fairness. \n\nSecure Archival Fairness Procedures: Procedures for securely archiving logs in a way that supports fairness. \n\nLog Retrieval and Fairness Audit Reports: Reports detailing how logs are retrieved for audits and how this process upholds fairness. | Safety & Performance  Explanation: Log Retention Policy Document: A clear policy defining the duration for log retention and the procedures for archiving and retrieval. \n\nAutomated Archival System Specifications: Detailed descriptions of the systems used to automate log archival, ensuring security and retrievability. \n\nArchival Integrity Reports: Regular reports that verify the integrity and accessibility of archived logs. | Impact Explanation: Log Retention Policy: A formal document detailing the duration for log retention, the criteria for archival, and methods for secure storage.\n\nAutomated Archival System Design Documents: Technical specifications and designs of the automated systems used for log archival and retrieval.\n\nCompliance and Audit Reports: Reports confirming adherence to log retention policies and regulatory requirements."
    },
    {
        "control_name": "High-Risk Monitoring Log Retention",
        "category": "Security Logging & Monitoring",
        "description": "Integrate automated log analysis tools to ensure compliance with monitoring regulatory requirements (e.g., EU AI Law Article 20). Set alerts for noncompliant activities or anomalies in high-risk AI system operations.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 12: Record-Keeping\nArticle 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nSI-4: System Monitoring\nAU-12: Audit Record Generation\nCM-6: Configuration Settings, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with regulations like Article 20 through effective log analysis and monitoring.\nMON-01: Continuous Monitoring - Implementing continuous monitoring with automated log analysis tools to detect non-compliant activities in AI systems.\nMON-01.12: Automated Alerts - Setting up automated alerts for detecting non-compliant activities or anomalies in AI system operations.\nMON-01.2: Automated Tools for Real-Time Analysis - Using automated tools for real-time analysis of logs to ensure regulatory compliance and detect anomalies.\nMON-02: Centralized Collection of Security Event Logs - Centralizing log collection to enhance the effectiveness of automated analysis and compliance monitoring.\nMON-03: Content of Event Logs - Ensuring event logs contain sufficient detail to facilitate compliance monitoring and anomaly detection.\nRSK-03: Risk Identification - Identifying risk of non-compliance and anomalies in AI systems through effective log analysis.\nTHR-09: Threat Catalog - Utilizing a threat catalog to inform automated log analysis and identify potential non-compliant activities or anomalies.",
        "explainability": "High Risk Monitoring Log Retention is a control that focuses on retaining logs and event records for a specific period, especially during high-risk months. The rationale for this control is to ensure that comprehensive logs are retained during critical periods, such as high-risk months, to facilitate incident response, auditing, and forensic analysis. This explanation should be delivered in an accessible and nontechnical  way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of high-risk month log retention and its role in ensuring the availability of comprehensive logs for incident response and auditing. \n\nRetention Policy Guidelines: Guidelines outlining the policies and procedures for retaining logs during high-risk months. \n\nLog Retention Compliance Reports: Reports demonstrating the effectiveness of log retention during high-risk months and its impact on incident response. | Responsibility Explanation: Log Analysis Tool Integration Plan: A plan detailing how automated tools will be integrated and used, including responsible roles. \n\nCompliance Alert System Setup: Documentation of the alert system that flags non-compliant activities, including roles for monitoring and responding to alerts. \n\nRegulatory Compliance Reports: Monthly or periodic reports generated by the analysis tools that demonstrate compliance or highlight issues. | Data Explanation: Compliance Monitoring System Configuration that details the setup of automated log analysis tools, the specific regulatory requirements being monitored, and the alerting mechanisms for non-compliance or anomalies. | Fairness Explanation: High-Risk Log Analysis Protocol: A protocol outlining how logs are analyzed to maintain compliance and fairness. \n\nCompliance and Fairness Alert System Documentation: Documentation for systems that alert to noncompliant or anomalous activities affecting fairness. \n\nMonthly Fairness Compliance Reports: Monthly reports that review log analysis findings in terms of compliance and fairness. | Safety & Performance  Explanation: Compliance Automation Tools Configuration: Configuration files and settings for automated tools that ensure log analysis is in line with Article 20. \n\nAnomaly and Non-Compliance Alert System Documentation: A detailed description of the alert system that flags noncompliance or anomalies. \n\nCompliance Reports: Regular reports that document compliance with log retention and analysis requirements. | Impact Explanation: Log Analysis and Retention Compliance Plan: A detailed plan outlining how log analysis tools will be used to ensure compliance with the specified retention period.\n\nAnomaly Detection and Alert System Documentation: Technical documentation on the setup of alerts for anomalies or noncompliant activities within the AI system's operations.\n\nRegulatory Compliance Certificates: Certificates or attestations showing the system's compliance with Article 20 or other relevant regulations."
    },
    {
        "control_name": "Log Compliance Monitoring",
        "category": "Security Logging & Monitoring",
        "description": "Update threat models regularly for high-risk AI systems to reflect changes in system design, functionality, or the threat landscape. Use these models to refine security measures and ensure resilience against evolving threats.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 12: Record-Keeping\nArticle 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: RA-3: Risk Assessment\nCA-7: Continuous Monitoring\nSA-10: Developer Configuration Management\nSI-5: Security Alerts, Advisories, and Directives, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining situational awareness of the AI threat landscape, updating threat models accordingly.\nMON-01: Continuous Monitoring - Implementing continuous monitoring that includes analysis of logs against updated threat models.\nRSK-03: Risk Identification - Identifying evolving risk in AI systems through continuous threat modeling and log analysis.\nRSK-04: Risk Assessment - Assessing risk based on updated threat models, refining security measures for AI systems to address identified risk.\nSEA-01: Secure Engineering Principles - Applying secure engineering principles that are informed by regularly updated threat models.\nTHR-07: Threat Hunting - Proactively hunting for threats against AI systems, guided by updated threat models.",
        "explainability": "Log Compliance Monitoring is a control that emphasizes the ongoing monitoring and verification of log data to ensure compliance with relevant policies and regulations. The rationale for this control is to maintain the integrity and accuracy of log records, facilitate auditing, and demonstrate adherence to data security and privacy requirements. Log compliance monitoring enhances transparency, accountability, and regulatory compliance. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of log compliance monitoring and its role in ensuring the integrity and accuracy of log records, accountability, and regulatory compliance. \n\nCompliance Monitoring Guidelines: Guidelines outlining the processes and practices for monitoring log compliance in AI systems. \n\nCompliance Assessment Reports: Reports demonstrating the effectiveness of log compliance monitoring in maintaining log integrity and meeting regulatory requirements. | Responsibility Explanation: Threat Model Update Protocol: A document outlining the procedure for regularly updating threat models, including roles and responsibilities. \n\nModel Compliance Monitoring Logs: Logs that record the updates to threat models and any compliance actions taken. \n\nSecurity Measure Refinement Documentation: Documentation on how security measures are refined based on updated threat models. | Data Explanation: Threat Modeling Update Documentation that records each update to the threat models, the reasons behind the changes, and how the updates have been factored into the system’s security measures. | Fairness Explanation: Threat Model Update and Fairness Assurance Plan: A plan that outlines how updating threat models ensures ongoing fairness. \n\nSecurity Measure Refinement for Fairness Protocol: A protocol that ensures refinements in security measures contribute to the fairness of the AI system. \n\nThreat Model and Fairness Evolution Logs: Logs that record updates to threat models and the implications for fairness. | Safety & Performance  Explanation: Updated Threat Model Documents: Detailed descriptions of updated threat models reflecting the current risk landscape. \n\nChange Logs: Records of changes in system design or functionality that impact the threat model. \n\nSecurity Update Reports: Documentation of the updates to security measures based on the revised threat models. | Impact Explanation: Threat Model Evolution Document: A comprehensive document that records changes to threat models and the rationale behind updates.\n\nAI System Security Update Logs: Logs that detail the implementation of new security measures in response to updated threat models.\n\nCompliance Audit Reports: Regular audit reports that verify the AI system's compliance with security requirements based on the latest threat models."
    },
    {
        "control_name": "Vulnerability Addressal",
        "category": "Vulnerability & Threat Management",
        "description": "Address any vulnerabilities or rights violations identified during evaluations in a timely manner.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 20: Corrective Actions and Duty of Information\nArticle 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models, NIST 800-53: SI-2: Flaw Remediation\nRA-5: Vulnerability Monitoring and Scanning\nIR-4: Incident Handling\nCM-6: Configuration Settings, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with relevant regulations, particularly in addressing rights violations in AI systems.\nPRI-01: Data Privacy Program - Implementing a data privacy program that addresses any identified rights violations related to AI systems.\nIRO-02: Incident Handling - Handling incidents involving vulnerabilities or rights violations in AI systems promptly and effectively.\nIRO-04: Incident Response Plan (IRP) - Having an IRP that includes protocols for addressing vulnerabilities and rights violations in AI systems.\nRSK-03: Risk Identification - Identifying risk, including vulnerabilities and rights violations, as part of the ongoing security management of AI systems.\nRSK-06: Risk Remediation - Actively remediating risk, including vulnerabilities and rights violations, identified in AI systems.\nVPM-02: Vulnerability Remediation Process - Establishing and following a process for promptly remediating vulnerabilities found in AI systems during evaluations.\nVPM-03: Vulnerability Ranking - Prioritizing vulnerabilities identified in AI systems to ensure timely addressal.\nVPM-04: Continuous Vulnerability Remediation Activities - Continuously working on remediation activities for vulnerabilities in AI systems.",
        "explainability": "Vulnerability Addressal is a control that focuses on identifying, assessing, and addressing vulnerabilities in AI systems. The rationale for this control is to proactively manage and mitigate vulnerabilities to enhance the security and reliability of AI systems. Addressing vulnerabilities helps prevent potential breaches and incidents. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of vulnerability addressal and its role in proactively managing and mitigating vulnerabilities in AI systems to enhance security and reliability. \n\nVulnerability Management Guidelines: Guidelines outlining the process and best practices for identifying and addressing vulnerabilities in AI systems. \n\nVulnerability Assessment Reports: Reports demonstrating the effectiveness of vulnerability addressal in maintaining AI system security. | Responsibility Explanation: Vulnerability Management Policy: A policy that outlines the process for addressing vulnerabilities, including responsible roles. \n\nRights Violation Response Plan: A plan that specifies the actions to be taken and by whom when rights violations are identified. \n\nVulnerability Remediation Logs: Logs that track the identification, response, and resolution of vulnerabilities. | Data Explanation: Vulnerability Remediation and Rights Restoration Plan, which should include the process for addressing identified vulnerabilities, timelines for action, and procedures for restoring rights if violated. | Fairness Explanation: Vulnerability Response and Fairness Protocol: A protocol that outlines the steps for addressing vulnerabilities with a focus on fairness. \n\nRights Violation Addressal and Fairness Actions Report: A report documenting the actions taken to address rights violations and their effectiveness in restoring fairness. \n\nVulnerability Rectification Fairness Certificates: Certificates that validate the fairness of the AI system post vulnerability rectification. | Safety & Performance  Explanation: Remediation Plan: A detailed strategy for addressing identified vulnerabilities, including timelines and responsibilities. \n\nVulnerability Patches: Documentation of updates, patches, or changes made to rectify vulnerabilities. \n\nRights Restoration Protocols: Steps taken to rectify any rights violations, including impact assessments and corrective actions taken. | Impact Explanation: Deliverables include: \n\nVulnerability Remediation Plan: A comprehensive document outlining the steps for addressing identified vulnerabilities, their potential impact, and timelines for resolution. \n\nRights Violation Impact Report: A detailed analysis of any rights violations, their consequences on affected individuals, and measures taken to address these issues. \n\nPostremediation Analysis: A report that evaluates the effectiveness of the remediation actions and their success in mitigating negative impacts."
    },
    {
        "control_name": "Comprehensive AI Vulnerability Solutions",
        "category": "Vulnerability & Threat Management",
        "description": "Implement comprehensive technical solutions to address various AI vulnerabilities, including data poisoning and model evasion.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 20: Corrective Actions and Duty of Information\nArticle 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models, NIST 800-53: SI-3: Malicious Code Protection\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-7: Software, Firmware, and Information Integrity, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Developing AI systems that are trustworthy by incorporating comprehensive solutions for identified vulnerabilities.\nAAT-02.1: AI & Autonomous Technologies Internal Controls - Establishing internal controls in AI systems to protect against vulnerabilities like data poisoning.\nRSK-04: Risk Assessment - Assessing risk associated with AI vulnerabilities and implementing appropriate technical solutions.\nTDA-06: Secure Coding - Applying secure coding practices to prevent vulnerabilities in AI systems, including defenses against data poisoning and model evasion.\nVPM-02: Vulnerability Remediation Process - Implementing a process for addressing various AI vulnerabilities, including technical solutions for specific threats like data poisoning and model evasion.\nVPM-03: Vulnerability Ranking - Prioritizing AI vulnerabilities and addressing them with suitable comprehensive technical solutions.\nVPM-04: Continuous Vulnerability Remediation Activities - Continuously addressing vulnerabilities in AI systems through comprehensive technical solutions.",
        "explainability": "Comprehensive AI Vulnerability Solutions is a control that emphasizes the deployment of extensive solutions to address vulnerabilities in AI systems. The rationale for this control is to provide a holistic approach to vulnerability management, addressing potential risk and threats comprehensively to enhance the security and reliability of AI systems. Comprehensive vulnerability solutions help prevent breaches and incidents. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of comprehensive AI vulnerability solutions and their role in addressing vulnerabilities comprehensively to enhance AI system security and reliability. \n\nVulnerability Solution Framework: A framework outlining the comprehensive solutions and practices used to manage and mitigate vulnerabilities in AI systems. \n\nVulnerability Solution Assessment Reports: Reports demonstrating the effectiveness of comprehensive vulnerability solutions in maintaining AI system security. | Responsibility Explanation: Vulnerability Solution Implementation Plan: A plan detailing technical solutions for known AI vulnerabilities and roles responsible for implementing these solutions. \n\nSolution Deployment Records: Records of the deployment of vulnerability solutions, including personnel and actions taken. \n\nSolution Efficacy Analysis Reports: Reports analyzing the effectiveness of deployed solutions in addressing AI vulnerabilities. | Data Explanation: AI Vulnerability Solution Framework, detailing the range of technical solutions applied to address specific vulnerabilities such as data poisoning, model evasion, and other potential threats. | Fairness Explanation: Comprehensive Vulnerability Solution Framework: A framework outlining solutions for AI vulnerabilities, emphasizing their role in supporting fairness. \n\nTechnical Solution Fairness Impact Report: A report evaluating the impact of technical solutions on the fairness of AI outcomes. \n\nVulnerability Solution Fairness Certification: Certification that the comprehensive vulnerability solutions uphold the fairness of the AI system. | Safety & Performance  Explanation: Technical Solution Documentation: Detailed descriptions and specifications of implemented technical solutions. \n\nUpdate and Patch Records: Logs of all updates, patches, and changes applied to address vulnerabilities. \n\nSystem Security Enhancements: A comprehensive list of security enhancements made to the AI system, including tools and processes to prevent data poisoning and model evasion. | Impact Explanation: Deliverables include: \n\nTechnical Solution Documentation: Detailed records of all the technical solutions implemented, including data sanitation protocols, model hardening techniques, and evasion detection mechanisms. \n\nVulnerability Mitigation Reports: Documents that provide insights into each identified vulnerability, the solutions applied, and their efficacy. \n\nSystem Integrity Whitepaper: A comprehensive outline of the system's security measures designed to safeguard against vulnerabilities and their potential societal impact."
    },
    {
        "control_name": "Duty to Investigate Vulnerabilities",
        "category": "Vulnerability & Threat Management",
        "description": "Establish a duty to investigate AI systems when vulnerabilities or rights violations are identified.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 20: Corrective Actions and Duty of Information\nArticle 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models, NIST 800-53: IR-4: Incident Handling\nRA-5: Vulnerability Monitoring and Scanning\nAU-6: Audit Record Review, Analysis, and Reporting\nCA-2: Control Assessments, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Compliance with regulations requiring investigation of vulnerabilities or rights violations in AI systems.\nPRI-01: Data Privacy Program - Implementing a data privacy program that includes the investigation of rights violations in AI systems.\nIRO-01: Incident Response Operations - Operationalizing incident response to include the duty of investigating vulnerabilities and rights violations.\nIRO-02: Incident Handling - Handling incidents in AI systems, including the duty to investigate when vulnerabilities or rights violations are identified.\nIRO-04: Incident Response Plan (IRP) - Developing an IRP that mandates the investigation of vulnerabilities and rights violations in AI systems.\nRSK-03: Risk Identification - Identifying risk that require investigation in AI systems, including vulnerabilities and rights violations.\nRSK-04: Risk Assessment - Conducting risk assessments that include the investigation of identified vulnerabilities or rights violations in AI systems.\nVPM-02: Vulnerability Remediation Process - Implementing processes to investigate and remediate vulnerabilities in AI systems.",
        "explainability": "Duty to Investigate Vulnerabilities is a control that emphasizes the obligation to investigate and address vulnerabilities in AI systems. The rationale for this control is to establish a clear duty and responsibility to identify, assess, and mitigate vulnerabilities to enhance AI system security and reliability. Investigating vulnerabilities helps prevent potential breaches and incidents. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of the duty to investigate vulnerabilities and its role in establishing clear responsibilities for addressing vulnerabilities to enhance AI system security and reliability. \n\nInvestigation Duty Guidelines: Guidelines outlining the responsibilities and processes for investigating vulnerabilities in AI systems. \n\nInvestigation Compliance Reports: Reports demonstrating the effectiveness of the duty to investigate vulnerabilities in maintaining AI system security. | Responsibility Explanation: Investigation Duty Charter: A charter that outlines the obligation to investigate vulnerabilities, including assigned roles. \n\nInvestigation Procedure Documentation: Documentation of the procedures for conducting investigations into reported vulnerabilities or rights violations. \n\nInvestigation Report Templates: Templates for reporting the findings and outcomes of investigations. | Data Explanation: Investigation Duty Protocol, which outlines the responsibilities and processes to be followed when investigating identified vulnerabilities or rights violations within AI systems. | Fairness Explanation: Vulnerability Investigation and Fairness Protocol: A protocol outlining the duty to investigate vulnerabilities with a commitment to fairness. \n\nInvestigation-Driven Fairness Assurance Plan: A plan that ensures investigations into vulnerabilities are conducted with fairness as a core principle. \n\nRights Violation Investigation Fairness Reports: Reports documenting the fairness of investigations into rights violations. | Safety & Performance  Explanation: Investigation Protocols: Detailed procedural documents outlining the steps to be taken during an investigation. \n\nInvestigation Reports: Comprehensive reports documenting the findings, conclusions, and recommended actions following an investigation. \n\nCorrective Action Plans: Strategic plans developed to address and rectify the identified issues. | Impact Explanation: Deliverables include: \n\nInvestigation Protocols: Comprehensive guidelines outlining the steps to be taken when a vulnerability or rights violation is identified. \n\nIncident Reports: Detailed accounts of each incident, including the nature of the vulnerability or violation, its potential impact, and the response undertaken. \n\nCorrective Action Plans: Strategic plans developed to address and remediate identified issues, including measures to prevent recurrence."
    },
    {
        "control_name": "API Query Rate Limiting",
        "category": "AI-Specific Network Security",
        "description": "Implement rate limits on querying machine learning application programming interfaces (APIs) to prevent data leakage or brute-force attempts.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 11: Technical Documentation\nArticle 15: Accuracy, robustness and cybersecurity, NIST 800-53: SC-5: Denial of Service Protection\nAC-17: Remote Access\nCM-6: Configuration Settings\nSI-4: System Monitoring, SCF: CLD-04: Application & Program Interface (API) Security - Securing cloud-based ML APIs by implementing query rate limiting.\nCFG-03: Least Functionality - Configuring ML APIs to operate with the least functionality necessary, including rate limiting to prevent misuse.\nMON-01: Continuous Monitoring - Continuously monitoring API usage, ensuring rate limits are effectively preventing data leakage and brute-force attacks.\nNET-03: Boundary Protection - Implementing boundary protections for ML APIs, including rate limiting to prevent brute-force attacks and data leakage.\nNET-04: Data Flow Enforcement – Access Control Lists (ACLs) - Utilizing ACLs to enforce rate limits on API queries as part of data flow management.\nNET-06: Network Segmentation - Segmenting networks to control access to ML APIs, incorporating rate limiting as a security measure.\nNET-08: Network Intrusion Detection / Prevention Systems (NIDS / NIPS) - Utilizing NIDS/NIPS to support rate limiting on ML API queries as a security control.\nNET-14: Remote Access - Managing remote access to ML APIs with rate limiting controls to enhance security against unauthorized access attempts.\nTDA-01.2: Integrity Mechanisms for Software / Firmware Updates - Ensuring the integrity of ML APIs by implementing rate limits as a defense against attacks.",
        "explainability": "API Query Rate Limiting is a control that emphasizes the implementation of rate limiting mechanisms for API queries to AI systems. The rationale for this control is to manage and control the volume of queries made to AI systems via APIs to prevent overloads, ensure system stability, and protect against abuse or misuse. Rate limiting helps maintain the availability and performance of AI services. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of API query rate limiting and its role in managing query volumes, ensuring system stability, and preventing misuse. \n\nRate Limiting Guidelines: Guidelines outlining the rate limiting mechanisms and best practices for controlling API queries to AI systems. \n\nRate Limiting Compliance Reports: Reports demonstrating the effectiveness of rate limiting in maintaining AI system availability and performance. | Responsibility Explanation: Rate Limiting Policy: A document outlining the rules for rate limiting and the responsibilities for implementation and oversight. \n\nAPI Usage Logs: Logs that record all API queries and track rate limiting enforcement. \n\nRate Limit Compliance Reports: Regular reports analyzing the effectiveness of rate limiting in preventing unauthorized access or data leakage. | Data Explanation: API Rate Limiting Policy, which should define the acceptable number of API requests within a given timeframe, the strategy for enforcing these limits, and the response plan for when the rate limit is exceeded. | Fairness Explanation: API Rate Limiting and Fairness Policy: A policy that connects the practice of rate limiting to the fairness of AI systems. \n\nRate Limiting Compliance and Fairness Protocol: A protocol that ensures rate limiting compliances are in line with fairness standards. \n\nQuery Rate Limiting and Fairness Audit Logs: Audit logs that show rate limiting in action and its implications for fairness. | Safety & Performance  Explanation: API Rate Limit Policies: Documents outlining the number of allowed API calls within a specific period and the rationale for these limits. \n\nRate Limit Implementation Guides: Technical documents detailing the process for setting up rate limits on machine learning APIs. \n\nIncident Response Plans: Prepared plans for responding to incidents where rate limits are exceeded. | Impact Explanation: Deliverables include: \n\nRate Limiting Policy Document: A comprehensive outline of the rate limits applied, including the rationale for chosen thresholds. \n\nSecurity Incident Reports: Documentation of any incidents related to rate-limiting breaches, detailing the impact and remedial actions taken. \n\nAPI Usage Guidelines: Clear guidelines for legitimate users on how to interact with the API within the set rate limits."
    },
    {
        "control_name": "External Data Connection Security",
        "category": "AI-Specific Network Security",
        "description": "Ensure secure connections between models and data when training against online data stores.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 11: Technical Documentation\nArticle 15: Accuracy, robustness and cybersecurity, NIST 800-53: SC-8: Transmission Confidentiality and Integrity\nAC-4: Information Flow Enforcement\nSC-7: Boundary Protection\nSA-17: Developer Security and Privacy Architecture and Design, SCF: CLD-02: Cloud Security Architecture - Designing a cloud security architecture that ensures secure connections for AI models accessing online data stores.\nCLD-07: Data Handling & Portability - Managing the handling and portability of data in cloud environments, especially for AI model training purposes.\nCFG-02: System Hardening Through Baseline Configurations - Hardening systems involved in AI model training to secure data connections with external sources.\nCRY-03: Transmission Confidentiality - Ensuring confidentiality of data transmitted between AI models and online data stores.\nCRY-04: Transmission Integrity - Maintaining the integrity of data during transmission between AI models and external data stores.\nNET-03: Boundary Protection - Implementing boundary protection mechanisms to secure connections between AI models and external data sources.\nNET-03.2: External Telecommunications Services - Securing telecommunications services that facilitate data connections between AI models and external data sources.\nNET-05: System Interconnections - Managing and securing system interconnections, including those between AI models and online data stores.\nNET-12: Safeguarding Data Over Open Networks - Protecting data transmitted over open networks, such as the internet, during AI model training.",
        "explainability": "External Data Connection Security is a control that emphasizes the security of external data connections to AI systems. The rationale for this control is to protect AI systems from potential security threats and data breaches originating from external data sources. Ensuring the security of data connections is crucial for safeguarding sensitive information and maintaining the integrity and confidentiality of AI data. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of external data connection security and its role in protecting AI systems from external security threats and data breaches. \n\nConnection Security Guidelines: Guidelines outlining the best practices and security measures for securing external data connections.\n \nSecurity Assessment Reports: Reports demonstrating the effectiveness of external data connection security in safeguarding AI systems. | Responsibility Explanation: Data Connection Security Plan: A plan detailing the security measures for connections to external data stores, including responsible roles. \n\nConnection Security Audit Trails: Logs that detail the establishment and maintenance of secure data connections. \n\nData Transfer Encryption Certificates: Certificates and other proofs of encryption for data transfers to and from external data sources. | Data Explanation: Data Connection Security Protocols, detailing the encryption standards, authentication procedures, and connection protocols used to secure the link between the AI model and external data sources. | Fairness Explanation: Data Connection Security Protocol: A protocol detailing secure connection practices and their importance for fairness. \n\nSecurity and Fairness Integration Report: A report showing how secure connections contribute to fair AI training. \n\nData Connection Security Audit Results: Audit results that evaluate the effectiveness of security measures in maintaining fairness during training. | Safety & Performance  Explanation: Security Protocol Documentation: Descriptions of the security protocols and measures in place for external data connections. \n\nData Transmission Encryption Standards: Specific encryption standards adopted for data in transit. \n\nAPI and Endpoint Security Certifications: Certificates and compliance documents for secure API endpoints. | Impact Explanation: Deliverables include: \n\nEncryption Protocol Documentation: Detailed descriptions of encryption standards used for data in transit. \n\nData Breach Response Plan: A ready-to-activate plan detailing steps to take in the event of a breach through these connections.\n \nSecurity Certification Records: Proof of adherence to industry-standard security certifications for data connections."
    },
    {
        "control_name": "API Rate Limiting",
        "category": "AI-Specific Network Security",
        "description": "Ensure rate limiting is in place to prevent malicious system bombardment and rapid vulnerability testing.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 11: Technical Documentation\nArticle 15: Accuracy, robustness and cybersecurity, NIST 800-53: SC-5: Denial of Service Protection\nAC-17: Remote Access\nCM-6: Configuration Settings\nSI-4: System Monitoring, SCF: CLD-04: Application & Program Interface (API) Security - Securing APIs by implementing query rate limiting as a preventive measure against malicious access.\nCFG-03: Least Functionality - Configuring APIs to operate with the least functionality necessary, including rate limiting to prevent abuse.\nMON-01: Continuous Monitoring - Continuously monitoring API usage, ensuring rate limits are in place and effective in preventing abuse.\nNET-03: Boundary Protection - Implementing boundary protections for APIs, including rate limiting to prevent system bombardment and vulnerability testing.\nNET-04: Data Flow Enforcement – Access Control Lists (ACLs) - Utilizing ACLs to enforce rate limits on API access as part of data flow management.\nNET-06: Network Segmentation - Segmenting networks to control access to APIs, incorporating rate limiting as a security measure.\nNET-08: Network Intrusion Detection / Prevention Systems (NIDS / NIPS) - Utilizing NIDS/NIPS to support rate limiting on API queries as a security control.\nNET-14: Remote Access - Managing remote access to APIs with rate limiting controls to protect against malicious bombardment.\nTDA-01.2: Integrity Mechanisms for Software / Firmware Updates - Ensuring the integrity of APIs by implementing rate limits as a defense against rapid vulnerability testing.",
        "explainability": "API Rate Limiting is a control that emphasizes the implementation of rate limiting mechanisms for APIs used in AI systems. The rationale for this control is to manage and control the rate at which API requests are made, preventing overloads, ensuring system stability, and protecting against abuse or misuse. Rate limiting helps maintain the availability and performance of AI services while preventing resource exhaustion. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of API rate limiting and its role in managing API request rates, ensuring system stability, and preventing misuse. \n\nRate Limiting Guidelines: Guidelines outlining the rate limiting mechanisms and best practices for controlling API requests in AI systems. \n\nRate Limiting Compliance Reports: Reports demonstrating the effectiveness of rate limiting in maintaining AI system availability and performance. | Responsibility Explanation: API Rate Limiting Configuration Document: A document that outlines the configuration settings for API rate limiting and identifies responsible individuals. \n\nAPI Access Monitoring Logs: Logs that track access to APIs and enforcement of rate limits. \n\nAttack Prevention Analysis Reports: Reports analyzing the success of rate limiting in preventing attacks and rapid vulnerability testing. | Data Explanation: API Rate Limiting Configuration Guide, which specifies how the rate limits are set, monitored, and enforced, along with the system's response to detected bombardment attempts. | Fairness Explanation: Rate Limiting and Fairness Policy: A policy that outlines the role of rate limiting in protecting AI system fairness. \n\nAPI Security and Fairness Impact Report: A report detailing the impact of API rate limiting on the fairness of AI systems. \n\nRate Limiting Compliance Logs: Logs that demonstrate compliance with rate limiting policies and their fairness implications. | Safety & Performance  Explanation: Rate Limiting Configuration Files: Detailed settings for the API server that control the rate of incoming requests. \n\nAPI Throttling Policies: Documented policies that define acceptable usage limits and consequences for exceeding them. \n\nTraffic Volume Reports: Regular reports that log the number of requests over time to monitor for abnormal patterns. | Impact Explanation: Deliverables include: \n\nAPI Rate Limiting Policy: A document outlining the rate limits and the rationale behind their thresholds. \n\nSystem Availability Reports: Regular reports demonstrating the effectiveness of rate limiting in ensuring system uptime. \n\nIncident Response Protocols: Procedures that detail actions to be taken in case of detected bombardment attempts."
    },
    {
        "control_name": "Emergency Response Procedures for AI Systems",
        "category": "AI Emergency Protocols",
        "description": "Develop and maintain procedures for rapid response to AI system emergencies, ensuring the minimization of harm and quick recovery of operations.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: CP-2: Contingency Plan\nIR-4: Incident Handling\nCP-10: System Recovery and Reconstitution\nRA-9: Criticality Analysis\n\n, SCF: BCD-01: Business Continuity Management System (BCMS) - Integrating AI system emergency response into the broader business continuity management practices.\nBCD-04: Contingency Plan Testing & Exercises - Regularly testing and exercising contingency plans for AI system emergencies to ensure quick and effective response.\nBCD-06: Contingency Planning & Updates - Updating contingency plans for AI systems to account for new types of emergencies and evolving threats.\nIRO-01: Incident Response Operations - Operationalizing incident response to include specific protocols for AI system emergencies.\nIRO-02: Incident Handling - Handling incidents specific to AI systems, including the establishment of rapid response procedures.\nIRO-04: Incident Response Plan (IRP) - Developing and maintaining an IRP that includes specific procedures for responding to AI system emergencies.\nIRO-05: Incident Response Training - Training relevant personnel on emergency response procedures for AI systems.\nIRO-06: Incident Response Testing - Testing incident response capabilities specifically for AI system emergencies to ensure readiness.\nRSK-11: Risk Monitoring - Continuously monitoring risk that could lead to AI system emergencies and refining response strategies.",
        "explainability": "Emergency Response Procedures for AI Systems is a control that emphasizes the development of procedures and plans to address emergencies and critical incidents related to AI systems. The rationale for this control is to establish clear and effective protocols for responding to unexpected events, outages, or disruptions that may impact AI system operations. Emergency response procedures are essential for ensuring the resilience, availability, and continuity of AI services. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of emergency response procedures for AI systems and their role in ensuring resilience, availability, and continuity in the face of critical incidents. \n\nEmergency Response Plan: A comprehensive plan outlining the procedures, responsibilities, and actions to be taken during emergencies. \n\nResponse Effectiveness Reports: Reports demonstrating the effectiveness of emergency response procedures in maintaining AI system resilience. | Responsibility Explanation: AI Emergency Response Plan: A comprehensive plan that outlines the procedures and assigns responsibilities for rapid response to AI emergencies. \n\nResponse Team Roster: A list of individuals on the emergency response team and their specific roles. \n\nEmergency Drill Records: Records of emergency response drills, including outcomes and lessons learned. | Data Explanation: Emergency Response Plan for AI Systems, which outlines the rapid response protocols, roles and responsibilities, and communication strategies during AI system emergencies. This plan should be regularly reviewed and updated. | Fairness Explanation: AI Emergency Fairness Response Plan: A response plan for AI system emergencies that emphasizes fairness. \n\nEmergency Procedure and Fairness Assurance Documentation: Documentation on how emergency procedures ensure fairness is upheld.\n \nEmergency Response Fairness Review Records: Records of fairness reviews following emergency responses. | Safety & Performance  Explanation: Emergency Response Plan: A comprehensive document that outlines steps to be taken in case of an AI emergency, including roles and responsibilities. \n\nRecovery Protocols: Step-by-step guides for restoring system functionality after an incident. \n\nPostincident Reports: Templates and previous examples of reports detailing incident causes, impacts, and responses. | Impact Explanation: Deliverables include: \n\nEmergency Response Plan: A comprehensive document that outlines the steps to be taken in various emergency scenarios. \n\nTraining Modules and Drills: Detailed training materials and records of drills conducted to prepare the response team. \n\nPostincident Reports: Analysis of the response to past emergencies and lessons learned."
    },
    {
        "control_name": "High-Risk AI Registration",
        "category": "AI Model Training",
        "description": "Ensure the registration process for high-risk systems and foundation models is performed in the applicable government-mandated database (e.g., public EU database, mainland China CAC filing, etc.).",
        "risk_level": "High Risk",
        "framework": "EU AI Law:  Article 71: EU Database for High-Risk AI Systems Listed in Annex III, NIST 800-53: PM-5: System Inventory\nPM-28: Risk Framing\nAU-6: Audit Record Review, Analysis, and Reporting\nSA-5: System Documentation, SCF: AST-01: Asset Governance - Governing AI assets, including adherence to registration requirements for high-risk systems.\nAST-02.9: Configuration Management Database (CMDB) - Utilizing a CMDB to track and manage registration information of high-risk AI systems.\nAST-31: Asset Categorization - Categorizing AI systems as high-risk assets and ensuring their registration in the EU database.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with EU regulations by registering high-risk AI systems in the public database.\nDCH-05: Cybersecurity & Data Privacy Attributes - Managing and documenting attributes of AI systems, including registration details, for compliance purposes.\nPRI-15: Register As A Data Controller and/or Data Processor - Registering as a data controller or processor for high-risk AI systems, as required by EU regulations.\nIAC-09: Identifier Management (User Names) - Managing identifiers for AI systems to align with registration processes in the EU database.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating registration requirements in project management processes for AI systems.\nTDA-04: Documentation Requirements - Maintaining comprehensive documentation for high-risk AI systems, including registration information.",
        "explainability": "High-Risk AI Registration is a control that emphasizes the registration and documentation of high-risk AI systems. The rationale for this control is to establish a clear process for identifying, registering, and monitoring AI systems that pose high risk. risk. High-risk AI registration is crucial for regulatory compliance, risk management, and ensuring transparency and accountability. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of high-risk AI registration and its role in regulatory compliance, risk management, and transparency. \n\nRegistration Guidelines: Guidelines outlining the procedures and criteria for registering high-risk AI systems. \n\nRegistration Compliance Reports: Reports demonstrating the effectiveness of high-risk AI registration in identifying and monitoring high-risk systems. | Responsibility Explanation: AI System Registration Protocol: A protocol that outlines the steps for registering AI systems, including who is responsible for each step. \n\nRegistration Compliance Logs: Logs that record the details of AI system registrations and compliance with EU requirements. \n\nPublic Registry Update Records: Records of updates to the public EU database for high-risk AI systems. | Data Explanation: Registration Process Outline that details the steps, required documentation, and criteria for high-risk AI systems to be registered in the EU database. This should include guidelines for system evaluation against the high-risk criteria. | Fairness Explanation: High-Risk AI Registration Guidelines: Guidelines that describe the registration process for high-risk AI systems and its importance for fairness. \n\nPublic Fairness Transparency Report: A report detailing how the registration process promotes fairness by ensuring transparency. \n\nRegistration and Fairness Compliance Certificates: Certificates that confirm the registration of high-risk AI systems complies with fairness standards. | Safety & Performance  Explanation: Registration Documentation: Official forms and evidence submitted for the registration of the AI system, including details about the system's intended use, capabilities, and safety features. \n\nPublic EU Database Entries: Records of registered AI systems, which may include compliance certificates, model specifications, and risk assessments. \n\nTransparency Reports: Documents that explain the AI system's functionalities, risk factors, and mitigation measures to the public. | Impact Explanation: Deliverables include: \n\nRegistration Dossier: A file containing detailed information on the AI system's capabilities, design, and intended use. \n\nPublic Disclosure Reports: Documents made available in the public database, detailing the AI system's potential impacts. \n\nCompliance Certificates: Certifications that verify the system has met all necessary EU regulatory requirements for high-risk systems."
    },
    {
        "control_name": "High-Risk AI Documentation",
        "category": "AI Model Training",
        "description": "Maintain clear documentation and listing of AI systems categorized as high risk.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 11: Technical Documentation, NIST 800-53: SA-5: System Documentation\nPM-5: System Inventory\nSA-10: Developer Configuration Management\nCM-8: System Component Inventory, SCF: AST-01: Asset Governance - Governing AI assets with a focus on the documentation and tracking of high-risk systems.\nAST-02: Asset Inventories - Maintaining inventories of AI assets, including clear documentation of those categorized as high-risk.\nAST-31: Asset Categorization - Categorizing AI systems, particularly identifying and documenting those that are high-risk.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with documentation requirements for high-risk AI systems as per statutory and regulatory guidelines.\nDCH-05: Cybersecurity & Data Privacy Attributes - Managing and documenting cybersecurity and data privacy attributes for high-risk AI systems.\nIAC-09: Identifier Management (User Names) - Managing identifiers and ensuring they are properly documented for high-risk AI systems.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the portfolio of AI systems with a focus on proper documentation of high-risk categories.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating documentation requirements in project management processes for high-risk AI systems.\nTDA-04: Documentation Requirements - Ensuring comprehensive documentation is maintained for all high-risk AI systems throughout the model training lifecycle.",
        "explainability": "High-Risk AI Documentation is a control that emphasizes the comprehensive documentation of high-risk AI systems. The rationale for this control is to ensure that detailed and accurate documentation is maintained for AI systems that pose high risk. risk. High-risk AI documentation is crucial for regulatory compliance, transparency, accountability, and risk management. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of high-risk AI documentation and its role in regulatory compliance, transparency, accountability, and risk management. \n\nDocumentation Guidelines: Guidelines outlining the required documentation, content, and format for high-risk AI systems. \n\nDocumentation Compliance Reports: Reports demonstrating the effectiveness of high-risk AI documentation in maintaining transparency and accountability. | Responsibility Explanation: High-Risk AI Documentation Policy: A document outlining the requirements for maintaining documentation of high-risk AI systems, including roles and responsibilities. \n\nHigh-Risk AI System Catalog: A catalog or list that includes all AI systems categorized as high-risk with relevant documentation. \n\nDocumentation Review Logs: Logs that record each review and update of the high-risk AI documentation. | Data Explanation: High-Risk AI System Documentation Guidelines, detailing the information required for each system, how it should be documented, and the process for updating documentation as the system evolves or as new information becomes available. | Fairness Explanation: High-Risk AI Documentation Standards: Standards for documenting high-risk AI systems with a focus on fairness. \n\nDocumentation and Fairness Review Protocols: Protocols for reviewing AI documentation for its adherence to fairness. \n\nHigh-Risk AI Documentation and Fairness Logs: Logs that record documentation practices and their implications for fairness. | Safety & Performance  Explanation: High-Risk AI System Registry: A detailed listing of all AI systems identified as high risk, including their characteristics, deployment environments, and operational protocols. \n\nRisk Assessment Reports: Documents containing the analysis that categorized the AI systems as high risk, including potential impact and mitigation strategies. \n\nDocumentation of AI System Lifecycle: A complete record of the development, testing, deployment, and maintenance phases of the AI system. | Impact Explanation: Deliverables include: \n\nHigh-Risk AI System Registry: A detailed registry or list of all high-risk AI systems, accessible to relevant stakeholders. \n\nSystem Impact Reports: Documents that outline the potential impacts of each high-risk AI system on individuals and society. \n\nTechnical and User Manuals: Detailed descriptions of system operations, intended use, and guidance on interpreting system outputs."
    },
    {
        "control_name": "High-Risk AI Identification & Record-Keeping",
        "category": "AI Model Versioning",
        "description": "Implement a procedure to identify, list, and maintain records of AI systems that are categorized as high risk.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: CM-8:System Component Inventory\nPM-5: System Inventory\nSA-5: System Documentation\nSA-10: Developer Configuration Management, SCF: AST-01: Asset Governance - Governing AI assets with a focus on identification, categorization, and record-keeping of high-risk systems.\nAST-02: Asset Inventories - Developing and maintaining inventories of AI systems, including the identification and recording of high-risk systems.\nAST-31: Asset Categorization - Categorizing AI systems, with a specific focus on identifying and documenting high-risk systems.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with requirements for identifying and documenting high-risk AI systems.\nDCH-05: Cybersecurity & Data Privacy Attributes - Managing and documenting attributes of high-risk AI systems, ensuring accurate records are kept.\nIAC-09: Identifier Management (User Names) - Managing identifiers for AI systems and ensuring they are documented, especially for high-risk categories.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the portfolio of AI systems with procedures for identifying and recording high-risk systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including procedures for identifying and keeping records of high-risk AI systems in project management practices.\nTDA-04: Documentation Requirements - Documenting all aspects of AI systems, particularly focusing on high-risk system identification and record maintenance.",
        "explainability": "High-Risk AI Identification & Record-Keeping is a control that emphasizes the identification and systematic record-keeping of high-risk AI systems. The rationale for this control is to establish clear processes for identifying and documenting AI systems that pose high risk. risk. This systematic identification and record-keeping are crucial for regulatory compliance, risk management, transparency, and accountability. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of high-risk AI identification and record-keeping and their role in regulatory compliance, risk management, transparency, and accountability. \n\nIdentification and Record-Keeping Guidelines: Guidelines outlining the processes, criteria, and documentation requirements for high-risk AI systems. \n\nCompliance and Record-Keeping Reports: Reports demonstrating the effectiveness of high-risk AI identification and record-keeping in maintaining transparency and accountability. | Responsibility Explanation: High-Risk AI Identification Procedure: A formal procedure for identifying and classifying AI systems as high-risk. \n\nHigh-Risk AI Records Database: A secure database for maintaining records of identified high-risk AI systems. \n\nRecord-Keeping Protocol: A protocol that defines how records are to be kept, updated, and accessed, including assigned responsibilities. | Data Explanation: Identification and Record-Keeping Procedure for High-Risk AI, which must include criteria for categorization, a registry of high-risk AI systems, and the maintenance schedule for these records. | Fairness Explanation: High-Risk AI Identification Protocol: A protocol for identifying high-risk AI systems with fairness considerations. \n\nRecord-Keeping and Fairness Integration Plan: A plan that integrates record-keeping practices with fairness in AI operations. \n\nHigh-Risk AI Fairness Audit Trails: Audit trails that reflect the identification and record-keeping of high-risk AI systems with a focus on fairness. | Safety & Performance  Explanation: Identification Criteria Documentation: Official guidelines that detail the criteria used to categorize AI systems as high risk.\n\nHigh-Risk AI Systems List: A regularly updated list containing all AI systems identified as high risk, including their operational scope and risk factors. \n\nRecord-Keeping Logs: Detailed logs that document the decision-making process for categorizing AI systems and any updates or changes to their risk status. | Impact Explanation: Deliverables include: \n\nRisk Assessment Documentation: Detailed reports outlining the criteria and processes used to designate AI systems as high risk. \n\nHigh-Risk AI Register: A continually updated database or registry of all identified high-risk AI systems. \n\nRecord-Keeping Protocols: Policies and procedures governing how high-risk AI records are maintained, accessed, and updated."
    },
    {
        "control_name": "Deployment Compliance Measures",
        "category": "AI Model Training",
        "description": "Ensure deployers of high-risk AI systems implement technical and organizational measures to ensure AI systems operate in accordance with provided instructions.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10, Paragraph 2: Emphasizes the importance of data governance and management. Ensuring a clear separation of training and testing data aligns with the provision to use high-quality datasets for AI systems.\nArticle 13, Paragraph 1: Requires transparency regarding the capabilities and limitations of AI systems. Clear demarcation of training and testing datasets supports accurate representation of a system's capabilities.\nArticle 16, Paragraph 1, Point a: Providers must ensure AI systems are developed using processes that allow for the assessment of their operation, which includes using separate training and testing datasets to prevent overfitting.\nArticle 22, Paragraph 1: Focuses on risk management in AI systems. Separation of training and testing data is a risk mitigation strategy, especially against overfitting and biased model performance.\nRecital 39: Discusses the adaptation of AI systems to changing environments. Training-testing data separation is essential for accurate model evaluation and adaptation., NIST 800-53: SA-9: External System Services\nCM-8: System Component Inventory\nAC-4: Information Flow Enforcement\nCM-6: Configuration Settings, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Governing AI deployments to ensure adherence to technical and organizational compliance measures.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring deployment of AI systems complies with relevant laws, regulations, and contractual obligations.\nCFG-01: Configuration Management Program - Managing configurations to ensure AI systems are deployed in compliance with operational guidelines.\nCFG-02: System Hardening Through Baseline Configurations - Hardening AI systems to comply with operational instructions through baseline configuration.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating compliance measures in project management for the deployment of high-risk AI systems.\nRSK-01: Risk Management Program - Implementing a risk management program that includes compliance aspects for high-risk AI deployments.\nRSK-03: Risk Identification - Identifying risk related to non-compliance in the operation of high-risk AI systems.",
        "explainability": "Deployment Compliance Measures is a control that emphasizes the implementation of measures to ensure compliance during the deployment of AI systems. The rationale for this control is to establish processes and measures that guarantee the AI system's adherence to regulatory requirements, industry standards, and organizational policies during deployment. Compliance measures are essential for mitigating legal and regulatory risk and maintaining ethical AI practices. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of deployment compliance measures and their role in ensuring regulatory compliance, adherence to industry standards, and ethical AI practices during deployment. \n\nDeployment Compliance Guidelines: Guidelines outlining the processes and measures for maintaining compliance during AI system deployment. \n\nCompliance Audit Reports: Reports demonstrating the effectiveness of deployment compliance measures in mitigating risk and ensuring compliance. | Responsibility Explanation: Deployment Compliance Plan: A plan that outlines the technical and organizational measures deployers must implement to ensure compliance. \n\nCompliance Monitoring Tools: Tools used to monitor and ensure high-risk AI systems operate as instructed. \n\nDeployment Instruction Manuals: Manuals that provide detailed instructions on the deployment and operation of high-risk AI systems. | Data Explanation: Compliance Measures Documentation, which describes the technical and organizational measures put in place to ensure that AI systems are deployed and operated as intended and in accordance with regulatory and ethical guidelines. | Fairness Explanation: Deployment Compliance and Fairness Policy: A policy detailing the technical and organizational measures to ensure the fair operation of high-risk AI systems. \n\nCompliance Measures and Fairness Assurance Plan: A plan that ensures compliance measures also maintain fairness in AI deployments. \n\nFair Operation Compliance Certificates: Certificates that attest to the fairness of high-risk AI system operations. | Safety & Performance  Explanation: Compliance Protocols: Detailed procedures that establish how AI systems must be operated in accordance with their designated purposes and instructions. \n\nTraining Manuals: Guides provided to personnel to educate them on the compliant operation of high-risk AI systems. \n\nTechnical Setup Documentation: Records detailing the technical configurations set to enforce operational compliance. | Impact Explanation: Deliverables include: \n\nCompliance Framework Documentation: Outlines the technical and organizational measures in place for ensuring adherence to operational guidelines. \n\nOperational Compliance Reports: Regular reports or logs showing real-time system performance against compliance measures. \n\nDeployment Guidelines: Specific instructions provided for deployers, detailing how the system should be operated in accordance with compliance standards."
    },
    {
        "control_name": "Sandbox Guidance for High-Risk AI",
        "category": "AI Model Training",
        "description": "Require that prospective providers receive guidance and supervision when developing high-risk AI systems within a sandbox environment to ensure regulatory compliance.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nArticle 57: Regulatory Sandboxes\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-8: Security and Privacy Training and Awareness\nAC-24: Access Control Decisions\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Complying with regulatory requirements in the development of high-risk AI systems within sandbox environments.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls in sandbox environments for high-risk AI systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Providing project management guidance for developing high-risk AI systems in sandbox environments.\nPRM-07: Secure Development Life Cycle (SDLC) Management - Managing the SDLC for AI systems, including guidance for development in sandbox environments.\nRSK-01: Risk Management Program - Including sandbox development of high-risk AI systems in the organization’s risk management program.\nTDA-05: Developer Architecture & Design - Ensuring architectural and design aspects of AI systems in sandbox environments comply with regulatory requirements.\nTDA-06: Secure Coding - Applying secure coding practices in sandbox environments for high-risk AI systems, in line with regulatory compliance.\nTDA-07: Secure Development Environments - Ensuring the secure development of high-risk AI systems within sandbox environments, adhering to regulatory guidelines.",
        "explainability": "Sandbox Guidance for High-Risk AI is a control that emphasizes the creation of sandbox environments and guidelines for high-risk AI systems. The rationale for this control is to establish isolated and controlled environments for testing, experimentation, and validation of high-risk AI systems, ensuring that their behavior and impacts are thoroughly assessed before deployment. Sandbox guidance is critical for risk mitigation, quality assurance, and regulatory compliance. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of sandbox guidance for high-risk AI and its role in risk mitigation, quality assurance, and regulatory compliance. \n\nSandbox Guidelines: Guidelines outlining the procedures and best practices for setting up and using sandbox environments for high-risk AI systems. \n\nSandbox Effectiveness Reports: Reports demonstrating the effectiveness of sandbox guidance in ensuring comprehensive testing and validation. | Responsibility Explanation: Sandbox Development Framework: A framework outlining the guidance and supervision provided to developers within the sandbox. \n\nRegulatory Compliance Checklist: A checklist used to verify regulatory compliance during sandbox development. \n\nSupervision Activity Logs: Logs that detail the supervision activities and guidance provided in the sandbox environment. | Data Explanation: Sandbox Development Guidance Documentation, outlines the support, resources, and oversight provided to developers of high-risk AI systems within the sandbox environment. | Fairness Explanation: Sandbox Fairness Compliance Guidelines: Guidelines to ensure AI development within sandbox environments adheres to fairness standards. \n\nRegulatory Compliance and Fairness Supervision Reports: Reports documenting the supervision process and compliance with fairness in the sandbox environment. \n\nSandbox Development Fairness Logs: Logs that record fairness considerations during the development of high-risk AI systems. | Safety & Performance  Explanation: Sandbox Testing Framework: A set of rules and tools for conducting controlled tests within a sandbox environment.\n \nCompliance Checklists: Detailed lists used to verify that all regulatory aspects are addressed during sandbox testing. \n\nSupervision Reports: Documentation of observations and guidance provided by regulators or experts during sandbox testing. | Impact Explanation: Deliverables include: \n\nSandbox Testing Reports: Detailed analysis of AI performance in the sandbox, including any issues or failures and their implications. \n\nRegulatory Compliance Maps: Documents aligning sandbox activities with regulatory requirements, indicating how each aspect of the AI complies with specific regulations. \n\nStakeholder Impact Analyses: Assessments of how the AI's decisions in the sandbox affect different stakeholders, predicting potential societal impacts."
    },
    {
        "control_name": "Localization of AI System Information",
        "category": "AI Model Versioning",
        "description": "Ensure information associated with high-risk AI systems is provided in the language of the country where the system is deployed.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 47: EU Declaration of Conformity, NIST 800-53: PL-2: System Security and Privacy Plans\nAC-4: Information Flow Enforcement, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Complying with regulatory requirements in the development of high-risk AI systems within sandbox environments.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls in sandbox environments for high-risk AI systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Providing project management guidance for developing high-risk AI systems in sandbox environments.\nPRM-07: Secure Development Life Cycle (SDLC) Management - Managing the SDLC for AI systems, including guidance for development in sandbox environments.\nRSK-01: Risk Management Program - Including sandbox development of high-risk AI systems in the organization’s risk management program.\nTDA-05: Developer Architecture & Design - Ensuring architectural and design aspects of AI systems in sandbox environments comply with regulatory requirements.\nTDA-06: Secure Coding - Applying secure coding practices in sandbox environments for high-risk AI systems, in line with regulatory compliance.\nTDA-07: Secure Development Environments - Ensuring the secure development of high-risk AI systems within sandbox environments, adhering to regulatory guidelines.",
        "explainability": "Localization of AI System Information is a control that emphasizes the adaptation of AI system information and interfaces to different languages and regions. The rationale for this control is to ensure that AI system information and communication are accessible and understandable to users and stakeholders from various linguistic and cultural backgrounds. Localization enhances inclusivity, user experience, and market accessibility. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of the localization of AI system information and its role in enhancing inclusivity, user experience, and market accessibility. \n\nLocalization Guidelines: Guidelines outlining the best practices and processes for adapting AI system information to different languages and regions. \n\nLocalization Compliance Reports: Reports demonstrating the effectiveness of localization in making AI information accessible to a diverse user base. | Responsibility Explanation: Localization Policy: A policy that outlines the requirements for translating and localizing high-risk AI system information. \n\nTranslated Documentation Registry: A registry of all localized documents and information for high-risk AI systems. \n\nLocalization Process Logs: Logs that track the process of translation and localization, including responsible personnel. | Data Explanation: Localization Policy and Process Documents describe how information is translated and adapted to meet the language requirements of the country of deployment, ensuring clarity and accessibility. | Fairness Explanation: AI Information Localization Policy: A policy that outlines the importance of providing AI system information in local languages for fairness. \n\nLocalization and Fairness Compliance Certificates: Certificates confirming that AI system information localization meets fairness requirements. \n\nLanguage Accessibility and Fairness Reports: Reports on the accessibility of AI system information in local languages and its impact on fairness. | Safety & Performance  Explanation: Localized Documentation: All necessary documentation such as user manuals, policy documents, and technical support materials translated into the local language. \n\nUser Interface (UI) Localization Packages: Software components that adapt the UI to the local language and cultural context.\n \nLocalization Verification Certificates: Official documents confirming that the localization process has been completed and verified for accuracy. | Impact Explanation: Deliverables include: \n\nLocalized User Manuals: Comprehensive guides and documentation translated into the local language explaining the AI system's functions and impact. \n\nCultural Impact Assessments: Documents detailing the potential cultural impacts and societal reactions to the AI system within a local context. \n\nTranslation Verification Records: Proof of accurate and context-sensitive translations, along with details of the translators' qualifications and methods used."
    },
    {
        "control_name": "AI Risk Classification Distinction",
        "category": "AI Model Versioning",
        "description": "Develop and implement a protocol to distinguish between the risk classification of an AI system and its associated product.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: CA-7: Continuous Monitoring\nRA-5: Vulnerability Monitoring and Scanning\nAT-2: Literacy Training and Awareness, SCF: AST-01: Asset Governance - Governing AI systems and products with clear distinction in their risk classifications.\nAST-31: Asset Categorization - Distinguishing AI systems as assets and categorizing them based on their risk levels, separate from associated products.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with regulations that may require distinct risk classifications for AI systems and products.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the portfolio of AI systems and products with a focus on risk classification distinction.\nPRM-05: Cybersecurity & Data Privacy Requirements Definition - Defining specific cybersecurity and data privacy requirements for AI systems and products based on their risk classifications.\nRSK-01: Risk Management Program - Implementing a risk management program that includes distinct classification for AI systems and products.\nRSK-02: Risk-Based Security Categorization - Categorizing AI systems and associated products based on their security and risk profiles.\nRSK-04: Risk Assessment - Conducting risk assessments that differentiate between the risk of AI systems and their related products.\nTDA-01: Technology Development & Acquisition - Considering risk classification in the development and acquisition of AI systems and products.",
        "explainability": "AI Risk Classification Distinction is a control that emphasizes the clear classification and distinction of AI system risk based on severity and potential impact. The rationale for this control is to establish a structured approach to categorizing AI-related risk, risk, ensuring that they are appropriately identified, prioritized, and addressed. Risk classification distinction is essential for effective risk management, resource allocation, and decision-making. decision making. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of AI risk classification distinction and its role in structured risk management, resource allocation, and decision making.\n\nRisk Classification Guidelines: Guidelines outlining the criteria and processes for classifying AI-related risk.\n\nClassification Effectiveness Reports: Reports demonstrating the effectiveness of risk classification distinction in prioritizing and managing AI risk. | Responsibility Explanation: Risk Classification Protocol: A detailed protocol defining the criteria and process for classifying the risk of AI systems independently from their associated products. \n\nAI Risk Classification Guide: A guide or manual for internal and external stakeholders outlining the risk classification process. \n\nRisk Classification Records: Documented records of each AI system’s risk classification determination. | Data Explanation: Risk Classification Protocol outlines the criteria and processes used to categorize AI systems and their associated products, ensuring that each has an appropriate and distinct risk classification. | Fairness Explanation: Risk Classification Distinction Protocol: A protocol that clearly distinguishes AI system risk from associated products with fairness in mind. \n\nRisk Classification and Fairness Assessment Plan: A plan that assesses AI systems for risk and fairness distinctions. \n\nProduct-AI Risk Distinction Fairness Logs: Logs that record the process of distinguishing product and AI system risk with a focus on fairness. | Safety & Performance  Explanation: Risk Classification Protocol: A document outlining the criteria and procedures for classifying the risk of AI systems independently from their products. \n\nRisk Assessment Reports: Detailed reports documenting the risk level of the AI system based on the established criteria. \n\nProduct-AI Risk Mapping: A matrix or guide that clarifies the distinctions between the product's risk classification and that of the AI system. | Impact Explanation: Deliverables include: \n\nRisk Classification Reports: Documents outlining the specific risk categories of the AI system and the product separately, with detailed analysis and justification. \n\nProduct Impact Profiles: Separate profiles that detail the potential societal and individual impacts of the product and the AI system. \n\nRisk Communication Strategies: Plans for how to communicate the different risk of the AI system and product to relevant stakeholders."
    },
    {
        "control_name": "Conformity Indicators for AI Systems",
        "category": "AI Model Versioning",
        "description": "Implement a system to affix conformity markings on high-risk AI systems, indicating their compliance with established standards.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Recital 129: Emphasizes the need for digital CE markings., NIST 800-53: AC-3: Access Enforcement\nCM-5: Access Restrictions for Change\nSA-4: Acquisition Process, SCF: AST-01: Asset Governance - Governing high-risk AI assets with a focus on conformity marking to indicate compliance with standards.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring high-risk AI systems comply with legal and regulatory standards and marking them for conformity.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, including conformity marking for high-risk AI systems.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of AI systems with a focus on applying conformity indicators for high-risk systems.\nRSK-01: Risk Management Program - Including conformity marking processes in the risk management program for high-risk AI systems.\nTDA-01: Technology Development & Acquisition - Incorporating conformity markings into the development and acquisition process of high-risk AI systems.\nTDA-04: Documentation Requirements - Documenting the application of conformity markings on high-risk AI systems as part of compliance requirements.",
        "explainability": "Conformity Indicators for AI Systems is a control that emphasizes the establishment of indicators and metrics to assess the conformity of AI systems with specific requirements, standards, and regulations. The rationale for this control is to provide a structured approach to measuring and ensuring AI system compliance with relevant criteria. Conformity indicators are essential for regulatory compliance, quality assurance, and transparency. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of conformity indicators for AI systems and their role in structured compliance assessment, quality assurance, and transparency. \n\nConformity Indicator Guidelines: Guidelines outlining the criteria and processes for establishing and using conformity indicators in AI systems.\n \nConformity Assessment Reports: Reports demonstrating the effectiveness of conformity indicators in ensuring compliance and quality. | Responsibility Explanation: Conformity Marking System: A system for creating and managing conformity markings, including who is responsible for marking the AI systems. \n\nCompliance Verification Logs: Logs that record each instance of conformity verification and marking. \n\nMarking Compliance Certificates: Certificates or documents provided with the AI system that verify the conformity marking. | Data Explanation: The Conformity Marking System Description details the process and standards for applying conformity markings to AI systems. This should include the types of markings used, their meaning, and the verification process to ensure that markings are correctly applied. | Fairness Explanation: AI Conformity Marking Guidelines: Guidelines for the application of conformity markings that indicate compliance with fairness standards. \n\nFairness Compliance Marking Procedures: Procedures that detail the application of conformity markings to demonstrate fairness compliance. \n\nConformity and Fairness Tracking Records: Records tracking the application of conformity markings and their relationship to fairness. | Safety & Performance  Explanation: Conformity Assessment Documentation: Records detailing the assessment process and confirming that the AI system meets the necessary standards. \n\nConformity Marking Guidelines: A set of instructions on how to affix and display conformity markings on AI systems. \n\nCompliance Certificates: Official documents issued to AI systems that meet the required standards, allowing them to display conformity markings. | Impact Explanation: Deliverables include: \n\nConformity Marking Guidelines: Detailed documentation on how conformity indicators should be applied and what they signify. \n\nCompliance Certificates: Official certificates that serve as proof that the AI system has met all required standards. \n\nImpact Compliance Reports: Reports that describe how the conformity indicators correlate with reduced negative impacts of the AI system."
    },
    {
        "control_name": "Criteria for Standalone AI Risk Classification",
        "category": "AI Model Versioning",
        "description": "Develop detailed criteria that specifically address the risk classification of standalone AI systems, especially those not integrated into other products.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: PL-2: System Security and Privacy Plans\nPL-4: Rules of Behavior\nRA-3: Risk Assessment\nCA-7: Continuous Monitoring, SCF:  AST-31: Asset Categorization - Categorizing AI systems as assets and developing criteria for the risk classification of stand-alone AI systems.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring that the developed criteria for stand-alone AI systems comply with relevant regulations and standards.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the AI system portfolio with criteria for classifying the risk of stand-alone systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Integrating the criteria for stand-alone AI risk classification into project management practices.\nRSK-01: Risk Management Program - Implementing a risk management program that includes specific criteria for classifying the risk of stand-alone AI systems.\nRSK-02: Risk-Based Security Categorization - Establishing criteria for the security categorization of stand-alone AI systems based on their specific risk.\nRSK-04: Risk Assessment - Conducting risk assessments that incorporate criteria specific to stand-alone AI systems.\nTDA-01: Technology Development & Acquisition - Incorporating risk classification criteria into the development and acquisition processes for stand-alone AI systems.",
        "explainability": "Criteria for Stand-alone Standalone AI Risk Classification is a control that emphasizes the establishment of clear criteria and guidelines for classifying AI-related risk as stand-alone risk as standalone or integrated into broader risk categories. The rationale for this control is to provide a structured approach to distinguishing risk specific to AI systems from other general risk. risk. Clear criteria for classification help helps ensure that AI risk are appropriately identified, prioritized, and addressed. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of criteria for standalone AI risk classification and its role in structured risk management, prioritization, and decision making.\n\nClassification Criteria Guidelines: Guidelines outlining the criteria and processes for classifying AI-related risk as standalone.\n\nClassification Effectiveness Reports: Reports demonstrating the effectiveness of classification criteria in distinguishing AI-specific risk. | Responsibility Explanation: Standalone AI Classification Criteria: A set of criteria specifically designed for classifying the risk of standalone AI systems. \n\nClassification Criteria Documentation: Formal documentation that outlines the criteria and classification process for internal and external use. \n\nClassification Decision Records: Records of decisions made regarding the risk classification of standalone AI systems. | Data Explanation: Standalone AI Risk Classification Guidelines lay out the risk factors, assessment criteria, and classification thresholds specific to standalone AI systems. | Fairness Explanation: Standalone AI Risk Classification Criteria: Criteria specifically addressing the risk classification of standalone AI systems with respect to fairness. \n\nStandalone AI Fairness Evaluation Framework: A framework that evaluates standalone AI systems for risk to fairness. \n\nCriteria Development and Fairness Documentation: Documentation of the criteria development process, emphasizing fairness considerations. | Safety & Performance  Explanation: Risk Classification Framework: A document outlining the specific criteria and methodologies used to classify the risk of standalone AI systems. \n\nRisk Assessment Reports: Detailed analyses and documentation on the categorization of standalone AI systems according to the developed criteria. \n\nAI System Risk Profile: A descriptive profile for each AI system that includes its risk classification and the rationale behind it. | Impact Explanation: Deliverables include: \n\nRisk Classification Framework: A comprehensive document detailing the criteria for risk levels of stand-alone AI systems. \n\nRisk Assessment Reports: Detailed assessments for each AI system, classifying its risk level based on the established criteria.\n \nRegulatory Compliance Mapping: Documents that align the risk classification with applicable regulations to demonstrate how compliance impacts broader societal outcomes."
    },
    {
        "control_name": "Standard Adherence for AI Systems",
        "category": "AI Model Versioning",
        "description": "Design a protocol to ensure that high-risk AI systems consistently adhere to established technical and operational specifications.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nPL-4: Rules of Behavior, SCF:  CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring high-risk AI systems comply with established technical and operational standards.\nCFG-02: System Hardening Through Baseline Configurations - Using baseline configurations to ensure AI systems adhere to technical and operational specifications.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the AI system portfolio with a focus on standard adherence for high-risk systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including standard adherence protocols in project management for AI systems.\nRSK-01: Risk Management Program - Implementing a risk management program that assesses adherence to standards for high-risk AI systems.\nTDA-01: Technology Development & Acquisition - Incorporating standard adherence protocols in the development and acquisition of high-risk AI systems.\nTDA-07: Secure Development Environments - Ensuring secure development environments incorporate protocols for standard adherence in AI systems.",
        "explainability": "Standard Adherence for AI Systems is a control that emphasizes the importance of adhering to relevant standards, guidelines, and best practices when developing and deploying AI systems. The rationale for this control is to ensure that AI systems meet established standards and adhere to industry-specific guidelines, leading to improved quality, safety, and compliance. Adherence to standards is crucial for maintaining public trust, ensuring quality, and mitigating risk associated with AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of standard adherence for AI systems and its role in maintaining quality, safety, and compliance. \n\nAdherence Guidelines: Guidelines outlining the processes and best practices for adhering to relevant standards and guidelines in AI development and deployment. \n\nAdherence Assessment Reports: Reports demonstrating the effectiveness of standard adherence in improving AI system quality and safety. | Responsibility Explanation: Standard Adherence Protocol: A protocol that outlines the steps and responsibilities for ensuring AI systems adhere to established standards. \n\nAdherence Monitoring Logs: Logs that track adherence to standards throughout the AI system's lifecycle. life cycle.\n\nStandard Compliance Reports: Reports that document the AI system’s compliance with the relevant standards at various life cycle stages. | Data Explanation: Standard Adherence Protocol, should include the set of standards the AI systems are expected to meet, the procedures for checking adherence, and the documentation of compliance. | Fairness Explanation: Standard Adherence Protocol for Fairness: A protocol ensuring that high-risk AI systems adhere to standards that promote fairness. \n\nTechnical Specification and Fairness Compliance Guide: A guide that connects technical specifications with compliance to fairness standards. \n\nOperational Specification and Fairness Review Reports: Reports reviewing how operational specifications uphold or impact fairness. | Safety & Performance  Explanation: Standard Adherence Protocols: A set of formalized documents that detail the specific technical and operational specifications high-risk AI systems must meet. \n\nCompliance Checklists: Detailed checklists used to verify each aspect of AI systems against the set standards. \n\nAudit Reports: Comprehensive reviews provided by internal or external auditors that document the degree of compliance of AI systems with the established standards. | Impact Explanation: Deliverables include: \n\nStandard Adherence Protocol: A detailed guideline document specifying the technical and operational specifications required for high-risk AI systems. \n\nCompliance Certificates: Certificates issued to AI systems that meet the standards, indicating their adherence. \n\nSystem Audit Reports: Comprehensive reports from audits assessing the AI system's adherence to the standards."
    },
    {
        "control_name": "Independent AI Risk Classification Verification",
        "category": "AI Model Versioning",
        "description": "Design a mechanism that independently verifies an AI system's risk classification, ensuring it is not influenced by other regulatory classifications.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: RA-2: Security Categorization\nRA-3: Risk Assessment, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring high-risk AI systems comply with established technical and operational standards.\nCFG-02: System Hardening Through Baseline Configurations - Using baseline configurations to ensure AI systems adhere to technical and operational specifications.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the AI system portfolio with a focus on standard adherence for high-risk systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including standard adherence protocols in project management for AI systems.\nRSK-01: Risk Management Program - Implementing a risk management program that assesses adherence to standards for high-risk AI systems.\nTDA-01: Technology Development & Acquisition - Incorporating standard adherence protocols in the development and acquisition of high-risk AI systems.\nTDA-07: Secure Development Environments - Ensuring secure development environments incorporate protocols for standard adherence in AI systems.",
        "explainability": "Independent AI Risk Classification Verification is a control that emphasizes the verification of AI risk classification by independent, third-party entities. The rationale for this control is to ensure that the risk classification of AI systems is unbiased, accurate, and in compliance with established criteria. Independent verification adds an extra layer of assurance and trust, which is critical for regulatory compliance and transparency. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of independent AI risk classification verification and its role in ensuring unbiased, accurate, and compliant risk classification. \n\nVerification Guidelines: Guidelines outlining the processes and best practices for third-party verification of AI risk classification. \n\nVerification Reports: Reports demonstrating the effectiveness of independent verification to ensure the accuracy of AI risk classification. | Responsibility Explanation: Independent Verification Protocol: A protocol for the independent verification of AI system risk classifications, including roles and procedures. \n\nVerification Process Documentation: Documentation that details the independent verification process. \n\nIndependent Verification Reports: Reports generated from the independent verification process that document the findings. | Data Explanation: Independent Verification Mechanism Description outlining the independent processes and criteria used for verifying the AI system’s risk classification, separate from other regulatory frameworks. | Fairness Explanation: Independent Risk Classification Verification Protocol: A protocol outlining independent verification processes for AI risk classifications with fairness considerations. \n\nVerification Process and Fairness Assurance Plan: A plan that ensures the verification process for AI risk classification is fair and unbiased. \n\nIndependent Verification and Fairness Certification: Certification that an AI system's risk classification has been independently verified for fairness. | Safety & Performance  Explanation: Independent Risk Assessment Reports: Documents generated by third-party evaluators that describe the risk classification findings. \n\nRisk Classification Guidelines: Standards and criteria used to determine the risk level of AI systems. \n\nVerification Records: Records of the verification process and conclusions, demonstrating the independence of the risk assessment. | Impact Explanation: Deliverables include: \n\nVerification Report: A detailed document that outlines the findings of the independent risk classification verification process. \n\nRisk Classification Guidelines: Official guidelines that detail the criteria and processes for AI system risk classification. \n\nIndependent Audit Logs: Records from third-party auditors that demonstrate the independence and integrity of the verification process."
    },
    {
        "control_name": "AI System Risk Reclassification Protocol",
        "category": "AI Model Versioning",
        "description": "Develop and implement a structured protocol to re-evaluate and manage AI systems when their risk classification is upgraded to a higher level.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: CP-2: Contingency Plan\nCP-4: Contingency Plan Testing \nIR-4: Incident Handling\nIR-8: Incident Response Plan, SCF: AST-31: Asset Categorization - Systematically categorizing AI assets, including procedures for updating risk classifications.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring that the reclassification protocol adheres to relevant legal and regulatory requirements for AI systems.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the AI systems portfolio to account for reclassification and implementing appropriate management strategies.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including AI system risk reclassification protocols in project management practices.\nRSK-01: Risk Management Program - Integrating a protocol into the risk management program to handle upgraded risk classifications of AI systems.\nRSK-02: Risk-Based Security Categorization - Applying security categorization processes that accommodate reclassification of AI system risk.\nRSK-04: Risk Assessment - Performing thorough risk assessments to guide the reclassification of AI systems when their risk level increases.\nRSK-07: Risk Assessment Update - Regularly updating risk assessments to reflect changes in AI system classifications and ensuring appropriate management protocols are in place.",
        "explainability": "AI System Risk Reclassification Protocol is a control that emphasizes the establishment of a protocol for reclassifying the risk associated with AI systems as they evolve or new information becomes available. The rationale for this control is to ensure that the risk classification of AI systems remains accurate and up-to-date. up to date. Risk reclassification is essential for proactive risk management, decision-making, decision making, and transparency. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of the AI System Risk Reclassification Protocol and its role in maintaining accurate and up-to-date risk classification. \n\nReclassification Guidelines: Guidelines outlining the procedures and best practices for reclassifying AI system risk. \n\nReclassification Protocol Effectiveness Reports: Reports demonstrating the effectiveness of the reclassification protocol in keeping risk classification current and relevant. | Responsibility Explanation: Risk Reclassification Protocol Document: A formal document outlining the process and criteria for reclassifying the risk level of AI systems. \n\nReclassification Review Logs: Logs recording the outcomes of reclassification reviews, including any changes in risk level. \n\nRisk Reclassification Reports: Reports detailing the justification for any reclassification of AI systems to be communicated with relevant stakeholders. | Data Explanation: Risk Reclassification Protocol, which must detail the triggers for reclassification, the reevaluation process, and the management measures to be taken when an AI system is moved to a higher risk category. | Fairness Explanation: Risk Reclassification Fairness Protocol: A protocol that details the fair reevaluation of AI systems when their risk classification changes.\n\nReclassification Impact on Fairness Report: A report analyzing how changes in classification impact the fairness of the AI system's use. \n\nReclassification Procedure Documentation: Documentation of the reclassification procedures emphasizing their fairness implications. | Safety & Performance  Explanation: Reclassification Procedure Documentation: A detailed account of the steps and criteria for upgrading the risk level. \n\nRisk Reassessment Reports: Comprehensive analyses that lead to a revised risk classification. \n\nChange Management Logs: Documented evidence of the changes, including stakeholder approvals and system modifications. | Impact Explanation: Deliverables include: \n\nReclassification Protocol: A comprehensive document detailing the steps and criteria for risk escalation. \n\nImpact Assessment Report: A detailed analysis of the implications of the higher risk classification. \n\nChange Management Logs: Documented evidence of the steps taken in response to the reclassification."
    },
    {
        "control_name": "Change-Driven Conformity Assessments",
        "category": "AI Model Versioning",
        "description": "Establish a procedure to conduct conformity assessments for high-risk AI systems following significant system modifications, excluding routine updates and security patches.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 29: Application of a Conformity Assessment Body for Notification\nArticle 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control\nAnnex VII: Conformity based on an assessment of the quality management system and an assessment of the technical documentation, NIST 800-53: AC-2: Account Management\nCM-3: Configuration Change Control\nCM-4: Impact Analysis\nCM-6: Configuration Settings, SCF: CHG-02: Configuration Change Control - Implementing change control procedures that trigger conformity assessments for significant modifications to high-risk AI systems.\nCHG-03: Security Impact Analysis for Changes - Analyzing the security impact of significant changes to AI systems and assessing conformity post-change.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring conformity assessments align with regulatory compliance, especially after significant AI system changes.\nCFG-02: System Hardening Through Baseline Configurations - Assessing conformity of AI systems to baseline configurations after significant changes.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with a focus on conducting conformity assessments after major system changes.\nRSK-01: Risk Management Program - Incorporating conformity assessments into the risk management program, particularly after key modifications to AI systems.\nRSK-04: Risk Assessment - Conducting risk assessments that include conformity evaluations following significant modifications to AI systems.\nTDA-01: Technology Development & Acquisition - Evaluating conformity as part of the development and acquisition process after significant changes to AI systems.",
        "explainability": "Change-Driven Conformity Assessments is a control that emphasizes conducting conformity assessments when significant changes occur in AI systems. The rationale for this control is to ensure that AI systems remain compliant with standards, regulations, and best practices as they evolve. Change-driven conformity assessments help identify and address compliance issues related to system modifications. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of change-driven conformity assessments and their role in maintaining compliance during AI system evolution. \n\nConformity Assessment Guidelines: Guidelines outlining the processes and best practices for conducting conformity assessments after significant changes. \n\nConformity Assessment Reports: Reports demonstrating the effectiveness of change-driven conformity assessments in maintaining compliance. | Responsibility Explanation: Conformity Assessment Procedure: A detailed procedure for conducting conformity assessments in response to significant changes in AI systems. \n\nSystem Modification Logs: Logs that record all significant modifications made to AI systems. \n\nConformity Assessment Reports: Reports generated post assessment that document the AI system's compliance with standards post modification. | Data Explanation: Conformity Assessment Procedure Document, which outlines the triggers for new assessments, the criteria for what constitutes a significant modification, and the steps for conducting the assessment. | Fairness Explanation: Conformity Reassessment Procedure: A procedure for conducting conformity assessments after significant AI system changes, considering fairness implications. \n\nModification Impact Fairness Analysis: Analysis of how system modifications affect the fairness of AI operations.\n \nChange-Driven Compliance Certificates: Certificates that affirm the AI system's continued compliance with fairness standards after significant changes. | Safety & Performance  Explanation: Change Logs: Detailed records of significant system modifications. \n\nConformity Assessment Reports: Documents detailing the assessment and confirmation that the AI system modifications meet regulatory standards. \n\nPostchange Evaluation Protocols: A set of procedures and checklists used to evaluate the impact of system changes. | Impact Explanation: Deliverables include a detailed Change Impact Report that outlines the changes made, a Compliance Verification Document confirming adherence to all relevant regulations, and a Stakeholder Impact Review that addresses the effects on users and affected parties."
    },
    {
        "control_name": "Provider-Led AI Conformity Verification",
        "category": "AI Model Versioning",
        "description": "Enable AI system providers to execute conformity assessments for specific high-risk AI systems, while ensuring that certain critical systems undergo third-party assessments.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: SA-11: Developer Testing and Evaluation\nSR-3: Supply Chain Controls and Processes\nSR-6: Supplier Assessments and Reviews\nCA-2: Control Assessments\nCM-5: Access Restrictions for Change, SCF: CHG-03: Security Impact Analysis for Changes - Provider-led analysis of security impacts in AI systems, complemented by third-party assessments for significant or high-risk changes.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring provider-led conformity assessments comply with relevant legal and regulatory standards, while critical systems receive third-party verification.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of conformity assessments conducted by AI system providers, with third-party verifications for key systems.\nCPL-03: Cybersecurity & Data Privacy Assessments - Facilitating AI system providers to conduct cybersecurity and data privacy conformity assessments, supplemented by third-party evaluations for critical systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects to include provider-led conformity assessments and third-party reviews for critical systems.\nRSK-01: Risk Management Program - Implementing a risk management program that encompasses provider-led conformity assessments and third-party reviews for high-risk AI systems.\nRSK-04: Risk Assessment - Conducting risk assessments by AI system providers, with additional third-party assessments for high-risk or critical AI systems.\nTDA-01: Technology Development & Acquisition - Incorporating provider-led conformity assessments in the development and acquisition of AI systems, with third-party verification for critical systems.",
        "explainability": "Provider-Led AI Conformity Verification is a control that emphasizes the role of AI service providers in verifying the conformity of their AI systems with established standards and guidelines. The rationale for this control is to ensure that AI service providers take an active role in maintaining the compliance and quality of their AI offerings. Provider-led conformity verification adds an extra layer of assurance and trust, which is crucial for customers and regulatory compliance. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of provider-led AI conformity verification and its role in ensuring trust, compliance, and quality in AI services. \n\nVerification Guidelines: Guidelines outlining the processes and best practices for AI service providers in verifying conformity. \n\nVerification Reports: Reports demonstrating the effectiveness of provider-led conformity verification in maintaining quality and compliance. | Responsibility Explanation: Provider Conformity Verification Plan: A plan that outlines how providers will conduct conformity assessments, including criteria for when third-party assessments are required. \n\nSelf-Assessment Records: Records maintained by providers documenting their self-conducted conformity assessments. \n\nThird-Party Assessment Guidelines: Guidelines that specify the conditions and procedures for third-party conformity assessments of critical systems. | Data Explanation: Provider Conformity Verification Protocol, outlining the standards for self-assessment, the conditions under which third-party assessments are required, and the documentation process for both. | Fairness Explanation: Provider Conformity Verification Guidelines: Guidelines that direct providers on conducting conformity assessments with a focus on fairness.\n \nThird-Party Fairness Verification Standards: Standards for third-party assessments to verify fairness in critical systems. \n\nProvider-Led Assessment Fairness Logs: Logs documenting provider-led assessments and third-party verification activities concerning fairness. | Safety & Performance  Explanation: Internal Conformity Assessment Reports: Documents created by providers demonstrating self-assessment outcomes. \n\nThird-Party Assessment Certifications: Certificates issued by external auditors validating the conformity of critical systems. \n\nProvider Assessment Protocols: Detailed methodology for how providers conduct internal conformity assessments. | Impact Explanation: Deliverables would include a Provider Self-Assessment Report, which details the provider's own evaluation of the AI system against predefined criteria and a Third-Party Verification Statement, which is an independent attestation of the AI system’s compliance with regulatory and ethical standards."
    },
    {
        "control_name": "Granular AI Risk Categorization",
        "category": "AI Model Versioning",
        "description": "Implement a detailed and granular risk categorization framework for AI systems, providing clarity in their classification based on specific functionalities and potential impacts.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: RA-2: Security Categorization\nRA-3: Risk Assessment\nCM-8: System Component Inventory, SCF: AST-31: Asset Categorization - Categorizing AI systems as assets with a focus on granular risk distinctions based on functionalities and impacts.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring that AI risk categorization aligns with compliance requirements and captures granular risk nuances.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI systems portfolio with detailed categorization criteria, reflecting their specific risk.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including granular risk categorization in project management practices for AI systems.\nRSK-01: Risk Management Program - Integrating granular AI risk categorization into the overall risk management program.\nRSK-02: Risk-Based Security Categorization - Developing a granular risk categorization framework for AI systems, taking into account specific functionalities and potential impacts.\nRSK-04: Risk Assessment - Conducting detailed risk assessments that contribute to the granular categorization of AI systems.\nTDA-01: Technology Development & Acquisition - Reflecting granular risk categorization in the development and acquisition processes for AI systems.",
        "explainability": "Granular AI Risk Categorization is a control that emphasizes the categorization of AI-related risk into fine-grained and specific categories. The rationale for this control is to provide a structured approach to identifying and understanding the diverse and nuanced risk associated with AI systems. Granular risk categorization is essential for effective risk management, targeted mitigation, and informed decision-making. decision making. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of granular AI risk categorization and its role in structured risk management, targeted mitigation, and informed decision making.\n\nCategorization Guidelines: Guidelines outlining the criteria and processes for categorizing AI-related risk in a granular manner. \n\nCategorization Effectiveness Reports: Reports demonstrating the effectiveness of granular risk categorization in improving risk management and decision making. | Responsibility Explanation: Risk Categorization Framework: A document that provides a detailed and granular categorization of AI systems' risk.\n\nCategorization Detailing Manual: A manual that describes the categorization process and specific criteria for each risk category. \n\nRisk Category Assignment Records: Records of the risk categorization assigned to each AI system, along with the rationale for the categorization. | Data Explanation: Risk Categorization Framework Document, which should detail the various levels of risk categories, the criteria for each, and the process by which AI systems are classified according to their functionalities and potential impacts. | Fairness Explanation: Detailed Risk Categorization Framework: A framework that categorizes AI risk in detail, taking fairness implications into account.\n \nFunctionality-Specific Fairness Impact Reports: Reports that outline the fairness impacts based on the specific functionalities of AI systems. \n\nGranular Risk Categorization Fairness Certificates: Certificates that confirm the fairness of AI systems based on a granular risk categorization. | Safety & Performance  Explanation: Risk Categorization Framework: A detailed structure defining various risk levels based on specific criteria. \n\nCategorized AI Systems List: A regularly updated list displaying AI systems along with their respective risk categories. | Impact Explanation: AI Risk Categorization Report: A comprehensive document detailing the classification of AI risk. This report should include an overview of the AI system, a list of identified risk categorized by type (e.g., privacy, bias, security), and the potential impact of each risk on stakeholders and society.\n \nImpact Analysis Studies: Detailed case studies or research papers analyzing the impacts of similar AI systems in the industry, providing a contextual background for the categorization of risk."
    },
    {
        "control_name": "AI Risk Classification Challenge Mechanism",
        "category": "AI Model Versioning",
        "description": "Establish a formal mechanism allowing AI system providers to challenge or seek clarification on the risk classification of their products with supervisory authorities.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: SR-6: Supplier Assessments and Reviews\nAC-22: Publicly Accessible Content\nAU-5: Response to Audit Logging Process Failures\nAU-6: Audit Record Review, Analysis, and Reporting\n, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Establishing formal processes for AI system providers to challenge risk classifications in compliance with regulatory frameworks.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight processes for managing disputes and challenges regarding AI system risk classifications.\nCPL-03: Cybersecurity & Data Privacy Assessments - Conducting assessments and providing a pathway for AI system providers to challenge or clarify assessment outcomes.\nGOV-01: Cybersecurity & Data Protection Governance Program - Governance programs that include protocols for AI providers to challenge risk classifications.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Documentation protocols that include guidelines for challenging AI system risk classifications.\nGOV-07: Contacts With Authorities - Facilitating communication channels between AI system providers and supervisory authorities for risk classification challenges.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI systems portfolio with established channels for risk classification challenges.\nRSK-01: Risk Management Program - Incorporating mechanisms within the risk management program for providers to challenge AI system risk classifications.\nRSK-03: Risk Identification - Identifying risk in AI systems and providing a mechanism for providers to challenge these identifications.",
        "explainability": "AI Risk Classification Challenge Mechanism is a control that emphasizes the establishment of a mechanism for stakeholders to challenge the risk classification of AI systems. The rationale for this control is to ensure transparency, accountability, and the involvement of stakeholders in the risk classification process. The challenge mechanism allows for a structured process to address disputes and concerns related to AI risk classification. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of the AI Risk Classification Challenge Mechanism and its role in ensuring transparency, accountability, and stakeholder involvement in the risk classification process. \n\nChallenge Mechanism Guidelines: Guidelines outlining the processes and best practices for stakeholders to challenge risk classification. \n\nChallenge Mechanism Effectiveness Reports: Reports demonstrating the effectiveness of the challenge mechanism in addressing disputes and concerns. | Responsibility Explanation: Classification Challenge Protocol: A protocol that outlines the process for providers to challenge or seek clarification on risk classifications. \n\nChallenge Submission Templates: Templates for providers to use when submitting challenges or requests for clarification. \n\nChallenge Outcome Documentation: Documentation that records the outcomes of classification challenges, including any changes to the classification or clarifications provided. | Data Explanation: Risk Classification Challenge Protocol, which outlines the procedure for providers to raise challenges or seek clarifications regarding their AI system's risk classification, including timelines and the format for submission. | Fairness Explanation: Risk Classification Challenge Protocol: A protocol that outlines the process for challenging AI risk classifications, with fairness at its core. \n\nClassification Challenge and Fairness Review Guidelines: Guidelines for reviewing classification challenges to ensure they are conducted fairly. \n\nChallenge Outcome Fairness Documentation: Documentation of challenge outcomes and their adherence to fairness principles. | Safety & Performance  Explanation: Challenge Protocol Document: A detailed guide on how to initiate a challenge or request for clarification regarding AI risk classification.\n \nRisk Classification Challenge Form: A standardized form for providers to submit their challenge or clarification requests. | Impact Explanation: Challenge Mechanism Guidelines: A detailed document outlining the procedure for challenging the risk classification. This should include criteria for submission, required documentation, and a timeline for the review process. \n\nCase Logs and Resolution Reports: Records of all challenges submitted, including the rationale for each challenge, actions taken, and the final decision. These reports provide transparency and accountability in the challenge process."
    },
    {
        "control_name": "Technical Documentation Maintenance",
        "category": "AI Model Versioning",
        "description": "Maintain comprehensive technical documentation for high-risk AI systems, detailing system design, functionality, and operational guidelines.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 11: Technical Documentation, NIST 800-53: CM-2: Baseline Configuration\nCM-3: Configuration Change Control\nCM-4:  Impact Analysis\nCM-5: Access Restrictions for Change, SCF: AST-02: Asset Inventories - Including high-risk AI systems in asset inventories with detailed technical documentation.\nAST-04: Network Diagrams & Data Flow Diagrams (DFDs) - Creating and maintaining network and data flow diagrams as part of AI system documentation.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with regulations by maintaining detailed technical documentation for high-risk AI systems.\nCFG-01: Configuration Management Program - Managing AI system configurations and documenting changes as part of technical documentation.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the portfolio of AI systems with an emphasis on maintaining comprehensive technical documentation.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including requirements for maintaining technical documentation in AI project management.\nRSK-04: Risk Assessment - Documenting risk assessments as part of the technical documentation for high-risk AI systems.\nTDA-04: Documentation Requirements - Ensuring comprehensive documentation of AI system design, functionality, and operational guidelines.",
        "explainability": "Technical Documentation Maintenance is a control that emphasizes the importance of maintaining up-to-date technical documentation for AI systems. The rationale for this control is to ensure that comprehensive technical documentation is available to support understanding, maintenance, and troubleshooting of AI systems. Well-maintained technical documentation is crucial for transparency, accountability, and efficient system management. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of technical documentation maintenance and its role in providing transparency, accountability, and efficient system management for AI systems. \n\nDocumentation Maintenance Guidelines: Guidelines outlining the processes and best practices for maintaining technical documentation. \n\nDocumentation Maintenance Reports: Reports demonstrating the effectiveness of documentation maintenance in supporting system understanding and management. | Responsibility Explanation: Technical Documentation Guidelines: Guidelines detailing the standards and scope of technical documentation required for high-risk AI systems. \n\nSystem Documentation Repository: A centralized repository where all technical documentation is maintained. \n\nDocumentation Update Logs: Logs that track updates to technical documentation, including who made the update and when. | Data Explanation: Technical Documentation Standards and Maintenance Procedures, which describe the format, content, and update frequency for the technical documentation of high-risk AI systems. | Fairness Explanation: Technical Documentation Fairness Standards: Standards for maintaining technical documentation that ensure the fairness of high-risk AI systems is transparent. \n\nDocumentation Update and Fairness Compliance Reports: Reports that assess the compliance of documentation updates with fairness standards. \n\nOperational Guideline Fairness Certificates: Certificates that attest to the fairness of the operational guidelines detailed in technical documentation. | Safety & Performance  Explanation: System Design Document: A comprehensive report that describes the AI system's architecture, including data flows, processing techniques, and algorithms used. \n\nOperational Guidelines: A set of procedures and protocols for operating the AI system safely, including fail-safes and redundancies. \n\nPerformance Metrics Report: A document detailing the key performance indicators, accuracy benchmarks, reliability data, and security features. \n\nRobustness Analysis: An analysis report that assesses the system's ability to handle various inputs and conditions without failure. | Impact Explanation: Comprehensive Technical Manuals: Detailed guides that describe the AI system's architecture, algorithms, data flow, and functionality.\n \nOperational Guidelines: Documents that explain how to operate the AI system, including handling exceptions and mitigating risk. \n\nUpdate and Revision Histories: Records of changes made to the AI system, illustrating how it has evolved over time and the impact of these changes."
    },
    {
        "control_name": "System Specification Disclosure",
        "category": "AI Model Versioning",
        "description": "Document and disclose the characteristics, capabilities, and limitations of high-risk AI systems to relevant stakeholders.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: PL-4: Rules of Behavior\nPL-2: System Security and Privacy Plans\nSA-4: Acquisition Process\nSA-8: Security and Privacy Engineering Principles, SCF: AST-01: Asset Governance - Governing AI assets by ensuring accurate and comprehensive disclosure of system specifications.\nAST-04: Network Diagrams & Data Flow Diagrams (DFDs) - Using diagrams to help document and disclose AI system specifications.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring disclosures about AI system specifications comply with relevant regulations.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Publishing detailed documentation on AI system specifications for stakeholders.\nGOV-07: Contacts With Groups & Associations - Facilitating disclosure of AI system specifications through contacts with relevant groups and associations.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI system portfolios with a focus on transparent disclosure of specifications.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Integrating system specification disclosure into AI project management practices.\nTDA-04: Documentation Requirements - Documenting the characteristics, capabilities, and limitations of high-risk AI systems for stakeholder awareness.",
        "explainability": "System Specification Disclosure is a control that emphasizes the disclosure of detailed system specifications for AI systems. The rationale for this control is to ensure that stakeholders have access to comprehensive information about the AI system's design, components, and functionalities. System specification disclosure enhances transparency, accountability, and understanding of AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of system specification disclosure and its role in providing transparency, accountability, and an understanding of AI systems. \n\nDisclosure Guidelines: Guidelines outlining the processes and best practices for disclosing system specifications. \n\nDisclosure Reports: Reports demonstrating the effectiveness of system specification disclosure in enhancing understanding and transparency. | Responsibility Explanation: Specification Disclosure Policy: A policy that outlines the requirements for documenting and disclosing AI system specifications. \n\nStakeholder Disclosure Records: Records of all disclosures made to stakeholders regarding the AI system’s specifications. \n\nCapabilities and Limitations Summaries: Summaries that provide an overview of the AI system’s characteristics, capabilities, and limitations for stakeholder reference. | Data Explanation: System Specification Disclosure Document, which must provide comprehensive details on the AI system's characteristics, capabilities, limitations, and any other relevant information to stakeholders. | Fairness Explanation: AI System Disclosure Documentation: Documentation that fully describes the AI system's specifications with an emphasis on fairness implications. \n\nCapability and Limitation Transparency Reports: Reports that detail the system's capabilities and limitations to ensure stakeholders are fairly informed. \n\nStakeholder Fairness Communication Logs: Logs of communications with stakeholders regarding system specifications, maintaining a focus on fairness. | Safety & Performance  Explanation: Specification Sheet: A detailed document listing all operational parameters, capabilities, and limitations of the AI system. \n\nLimitations Report: An in-depth analysis of the system's limitations and the implications for safety and performance. \n\nStakeholder Briefing Documents: Prepared materials that communicate the system's specifications in relation to safety and performance to stakeholders. | Impact Explanation: System Specification Reports: Detailed documents outlining the AI system's characteristics, capabilities, and limitations. These should include technical specifications, performance metrics, and known constraints. \n\nStakeholder Briefs: Concise summaries tailored to different stakeholder groups, explaining the system's specifications in an accessible manner. \n\nDisclosure Logs: Records of all disclosures made, including the date, recipient, and content of the disclosure, ensuring transparency and accountability."
    },
    {
        "control_name": "Digital Instruction Provision",
        "category": "AI Model Training",
        "description": "Ensure that high-risk AI systems are accompanied by comprehensive, clear, and concise instructions in a durable digital format. These instructions should guide users on system operation, maintenance, and decision-making processes.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-4: Acquisition Process\nSA-9: External System Services\n, SCF: AST-04: Network Diagrams & Data Flow Diagrams (DFDs) - Including operation and maintenance instructions in network and data flow documentation.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with regulations that require provision of digital instructions for high-risk AI systems.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Publishing digital instructions for AI systems as part of cybersecurity and data protection documentation.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI systems with an emphasis on providing clear digital instructions for users.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Integrating the provision of digital instructions into AI system project management processes.\nTDA-01: Technology Development & Acquisition - Developing and acquiring AI systems with a requirement for comprehensive digital instructions.\nTDA-04: Documentation Requirements - Creating comprehensive digital instructions for high-risk AI systems as part of documentation requirements.",
        "explainability": "Digital Instruction Provision is a control that emphasizes the provision of digital instructions or user guides for AI systems. The rationale for this control is to ensure that users and stakeholders have access to clear and user-friendly instructions for operating, interacting with, and understanding the AI system. Digital instruction provision enhances usability, user experience, and reduces the risk of misuse or misunderstanding of the AI system. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of digital instruction provision and its role in enhancing usability, user experience, and reducing the risk of misuse or misunderstanding of AI systems. \n\nInstruction Provision Guidelines: Guidelines outlining the processes and best practices for providing digital instructions. \n\nUser Feedback Reports: Reports demonstrating the effectiveness of digital instruction provision in enhancing user experience and reducing misunderstandings. | Responsibility Explanation: Digital Instruction Manual: A comprehensive digital manual that provides instructions on operating, maintaining, and understanding the AI system. \n\nInstruction Dissemination Log: A log that tracks the distribution of digital instructions to users. \n\nUser Feedback on Instructions: Feedback collected from users on the clarity and usefulness of the digital instructions provided. | Data Explanation: Digital Instruction Manual, which outlines the operational, maintenance, and decision-making guidance for the high-risk AI system. The manual should be easily accessible and user-friendly. | Fairness Explanation: Digital Instruction Fairness Guidelines: Guidelines that ensure instructions for high-risk AI systems are comprehensive and fair. \n\nInstruction Clarity and Fairness Compliance Certificates: Certificates that confirm the instructions meet clarity and fairness standards. \n\nDigital Instruction Fairness Audit Results: Audit results that evaluate the fairness of the instruction provision process. | Safety & Performance  Explanation: User Manual: A comprehensive guide in digital format detailing the operation and maintenance of the AI system. \n\nQuick Reference Guide: A concise document for common tasks and troubleshooting. \n\nDecision-Making Flowcharts: Digital diagrams that guide users through key processes and decision points in operating the AI system. | Impact Explanation: Digital User Manuals: In-depth digital guides that detail system operation, maintenance, and decision-making processes. \n\nQuick Reference Guides: Concise digital documents or infographics for quick reference, covering key operational aspects of the AI system. \n\nVersion Control Records: Documentation tracking updates and changes to the instructions, ensuring users always have access to the most current information."
    },
    {
        "control_name": "Secure AI Training Data Sources",
        "category": "AI Model Training",
        "description": "Prioritize the use of secure and trusted datasets for AI training to mitigate the risk of adversarial attacks and data poisoning.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SC-7: Boundary Protection\nSA-9: External System Services\nSI-3: Malicious Code Protection, SCF: AST-02: Asset Inventories - Including secure and trusted datasets in the asset inventory for AI training purposes.\nMON-01: Continuous Monitoring - Continuously monitoring AI training datasets for signs of adversarial attacks or data poisoning.\nDCH-01: Data Protection - Protecting data used in AI training to ensure its security and trustworthiness.\nDCH-02: Data & Asset Classification - Classifying AI training data assets based on their level of security and trustworthiness.\nDCH-06: Media Storage - Securely storing training datasets to prevent unauthorized access or tampering.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI training projects with a focus on secure and trusted data sources.\nRSK-01: Risk Management Program - Including the management of risk related to AI training data sources in the overall risk management program.\nRSK-04: Risk Assessment - Assessing the risk associated with AI training data sources and implementing controls to mitigate these risk.\nTDA-01: Technology Development & Acquisition - Ensuring secure and trusted data sources are used during the development and acquisition of AI systems.",
        "explainability": "Secure AI Training Data Sources is a control that emphasizes the importance of ensuring the security and integrity of training data used in AI systems. The rationale for this control is to protect AI systems from potential data breaches, tampering, or contamination. Secure training data sources are essential for maintaining the reliability, accuracy, and trustworthiness of AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of secure AI training data sources and their role in safeguarding AI system integrity and trustworthiness. \n\nData Source Security Guidelines: Guidelines outlining the processes and best practices for securing training data sources. \n\nData Security Assessment Reports: Reports demonstrating the effectiveness of secure training data sources in ensuring data integrity and system trustworthiness. | Responsibility Explanation: Data Sourcing Policy: A policy that outlines the criteria for selecting secure and trusted data sources for AI training. \n\nTraining Data Provenance Logs: Logs that record the sources, access, and use of training data.\n \nData Security Compliance Certificates: Certificates or attestations that verify the security and integrity of the training data used. | Data Explanation: Data Security Protocol for AI Training outlining the criteria for dataset selection, the security measures to protect training data, and the procedures for monitoring data integrity. | Fairness Explanation: Secure Training Data Selection Policy: A policy that outlines the criteria for selecting secure and unbiased training data sources. \n\nData Security and Fairness Assurance Plan: A plan that ensures the chosen data sources are secure and contribute to the fairness of the AI system. \n\nSecure Data Source Fairness Certification: Certification that the data sources used for AI training are secure and uphold fairness. | Safety & Performance  Explanation: Data Source Audit Report: A detailed review of the security and reliability of data sources used for training. \n\nData Validation Protocols: Documentation of the procedures and checks used to validate data integrity before use in training. \n\nData Provider Agreements: Contracts or agreements that stipulate the security expectations and requirements for data providers. | Impact Explanation: Data Source Security Audits: Detailed reports on the security measures and vetting processes used for each dataset. \n\nData Integrity Policies: Documents outlining the standards and practices implemented to ensure the integrity and security of training data. \n\nIncident Response Records: Logs of any security incidents related to data sources, including responses and resolutions."
    },
    {
        "control_name": "AI Data Segregation Strategies",
        "category": "AI Model Training",
        "description": "Isolate AI training data from production data to minimize the risk of data breaches or compromises.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-20: Use of External Systems\nAC-21: Information Sharing\nAC-22: Publicly Accessible Content\nCM-12: Information Location, SCF: AST-02: Asset Inventories - Including AI training data in asset inventories with protocols for segregation from production data.\nCFG-02: System Hardening Through Baseline Configurations - Establishing baseline configurations that include segregation strategies for AI training data.\nDCH-01: Data Protection - Implementing data protection measures that include the segregation of AI training data from production data.\nDCH-02: Data & Asset Classification - Classifying AI training data to facilitate segregation from production data.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Project management practices that prioritize AI data segregation to minimize risk.\nRSK-01: Risk Management Program - Incorporating AI data segregation strategies into the overall risk management program.\nRSK-04: Risk Assessment - Assessing risk related to the integration of AI training and production data and implementing segregation strategies.\nTDA-01: Technology Development & Acquisition - Considering data segregation in the development and acquisition of AI systems.",
        "explainability": "AI Data Segregation Strategies is a control that emphasizes the use of strategies to segregate different types of data used in AI systems. The rationale for this control is to ensure that data with varying sensitivity levels or usage purposes is appropriately segregated to prevent unauthorized access or data leakage. Data segregation strategies are essential for data privacy, security, and regulatory compliance. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of AI Data Segregation Strategies and their role in safeguarding data privacy, security, and regulatory compliance. \n\nSegregation Strategy Guidelines: Guidelines outlining the processes and best practices for segregating data in AI systems. \n\nSegregation Effectiveness Reports: Reports demonstrating the effectiveness of data segregation strategies in protecting data and ensuring compliance. | Responsibility Explanation: Data Segregation Protocol: A protocol that outlines how training data should be segregated from production data. \n\nData Segregation Implementation Records: Records documenting the implementation of data segregation strategies. \n\nData Breach Response Plan: A plan detailing how to respond in case a data breach occurs despite segregation measures. | Data Explanation: Data Segregation Policy and Implementation Guide detailing the separation of environments, access controls, and the methods for ensuring segregation between training and production datasets. | Fairness Explanation: Data Segregation and Fairness Policy: A policy that mandates the segregation of AI training data from production data to uphold fairness. \n\nSegregation Strategy Fairness Impact Report: A report evaluating how data segregation strategies affect the fairness of AI systems. \n\nData Breach and Fairness Response Records: Records of any data breaches and the measures taken to maintain fairness despite such incidents. | Safety & Performance  Explanation: Data Management Policy: A document outlining the procedures for handling different datasets.\n\nAccess Control Logs: Records showing who has accessed the training and production datasets. \n\nData Segregation Architecture Diagrams: Visual representations of how data is segregated within the system. | Impact Explanation: Data Segregation Policy Documents: Formal documents outlining the strategies and protocols for segregating AI training data from production data, including technical and procedural details. \n\nSystem Architecture Diagrams: Visual representations showing the segregation of data flows and storage between training and production environments. \n\nAudit Trail Records: Logs that track the implementation and maintenance of data segregation, ensuring compliance with the policy."
    },
    {
        "control_name": "Training Data Representativeness Over Time",
        "category": "AI Model Training",
        "description": "Periodically assess the training data's relevance to ensure it remains representative of the evolving operational environment.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-11: Developer Testing and Evaluation\nPM-6: Measures of Performance\nSR-11: Component Authenticity\nSR-12: Component Disposal\nSA-23: Specialization, SCF: AST-02: Asset Inventories - Cataloging AI training data as an asset and tracking its relevance and representativeness over time.\nCFG-02: System Hardening Through Baseline Configurations - Updating baseline configurations to reflect changes in AI training data representativeness.\nMON-01: Continuous Monitoring - Continuously monitoring AI training data to ensure its ongoing relevance and representativeness.\nDCH-01: Data Protection - Protecting the integrity of AI training data and ensuring it is updated to reflect current environments.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the AI systems portfolio with an emphasis on maintaining relevant and representative training data.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including the representativeness of AI training data in project management and review processes.\nRSK-01: Risk Management Program - Incorporating the assessment of training data representativeness into the risk management program.\nRSK-04: Risk Assessment - Regularly assessing the risk associated with the representativeness of AI training data over time.\nTDA-01: Technology Development & Acquisition - Considering the evolving representativeness of training data in the development of AI systems.",
        "explainability": "Training Data Representativeness Over Time is a control that emphasizes the need to maintain the representativeness of training data used in AI systems over time. The rationale for this control is to ensure that the training data remains reflective of the real-world conditions and continues to provide accurate and relevant results. Maintaining data representativeness is essential for the ongoing accuracy and reliability of AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of training data representativeness over time and its role in ensuring ongoing accuracy and reliability of AI systems. \n\nRepresentativeness Maintenance Guidelines: Guidelines outlining the processes and best practices for preserving data representativeness. \n\nRepresentativeness Assessment Reports: Reports demonstrating the effectiveness of maintaining training data representativeness in achieving accurate and relevant results. | Responsibility Explanation: Data Relevancy Review Plan: A document that outlines how and when the training data will be reviewed for ongoing relevancy and representativeness. \n\n Representativeness Assessment Reports: Periodic reports that evaluate the training data's current representation of real-world conditions. \n\n Data Update Logs: Logs detailing updates made to the training dataset in response to relevancy assessments. | Data Explanation: Data Relevance Review Protocol, which outlines the frequency and methodology for assessing the representativeness of the training data over time. | Fairness Explanation: Representativeness Review Protocol: A protocol for regularly reviewing the representativeness of training data with fairness as a key concern. \n\nData Relevance and Fairness Update Guidelines: Guidelines that ensure training data remains relevant and fair over time. \n\nRepresentativeness and Fairness Evaluation Reports: Reports evaluating the representativeness of data and its fairness implications over time. | Safety & Performance  Explanation: Data Update Reports: Documented analysis of how the training data has changed and why. \n\nData Quality Metrics: Regularly updated statistics that measure the representativeness of the training data. \n\nRetraining Logs: Records of when and how the model was retrained with new data. | Impact Explanation: Periodic Assessment Reports: Detailed documents produced from regular assessments, outlining the current representativeness of the training data in relation to the operational environment. \n\nData Evolution Logs: Logs tracking changes in the operational environment and corresponding updates to the training data. \n\nRepresentativeness Metrics and Benchmarks: Standards and metrics used to evaluate the representativeness of the training data."
    },
    {
        "control_name": "Advanced Regularization Techniques",
        "category": "AI Model Training",
        "description": "Deploy techniques like Lasso regression L1 or L2 ridge regression regularization to prevent model overfitting, enhancing AI system resilience against membership inference attacks.\nGoal of predicting future sales and determine which features are most important.",
        "risk_level": "General",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behavior that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behavior, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: SA-11: Developer Testing and Evaluation\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSI-4: System Monitoring, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Complying with relevant AI regulations by implementing advanced regularization techniques.\nCFG-02: System Hardening Through Baseline Configurations - Integrating regularization techniques into baseline configurations for AI systems.\nDCH-01: Data Protection - Ensuring data protection in AI systems through the use of regularization techniques.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the portfolio of AI systems with a focus on advanced regularization techniques.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on implementing regularization techniques.\nRSK-01: Risk Management Program - Integrating risk management strategies that include the use of regularization techniques to prevent overfitting.\nRSK-04: Risk Assessment - Assessing the risk of overfitting in AI models and the effectiveness of regularization techniques in mitigating these risk.\nTDA-01: Technology Development & Acquisition - Including advanced regularization techniques in the criteria for AI system development and acquisition.",
        "explainability": "Advanced Regularization Techniques is a control that emphasizes the use of advanced techniques for regularization in AI models. The rationale for this control is to improve the robustness, generalization, and performance of AI models while mitigating issues like overfitting. Advanced regularization techniques play a vital role in ensuring that AI models can handle diverse data and make accurate predictions. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of advanced regularization techniques and their role in improving model robustness and generalization. \n\nRegularization Technique Guidelines: Guidelines outlining the processes and best practices for implementing advanced regularization techniques. \n\nModel Performance Reports: Reports demonstrating the effectiveness of advanced regularization techniques in enhancing AI model performance. | Responsibility Explanation: Regularization Implementation Guide: A guide detailing the regularization techniques to be used, such as L1 or L2, and the rationale for their selection. \n\n Model Regularization Records: Records of the implementation of regularization techniques within AI models. \n\nOverfitting Mitigation Reports: Reports that document the efficacy of regularization techniques in reducing overfitting and enhancing model resilience. | Data Explanation: Regularization Technique Implementation Guide detailing the use of L1 and L2 regularization in AI models, including the scenarios for their application and the expected impact on model performance and security. | Fairness Explanation: Regularization and Fairness Strategy Document: A document outlining how regularization techniques contribute to the fairness of AI systems. \n\nOverfitting Prevention and Fairness Impact Report: A report detailing the impact of regularization on preventing overfitting and its importance for maintaining fairness. \n\nModel Resilience and Fairness Certificates: Certificates confirming that regularization techniques have been successfully applied to enhance both model resilience and fairness. | Safety & Performance  Explanation: Regularization Techniques Report detailing the regularization methods used, parameters selected, and rationale for their choice.\n\nModel Validation Reports showing performance metrics pre- and post-regularization.\n\nOverfitting Mitigation Plan outlining the strategies for monitoring and mitigating overfitting. | Impact Explanation: Regularization Implementation Reports: Documentation detailing the use of L1 or L2 regularization techniques in the AI models, including technical specifics and rationales. \n\nModel Performance Records: Records of model performance metrics pre- and post-regularization, demonstrating the impact on overfitting and resilience. \n\nSecurity Assessment Findings: Reports of security assessments specifically evaluating the resilience of the AI system against membership inference attacks."
    },
    {
        "control_name": "AI Data Validation & Verification",
        "category": "AI Model Training",
        "description": "Implement rigorous protocols to validate and verify training data, ensuring its quality and relevance for AI modeling.",
        "risk_level": "General",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behavior that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behavior, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: CM-4: Impact Analysis\nCM-5: Access Restrictions for Change\nSA-3: System Development Life Cycle\nSA-9: External System Services, SCF: CFG-02: System Hardening Through Baseline Configurations - Incorporating data validation and verification in baseline configurations for AI systems.\nDCH-01: Data Protection - Protecting AI training data integrity through validation and verification protocols.\nDCH-06: Media Storage - Securely storing and managing AI training data, including protocols for validation and verification.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI systems with a focus on rigorous training data validation and verification.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including validation and verification of AI training data in cybersecurity and data privacy project management practices.\nRSK-01: Risk Management Program - Incorporating training data validation and verification into the overall risk management strategy for AI systems.\nRSK-04: Risk Assessment - Assessing the risk associated with training data quality and the effectiveness of validation and verification protocols.\nTDA-01: Technology Development & Acquisition - Ensuring that validation and verification of training data are integral to AI system development and acquisition processes.",
        "explainability": "AI Data Validation & Verification is a control that emphasizes the importance of validating and verifying the quality, accuracy, and reliability of data used in AI systems. The rationale for this control is to ensure that the data is trustworthy and suitable for the intended AI applications. Data validation and verification are essential for reducing errors, biases, and improving the overall performance of AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of AI Data Validation & Verification and its role in ensuring data quality and reliability for AI applications. \n\nValidation & Verification Guidelines: Guidelines outlining the processes and best practices for validating and verifying data. \n\nData Quality Reports: Reports demonstrating the effectiveness of data validation and verification in enhancing data quality and AI system performance. | Responsibility Explanation: Data Validation Protocol: A comprehensive protocol for the validation and verification of AI training data. \n\nValidation and Verification Records: Detailed records of all validation and verification activities conducted on AI training data. \n\nData Quality Reports: Reports that summarize the findings from data validation and verification processes, highlighting the data’s quality and fitness for use. | Data Explanation: Data Validation and Verification Protocols describing the methods and criteria used to check the quality and relevance of data for training AI models. | Fairness Explanation: Data Validation and Fairness Protocol: A protocol that establishes rigorous procedures for data validation and verification with an emphasis on fairness.\n \nValidation Process Fairness Evaluation Reports: Reports evaluating the fairness of the data validation and verification processes. \n\nData Quality and Fairness Assurance Documentation: Documentation that certifies the training data's quality and its compliance with fairness standards. | Safety & Performance  Explanation: Data Validation Report that documents the data validation processes, data quality metrics, and verification results.\n\nData Quality Assurance Plan detailing the procedures for ongoing data quality checks.\n\nTraining Data Certification issued by data experts confirming the dataset’s validity for model training. | Impact Explanation: Data Validation Protocols: Detailed guidelines and procedures for validating the quality and relevance of training data. \n\nVerification Reports: Comprehensive reports documenting the validation and verification processes, findings, and any corrective actions taken. \n\nQuality Assurance Logs: Ongoing logs that record the results of regular data quality checks and verifications."
    },
    {
        "control_name": "Secure AI Training Data Protocols",
        "category": "AI Model Training",
        "description": "Prioritize the use of secure and trusted datasets for AI training, mitigating the risk of adversarial attacks and data poisoning.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nSC-13: Cryptographic Protection\nSI-3: Malicious Code Protection, SCF: AST-02: Asset Inventories - Including secure and trusted datasets in the asset inventories for AI training purposes.\nCFG-02: System Hardening Through Baseline Configurations - Establishing baseline configurations that include secure AI training data protocols.\nMON-01: Continuous Monitoring - Continuously monitoring AI training datasets to detect risk of adversarial attacks or data poisoning.\nDCH-01: Data Protection - Ensuring the protection of AI training data to maintain its security and trustworthiness.\nDCH-02: Data & Asset Classification - Classifying AI training data assets based on their level of security and trustworthiness.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI training projects with a focus on secure and trusted data sources.\nRSK-01: Risk Management Program - Incorporating the management of risk related to AI training data sources into the overall risk management program.\nRSK-04: Risk Assessment - Assessing the risk associated with AI training data sources and implementing controls to mitigate these risk.\nTDA-01: Technology Development & Acquisition - Ensuring secure and trusted data sources are used during the development and acquisition of AI systems.",
        "explainability": "Secure AI Training Data Protocols is a control that emphasizes the establishment of secure protocols for handling and managing AI training data. The rationale for this control is to ensure that AI training data is protected from unauthorized access, breaches, and tampering. Secure training data protocols are essential for maintaining data privacy, security, and the overall integrity of AI models. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of Secure AI Training Data Protocols and their role in safeguarding data privacy, security, and AI model integrity. \n\nProtocol Guidelines: Guidelines outlining the processes and best practices for secure handling of AI training data. \n\nProtocol Effectiveness Reports: Reports demonstrating the effectiveness of secure training data protocols in protecting data and ensuring model integrity. | Responsibility Explanation: Secure Training Data Policy: A policy that outlines the security requirements and protocols for handling AI training data. \n\nData Security Implementation Logs: Logs that document the security measures implemented to protect training data. \n\nTraining Data Security Audits: Periodic audits that assess the security of the training data against potential adversarial threats and data poisoning. | Data Explanation: The Secure Training Data Protocol defines the security measures, data sourcing criteria, and handling procedures to ensure the integrity and trustworthiness of training data. | Fairness Explanation: Secure Training Data Protocol: A protocol that mandates the use of secure training data to prevent biases and ensure fairness. \n\nAdversarial Attack Prevention and Fairness Report: A report on how secure data protocols contribute to preventing biases from adversarial attacks and maintaining fairness. \n\nData Security and Fairness Compliance Certificates: Certificates that attest to the compliance of data security protocols with fairness standards. | Safety & Performance  Explanation: Secure Dataset Acquisition Plan outlining the criteria and protocols for selecting secure training data. \n\nData Security Risk Assessment Report evaluating potential threats and vulnerabilities in the data. \n\nTrusted Data Source Certification confirming the security and reliability of the data sources used. | Impact Explanation: Data Security Protocol Documentation: Detailed guidelines and standards for securing AI training data, outlining procedures for data acquisition, storage, and handling. \n\nRisk Management Plans: Documents outlining strategies and actions for mitigating risk associated with adversarial attacks and data poisoning. \n\nIncident Response Records: Logs detailing any security breaches or incidents, along with responses and remedial actions taken."
    },
    {
        "control_name": "Feedback Data Authenticity Checks",
        "category": "AI Model Training",
        "description": "Implement cryptographic methods to verify the authenticity of feedback data, ensuring its integrity and relevance.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nSC-13: Cryptographic Protection\nSI-3: Malicious Code Protection\nSI-4: System Monitoring, SCF: CFG-02: System Hardening Through Baseline Configurations - Including cryptographic verification methods in baseline configurations for AI training systems.\nMON-01: Continuous Monitoring - Continuously monitoring the feedback data for signs of tampering or authenticity issues.\nCRY-01: Use of Cryptographic Controls - Implementing cryptographic controls to ensure the authenticity and integrity of feedback data.\nDCH-01: Data Protection - Protecting the integrity and authenticity of feedback data in AI systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI training projects with a focus on feedback data authenticity and cryptographic methods.\nRSK-01: Risk Management Program - Incorporating the management of feedback data authenticity risk into the overall risk management program for AI systems.\nRSK-04: Risk Assessment - Assessing risk related to the authenticity and integrity of feedback data in AI model training.\nTDA-01: Technology Development & Acquisition - Ensuring feedback data authenticity checks are incorporated during the development and acquisition of AI systems.",
        "explainability": "Feedback Data Authenticity Checks is a control that emphasizes the importance of verifying the authenticity and reliability of feedback data used in AI systems. The rationale for this control is to ensure that feedback data is trustworthy and free from manipulation or bias. Authenticity checks on feedback data are essential for maintaining the accuracy, fairness, and overall quality of AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of Feedback Data Authenticity Checks and their role in maintaining data accuracy, fairness, and quality in AI systems. \n\nAuthenticity Check Guidelines: Guidelines outlining the processes and best practices for verifying feedback data authenticity. \n\nAuthenticity Assessment Reports: Reports demonstrating the effectiveness of authenticity checks in ensuring trustworthy feedback data. | Responsibility Explanation: Authenticity Verification Protocol: A protocol outlining the cryptographic methods used to verify the authenticity of feedback data. \n\nFeedback Authenticity Logs: Logs that document each instance of feedback data authenticity verification. \n\nAuthenticity Check Compliance Reports: Reports that detail the compliance of feedback data with authenticity verification protocols. | Data Explanation: Feedback Data Authenticity Protocol, which outlines the cryptographic techniques used to validate the source and integrity of feedback data. | Fairness Explanation: Authenticity Verification Protocol: A protocol that outlines methods for verifying the authenticity of feedback data, with a focus on fairness.\n \nFeedback Integrity and Fairness Assurance Plan: A plan that details how the authenticity of feedback data contributes to fair AI system operations. \n\nData Authenticity and Fairness Audit Reports: Audit reports that assess the authenticity of feedback data and its implications for fairness. | Safety & Performance  Explanation: Data Authenticity Protocol Document outlining the cryptographic methods used for data verification. \n\nData Integrity Report summarizing the results of authenticity checks. \n\nCryptographic Audit Trail detailing the verification process for accountability and traceability. | Impact Explanation: Cryptographic Implementation Reports: Documentation detailing the cryptographic methods used for verifying feedback data, including technical specifications and protocols. \n\nIntegrity Assurance Records: Logs of authenticity checks and any detected anomalies or issues in feedback data, demonstrating the ongoing monitoring process.\n\nProtocol Update Histories: Records of updates or changes to cryptographic methods, reflecting adaptability to new threats or advancements."
    },
    {
        "control_name": "AI Data Cleaning & Validation Mechanisms",
        "category": "AI Model Training",
        "description": "Deploy advanced data validation and cleaning techniques to ensure the integrity and accuracy of AI training data.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: AST-02: Asset Inventories - Cataloging AI training data as an asset and managing its quality through cleaning and validation processes.\nCFG-02: System Hardening Through Baseline Configurations - Implementing data cleaning and validation as part of system hardening for AI training environments.\nMON-01: Continuous Monitoring - Continuously monitoring the AI training data for integrity and accuracy, ensuring effective data cleaning and validation.\nDCH-01: Data Protection - Ensuring the integrity and accuracy of AI training data through appropriate data protection measures.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on advanced data cleaning and validation techniques.\nRSK-01: Risk Management Program - Integrating data cleaning and validation strategies into the overall risk management program for AI systems.\nRSK-04: Risk Assessment - Assessing the risk associated with the quality and integrity of AI training data and the effectiveness of validation mechanisms.\nTDA-01: Technology Development & Acquisition - Incorporating data cleaning and validation techniques in the development and acquisition of AI systems.",
        "explainability": "AI Data Cleaning & Validation Mechanisms is a control that emphasizes the need for implementing mechanisms for cleaning and validating AI data. The rationale for this control is to ensure that data used for AI systems is free from errors, inconsistencies, and inaccuracies. Data cleaning and validation mechanisms are essential for maintaining data quality, improving AI system performance, and reducing the risk of erroneous outputs. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of AI Data Cleaning & Validation Mechanisms and their role in maintaining data quality and improving AI system performance. \n\nCleaning & Validation Guidelines: Guidelines outlining the processes and best practices for data cleaning and validation. \n\nMechanism Effectiveness Reports: Reports demonstrating the effectiveness of data cleaning and validation mechanisms in ensuring high-quality data. | Responsibility Explanation: Data Cleaning and Validation Framework: A comprehensive framework outlining the techniques and processes for data cleaning and validation. \n\n\nData Integrity Logs: Logs that track the cleaning and validation steps taken on AI training data. \n\nValidation Technique Efficacy Reports: Reports evaluating the effectiveness of the data validation and cleaning techniques used. | Data Explanation: Data Cleaning and Validation Framework detailing the methodologies, tools, and processes used for ensuring data quality prior to its use in training AI systems. | Fairness Explanation: Data Cleaning and Validation Fairness Guidelines: Guidelines that ensure data cleaning and validation processes support the fairness of AI systems. \n\nData Integrity and Fairness Compliance Report: A report that links data cleaning and validation to compliance with fairness standards. \n\nValidation Mechanism Fairness Certification: Certification confirming that data cleaning and validation mechanisms have been assessed for fairness. | Safety & Performance  Explanation: Deliverables would include a Data Cleaning and Validation Protocol outlining the methods and tools used to ensure data quality. \n\nData Integrity Report documenting the results of the data cleaning and validation processes. \n\nAdditionally, a Data Quality Assurance Log would track ongoing data validation and cleaning activities. | Impact Explanation: Data Cleaning and Validation Protocols: Detailed guidelines and procedures outlining the techniques and tools used for data cleaning and validation. \n\nQuality Assurance Reports: Documentation of data quality assessments and cleaning processes, including metrics and outcomes. \n\nChange Management Logs: Records of updates or modifications made to the data cleaning and validation mechanisms, reflecting ongoing improvements."
    },
    {
        "control_name": "Training Data Description",
        "category": "AI Model Training",
        "description": "Provide a concise and detailed description of the training data used.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-6: Timely Maintenance\nMP-6: Media Sanitization\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity\nSI-8: Spam Protection\nSC-12: Cryptographic Key Establishment and Management\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: AST-02: Asset Inventories - Cataloging AI training data in asset inventories with detailed descriptions.\nCFG-02: System Hardening Through Baseline Configurations - Ensuring baseline configurations for AI systems include requirements for training data descriptions.\nDCH-01: Data Protection - Ensuring data protection practices include maintaining comprehensive descriptions of training data.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI system portfolios with an emphasis on detailed training data descriptions.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating the requirement for detailed training data descriptions in AI project management practices.\nRSK-01: Risk Management Program - Incorporating the management of risk related to training data descriptions into the overall risk management strategy.\nRSK-04: Risk Assessment - Assessing the risk associated with the adequacy of training data descriptions in AI model development.\nTDA-04: Documentation Requirements - Including detailed descriptions of AI training data in technical documentation requirements.",
        "explainability": "Training Data Description is a control that emphasizes the need to provide comprehensive descriptions of the training data used in AI systems. The rationale for this control is to ensure that stakeholders, including nontechnical individuals, have access to clear and detailed information about the nature, sources, and characteristics of the training data. Training data descriptions enhance transparency, understanding, and accountability in AI systems. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of Training Data Description and its role in enhancing transparency, understanding, and accountability in AI systems. \n\nDescription Guidelines: Guidelines outlining the processes and best practices for creating detailed training data descriptions. \n\nDescription Effectiveness Reports: Reports demonstrating the effectiveness of training data descriptions in providing comprehensive information. | Responsibility Explanation: Training Data Documentation: Documentation that provides in-depth descriptions of the training data, its sources, and preparation methods. \n\nData Description Records: Records that detail the characteristics of the training data used. \n\nTraining Data Provenance and Preparation Reports: Reports that describe the provenance of the training data and the steps taken to prepare it for use in model training. | Data Explanation: Training Data Documentation to build a datasheet of each AI Dataset, which must include details on the data sources, collection methods, data types, and any preprocessing steps applied.\n | Fairness Explanation: Training Data Description Document: A comprehensive document that describes the training data in detail, focusing on aspects relevant to fairness. \n\nData Composition Fairness Summary: A summary that outlines how the composition of the training data supports or could impact fairness.\n \nData Description Fairness Assessment Report: A report that assesses the fairness implications of the training data based on its description. | Safety & Performance  Explanation: Training Data Specification Document that includes details on data sources, data types, collection methods, and preprocessing steps. \n\nDataset Summary Report summarizing statistics, distributions, and potential biases.\n\nData Feature Explanation that describes each feature's role and importance in the model. | Impact Explanation: Training Data Documentation: Comprehensive documents describing the training data in detail, including its sources, types, preprocessing methods, and any modifications. \n\nData Source Overviews: Summaries providing insight into the origins and nature of the data sources used. \n\nProcessing and Transformation Logs: Detailed records of how the training data has been processed or transformed throughout its life cycle."
    },
    {
        "control_name": "Training Dataset Quality",
        "category": "AI Model Training",
        "description": "Ensure training datasets are representative, thoroughly vetted, and complete.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: AST-02: Asset Inventories - Including training datasets in asset inventories with detailed quality and completeness criteria.\nCFG-02: System Hardening Through Baseline Configurations - Including measures to ensure training dataset quality in system hardening processes for AI training.\nMON-01: Continuous Monitoring - Continuously monitoring the AI training datasets to maintain their quality and ensure their representativeness.\nDCH-01: Data Protection - Ensuring the protection and quality of AI training data through robust data management practices.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with a focus on maintaining high-quality training datasets.\nRSK-01: Risk Management Program - Incorporating the management of training dataset quality into the overall risk management strategy for AI systems.\nRSK-04: Risk Assessment - Assessing risk associated with the quality, representativeness, and completeness of AI training datasets.\nTDA-01: Technology Development & Acquisition - Emphasizing the importance of quality, representativeness, and completeness of training datasets in the development and acquisition of AI technologies.",
        "explainability": "Training Dataset Quality is a control that emphasizes the importance of ensuring the quality and reliability of training datasets used in AI systems. The rationale for this control is to guarantee that the data used for training is accurate, representative, and free from errors or biases. High-quality training datasets are essential for achieving reliable AI model performance. This explanation should be delivered in an accessible and nontechnical way for various stakeholders.",
        "evidence": "Rationale Explanation: Rationale Explanation Document: A document explaining the importance of Training Dataset Quality and its role in achieving reliable AI model performance. \n\nQuality Guidelines: Guidelines outlining the processes and best practices for ensuring training dataset quality. \n\nQuality Assurance Reports: Reports demonstrating the effectiveness of measures in place to maintain dataset quality. | Responsibility Explanation: Dataset Quality Assurance Policy: A policy that sets standards for the quality of training datasets. \n\nDataset Vetting Logs: Logs that document the vetting process of the training datasets. \n\nDataset Completeness Certificates: Certificates that attest to the representativeness and completeness of the training datasets. | Data Explanation: Dataset Quality Assurance Protocol outlining the standards and checks implemented to ascertain the representativeness, thorough vetting, and completeness of training datasets. | Fairness Explanation: Training Dataset Quality Standards: Standards that define the quality criteria for training datasets with an emphasis on fairness. \n\nDataset Vetting and Fairness Compliance Protocols: Protocols that outline the vetting process for datasets to ensure they are complete and representative in a fair manner. \n\nTraining Data Quality and Fairness Certificates: Certificates that validate the quality and representativeness of training datasets in terms of fairness. | Safety & Performance  Explanation: Representativeness Report detailing the demographic and data diversity checks performed. \n\nData Vetting Records documenting the processes of data curation and completeness checks. \n\nDataset Completeness Certificate issued upon verification of the dataset’s comprehensiveness. | Impact Explanation: Dataset Quality Standards: Detailed standards and criteria defining what constitutes a high-quality, representative dataset. \n\nDataset Evaluation Reports: Comprehensive assessments of datasets, detailing their representativeness, completeness, and vetting processes. \n\nVetting Procedure Documentation: Guidelines and records of the procedures used to vet datasets for quality and completeness."
    },
    {
        "control_name": "Model Fitting Considerations",
        "category": "AI Model Training",
        "description": "Prevent underfitting and overfitting in AI models.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: AST-02: Asset Inventories - Cataloging AI models in asset inventories with emphasis on their fitting characteristics.\nCFG-02: System Hardening Through Baseline Configurations - Incorporating model fitting considerations into baseline configurations for AI systems.\nMON-01: Continuous Monitoring - Continuously monitoring AI models for signs of underfitting and overfitting.\nDCH-01: Data Protection - Protecting AI training data integrity to support accurate model fitting and prevent underfitting or overfitting.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with a focus on addressing model fitting considerations.\nRSK-01: Risk Management Program - Including model fitting considerations in the overall risk management program for AI systems.\nRSK-04: Risk Assessment - Assessing risk related to model fitting, including potential underfitting and overfitting in AI models.\nTDA-01: Technology Development & Acquisition - Ensuring model fitting considerations, including prevention of underfitting and overfitting, are included in AI system development and acquisition.",
        "explainability": "Model Fitting Considerations involve a description of the considerations and processes related to fitting the AI model. This includes details about the model selection, parameter tuning, and the rationale behind these choices.",
        "evidence": "Rationale Explanation: Model Selection Report: A document that outlines the specific model chosen for the AI system, along with the reasons for selecting it. \n\nParameter Tuning Strategy: A document that describes the approach used for tuning model parameters and hyperparameters. \n\nRationale Document: A comprehensive report that provides a nontechnical explanation of the model fitting process, its goals, and how it aligns with the overall AI system's objectives. | Responsibility Explanation: Model Fitting Strategy Document: A document outlining the strategies and techniques to address model fitting issues. \n\nFitting Issue Logs: Logs that record any instances of underfitting or overfitting, along with the measures taken to address them. \n\nModel Fitting Evaluation Reports: Reports that assess the model fitting and the effectiveness of the strategies employed to prevent fitting issues. | Data Explanation: Model Fitting Strategy Document, which should detail the techniques and practices used to prevent underfitting and overfitting in AI models, including model selection, training procedures, and validation approaches. | Fairness Explanation: Model Fitting Fairness Guidelines: Guidelines that instruct on balancing model complexity with fairness in decision making.\n\nFitting Consideration Fairness Analysis: An analysis of how fitting considerations like underfitting and overfitting prevention impact the fairness of AI models. \n\nModel Training Fairness Adjustment Records: Records of adjustments made during model training to prevent underfitting or overfitting with a fairness perspective. | Safety & Performance  Explanation: Model Complexity Analysis Report that assesses the balance between bias and variance. \n\nModel Training Process Documentation that records the steps taken to avoid underfitting and overfitting. \n\nModel Evaluation Records that include crossvalidation scores and learning curves analysis. | Impact Explanation: Model Fitting Strategy Documents: Detailed descriptions of the strategies and techniques used to prevent underfitting and overfitting in AI models, including algorithmic choices and parameter tuning methods. \n\nPerformance Evaluation Reports: Records of model performance tests under various conditions, highlighting how well the model fits the training and validation data. \n\nAlgorithm Adjustment Logs: Documentation of any adjustments or tuning done to the models to address fitting issues, including rationale and outcomes."
    },
    {
        "control_name": "Model Training on Randomized Data",
        "category": "AI Model Training",
        "description": "Train models on randomized data to deter attackers from identifying training examples.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: AST-02: Asset Inventories - Including AI models trained on randomized data in asset inventories, ensuring they are managed effectively.\nCFG-02: System Hardening Through Baseline Configurations - Implementing baseline configurations that include data randomization techniques for AI model training.\nMON-01: Continuous Monitoring - Continuously monitoring the effectiveness of randomized data training in AI models.\nDCH-01: Data Protection - Ensuring the protection of AI training data by incorporating randomization techniques.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI model training projects with a focus on the use of randomized data.\nRSK-01: Risk Management Program - Incorporating the strategy of training AI models on randomized data into the overall risk management program.\nRSK-04: Risk Assessment - Assessing the risk associated with the potential identification of training data by attackers and implementing randomization as a mitigation strategy.\nTDA-01: Technology Development & Acquisition - Ensuring the use of randomized data in the development and acquisition of AI models.",
        "explainability": "Model Training on Randomized Data describes the process of training an AI model on randomized or synthetic data, explaining the purpose and methods used in this data generation.",
        "evidence": "Rationale Explanation: Randomized Data Generation Process: A document detailing the procedures and algorithms used to generate randomized or synthetic data. \n\nPurpose and Benefits Document: An explanation of why randomized data was used, its benefits for model training, and how it aids in addressing privacy concerns. \n\nData Source Information: A report on the source of the randomized data, whether it's synthetic, anonymized, or otherwise.\n \nData Validation Plan: A plan outlining how the randomized data is validated for its appropriateness and relevance to the AI system's objectives. \n\nRationale Document: A comprehensive nontechnical report that justifies the use of randomized data for model training. | Responsibility Explanation: Data Randomization Implementation Plan: A plan that details how and when data randomization will be applied during the training process. \n\nRandomization Process Logs: Logs that record the randomization steps taken for each training dataset. \n\nTraining Effectiveness Reports: Reports that analyze the impact of data randomization on model performance and security against inference attacks. | Data Explanation: Data Randomization Techniques Document, which outlines the methods used to randomize training data, the rationale behind these methods, and their expected impact on model security. | Fairness Explanation: Data Randomization Protocol: A protocol outlining how data randomization contributes to the fairness and security of AI model training. \n\nRandomization Technique Fairness Impact Report: A report detailing the impact of data randomization techniques on the fairness of AI training. \n\nTraining Process Randomization and Fairness Logs: Logs that document the use of randomization in training data and its implications for fairness. | Safety & Performance  Explanation: Data Randomization Protocol outlining the methods used to randomize the dataset before training. \n\nModel Training Records capturing the sequence of randomized data input and system configurations. \n\nPrivacy Protection Report detailing how data randomization contributes to safeguarding the training data. | Impact Explanation: Data Randomization Protocols: Detailed guidelines on how data is randomized before being used for training, including techniques and rationales. \n\nModel Training Reports: Documentation of the training process, focusing on how randomization was implemented and its effects on model performance. \n\nSecurity Assessment Summaries: Summaries of security assessments highlighting the effectiveness of data randomization in protecting against specific types of attacks."
    },
    {
        "control_name": "Regularization Techniques for Overfitting Prevention",
        "category": "AI Model Training",
        "description": "Add regularization techniques to prevent overfitting and model poisoning attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behavior that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behavior, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Complying with relevant AI regulations by implementing advanced regularization techniques to prevent overfitting and model poisoning.\nCFG-02: System Hardening Through Baseline Configurations - Integrating regularization techniques into baseline configurations for AI systems to prevent overfitting.\nDCH-01: Data Protection - Ensuring data protection in AI systems through the use of regularization techniques to prevent overfitting.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the portfolio of AI systems with a focus on advanced regularization techniques to prevent overfitting.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on implementing regularization techniques to prevent overfitting.\nRSK-01: Risk Management Program - Integrating risk management strategies that include the use of regularization techniques to prevent overfitting and model poisoning.\nRSK-04: Risk Assessment - Assessing the risk of overfitting and model poisoning in AI models and evaluating the effectiveness of regularization techniques in mitigating these risk.\nTDA-01: Technology Development & Acquisition - Including regularization techniques in the criteria for AI system development and acquisition to prevent overfitting and model poisoning.",
        "explainability": "Regularization Techniques for Overfitting Prevention explains the methods and strategies used to prevent overfitting in the AI model. It describes the rationale behind applying regularization techniques and their impact on model performance.",
        "evidence": "Rationale Explanation: Regularization Techniques Document: A document outlining the specific regularization techniques used, such as L1, L2, dropout, or others and the reasons for their application. \n\nOverfitting Risk Assessment: A report that assesses the potential  risk of overfitting in the model and explains how the chosen regularization techniques mitigate this risk.\n\nModel Performance Metrics: Detailed information on how the model's performance is monitored and improved through the application of regularization techniques. \nRationale Document: A comprehensive nontechnical report that justifies the use of regularization techniques and their contribution to the AI system's objectives. | Responsibility Explanation: Regularization Technique Guidelines: A comprehensive set of guidelines on how to apply regularization techniques correctly in the model training process. \n\nModel Training Records: Documentation of the model training process, including details on the regularization techniques used.\n \nOverfitting Prevention Audit Reports: Reports from audits that evaluate the effectiveness of the regularization techniques in preventing overfitting. | Data Explanation: Regularization Implementation Guide detailing the types of regularization techniques used, the scenarios for their application, and guidance on how to configure them to mitigate the risk of overfitting and poisoning. | Fairness Explanation: Overfitting Prevention Fairness Strategy: A strategy that includes regularization techniques to prevent overfitting while considering fairness. \n\nRegularization Impact on Model Fairness Report: A report on how regularization techniques like L1 and L2 impact the fairness of model outcomes. \n\nOverfitting Prevention Technique Fairness Certificates: Certificates that confirm the application of regularization techniques enhances the fairness of the AI system. | Safety & Performance  Explanation: Regularization Implementation Guide that outlines the regularization methods applied to the model. \n\nOverfitting Mitigation Report which documents the effect of regularization on model performance. \n\nModel Poisoning Prevention Strategy that details how regularization contributes to model security. | Impact Explanation: Regularization Implementation Documentation: Detailed descriptions of the regularization techniques used, including technical specifics, implementation strategies, and rationales for their selection. \n\nModel Evaluation Reports: Documentation of model performance before and after applying regularization, highlighting changes in overfitting tendencies and model robustness. \n\nSecurity Assessment Reports: Analysis of the model's resilience to poisoning attacks post regularization implementation."
    },
    {
        "control_name": "Model Retraining Frequency",
        "category": "AI Model Training",
        "description": "Establish a regular schedule for model retraining to ensure it stays current and avoids obsolescence.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: CFG-02: System Hardening Through Baseline Configurations - Including model retraining schedules in the baseline configurations for AI systems.\nMNT-03: Timely Maintenance - Scheduling regular maintenance (including retraining) of AI models to ensure they remain current and effective.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing the AI system portfolio with a focus on regular model retraining schedules.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on establishing and adhering to model retraining schedules.\nPRM-07: Secure Development Life Cycle (SDLC) Management - Integrating model retraining into the SDLC to maintain the currency and relevance of AI systems.\nRSK-01: Risk Management Program - Incorporating model retraining into the overall risk management program for AI systems.\nRSK-07: Risk Assessment Update - Regularly updating risk assessments to reflect changes made during AI model retraining sessions.\nTDA-01: Technology Development & Acquisition - Ensuring model retraining frequency is considered in the development and acquisition of AI technologies.",
        "explainability": "Model Retraining Frequency describes how often the AI model is retrained and provides a rationale for the chosen retraining schedule. It explains the reasons behind the frequency of model updates and their impact on the AI system's performance.",
        "evidence": "Rationale Explanation: Retraining Schedule Document: A document specifying the schedule for model retraining, including time intervals or triggers for updates. \n\nRationale for Retraining: A report explaining the reasons for the chosen retraining frequency, considering factors such as data drift, concept drift, or system requirements. \n\nModel Performance Metrics: Detailed information on how model performance is monitored and improved through periodic retraining. \n\nRationale Document: A comprehensive nontechnical report that justifies the selected retraining frequency and its alignment with the AI system's objectives. | Responsibility Explanation: Retraining Schedule Policy: A policy that outlines how frequently models should be retrained, including any triggers that would require retraining outside of the regular schedule. \n\nRetraining Logs: Logs that record each instance of model retraining, what prompted it, and the outcomes. \n\nRetraining Effectiveness Reports: Reports that evaluate how retraining has affected the model's performance and relevance. | Data Explanation: Model Retraining Schedule, which should outline the frequency of retraining sessions, criteria for triggering unscheduled retraining, and the process for implementing retrained models. | Fairness Explanation: Retraining Schedule and Fairness Protocol: A protocol that outlines a regular schedule for retraining models with a focus on maintaining fairness. \n\nRetraining Impact on Fairness Documentation: Documentation that evaluates the impact of retraining frequency on the fairness of AI system decisions. \n\nModel Currency and Fairness Review Reports: Reports that review the currency of the model and its implications for ongoing fairness. | Safety & Performance  Explanation: Expanded deliverables would encompass a Detailed Retraining Strategy Document, which outlines the criteria for when and how retraining should be initiated, including triggers based on performance metrics or environmental changes. \n\nA Comprehensive Retraining Log would be maintained, documenting each retraining cycle, the data used, and any adjustments made to the model. \n\nAn Enhanced Model Performance Analysis Report would also be prepared after each retraining cycle, offering in-depth insights into the improvements or changes in the model's performance. | Impact Explanation: Retraining Schedule and Protocols: Documentation outlining the frequency and methods of retraining, including criteria for determining retraining intervals. \n\nPerformance Tracking Reports: Records of model performance metrics over time, showing the effects of regular retraining. \n\nUpdate and Version Control Logs: Documentation detailing each retraining cycle, changes made, and version updates."
    },
    {
        "control_name": "Training-Testing Data Separation",
        "category": "AI Model Training",
        "description": "Ensure a clear separation between training and testing datasets to prevent overfitting.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-6: Monitoring Physical Access\nPE-9: Power Equipment and Cabling\nPS-3: Personnel Screening\nPS-7: External Personnel Security\nSA-9: External System Services\nSC-2: Separation of System and User Functionality\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-8: Spam Protection\nSI-12: Cryptographic Key Establishment and Management\nSA-11: Developer Testing and Evaluation, SCF: CFG-02: System Hardening Through Baseline Configurations - Enforcing baseline configurations that maintain a clear separation between training and testing datasets in AI models.\nMON-01: Continuous Monitoring - Continuously monitoring AI models for compliance with training-testing data separation standards.\nDCH-01: Data Protection - Implementing data protection measures that enforce the separation of training and testing datasets in AI model development.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI system portfolios with a focus on maintaining separation between training and testing datasets.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on maintaining clear separation between training and testing datasets.\nRSK-01: Risk Management Program - Incorporating strategies for training-testing data separation into the overall risk management program for AI systems.\nRSK-04: Risk Assessment - Assessing the risk associated with the potential mixing of training and testing datasets and the impact on model accuracy and integrity.\nTDA-01: Technology Development & Acquisition - Including requirements for training-testing data separation in the development and acquisition of AI models.",
        "explainability": "Training-Testing Data Separation describes how data is separated into training and testing sets for AI model development and validation. It provides a rationale for the chosen data separation methods and their importance in ensuring model robustness and generalization.",
        "evidence": "Rationale Explanation: Data Separation Methodology: A document explaining the methodology used to separate data into training and testing sets, including details about randomization, stratification, and data splitting ratios. \n\nRationale for Data Separation: A report that justifies the chosen data separation methods, considering factors like model complexity, dataset size, and potential sources of bias. \n\nValidation Metrics: Detailed information on how the testing set is used to evaluate model performance and validate the AI system. \n\nRationale Document: A comprehensive nontechnical report that justifies the data separation process and its contribution to the AI system's objectives. | Responsibility Explanation: Data Separation Protocol: A detailed protocol specifying how training and testing datasets are to be separated and maintained. \n\nDataset Usage Records: Records of how each dataset is used, ensuring compliance with the separation protocol. \n\nData Leakage Audit Reports: Reports from audits conducted to detect any instances of data leakage between training and testing datasets. | Data Explanation: Data Management Protocol specifying the guidelines for maintaining a strict separation between training and testing datasets, including the processes for dataset division and management. | Fairness Explanation: Data Separation Protocol: A protocol describing the methods used to separate training and testing data, focusing on fairness. \n\nOverfitting Prevention and Fairness Report: A report outlining the measures taken to prevent overfitting and their impact on fairness. \n\nTraining/Testing Data Integrity Logs: Logs documenting the integrity of data separation and its fairness implications. | Safety & Performance  Explanation: Deliverables would include a Data Management Protocol outlining procedures for maintaining separation between datasets and a Dataset Utilization Report documenting how each set of data is used throughout the model development process. \n\nA Model Generalization Performance Record would also be kept that analyzes the model's effectiveness on testing data as compared to training data. | Impact Explanation: Data Management Protocols: Comprehensive guidelines outlining procedures for segregating training and testing datasets. \n\nDataset Usage Logs: Records detailing the usage of specific datasets in training and testing phases, ensuring adherence to segregation protocols. \n\nModel Development Reports: Documentation of the AI model development process, highlighting the implementation of training-testing separation."
    },
    {
        "control_name": "Model Retraining Attacks",
        "category": "AI Model Training",
        "description": "Implement mechanisms to detect unauthorized retraining or fine-tuning of models.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: AST-02: Asset Inventories - Cataloging AI models in asset inventories with a focus on monitoring for unauthorized retraining activities.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with relevant regulations by implementing controls to detect unauthorized AI model modifications.\nCFG-02: System Hardening Through Baseline Configurations - Hardening AI systems to prevent unauthorized retraining or fine-tuning of models.\nMON-01: Continuous Monitoring - Continuously monitoring AI models to detect signs of unauthorized retraining or adjustments.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with a focus on preventing and detecting unauthorized model retraining.\nRSK-01: Risk Management Program - Incorporating the detection of unauthorized AI model retraining into the overall risk management strategy for AI systems.\nRSK-04: Risk Assessment - Assessing the risk of unauthorized retraining or fine-tuning in AI models and implementing controls to mitigate these risk.\nTDA-01: Technology Development & Acquisition - Including measures in the development and acquisition of AI models to detect and prevent unauthorized retraining or fine-tuning.",
        "explainability": "The Model Retraining Attacks control describes how the AI system defends against potential attacks during the retraining process. It provides a rationale for the security measures and safeguards in place to protect the AI model from adversarial threats.",
        "evidence": "Rationale Explanation: Retraining Security Measures: A document outlining the security measures and strategies in place to safeguard the retraining process. This may include techniques such as data validation, input sanitization, and model robustness checks. \n\nAdversarial Threat Assessment: A report assessing potential threats and attacks that could target the retraining process and explaining how these threats are mitigated. \n\nRetraining Security Testing Plan: A plan describing how the security of the retraining process is tested and validated. \n\nRationale Document: A comprehensive nontechnical report justifying the security measures and their importance in protecting the AI system. | Responsibility Explanation: Retraining Attack Detection System: A system designed to detect unauthorized retraining or fine-tuning of AI models. \n\nUnauthorized Retraining Logs: Logs that specifically record any unauthorized attempts to retrain models. \n\nModel Security Incident Reports: Reports that detail any detected retraining attacks and the response to them. | Data Explanation: Unauthorized Retraining Detection Protocol outlining the security measures and monitoring systems in place to identify unauthorized attempts to retrain or modify the model. | Fairness Explanation: Retraining Attack Detection Guidelines: Guidelines for detecting and preventing unauthorized model retraining, with an emphasis on fairness. \n\nUnauthorized Retraining Fairness Impact Analysis: Analysis of how unauthorized retraining may affect the fairness of the AI system's outcomes.\n \nModel Retraining Security Certificates: Certificates that affirm the security measures against unauthorized retraining support fairness. | Safety & Performance  Explanation: Deliverables would include a Model Retraining Security Protocol, which outlines the safeguards against unauthorized model adjustments, and an Unauthorized Access Detection Report documenting any attempts or breaches in the model retraining process. \n\nAdditionally, a Model Integrity Monitoring Log would be maintained recording all authorized and unauthorized changes to the model. | Impact Explanation: Unauthorized Retraining Detection Protocols: Detailed guidelines and strategies for detecting unauthorized model retraining or tampering. \n\nSecurity Incident Reports: Documentation of any detected unauthorized retraining attempts, including the nature of the attempt and the response action taken. \n\nSystem Integrity Logs: Continuous logs monitoring model performance and behavior for signs of unauthorized changes."
    },
    {
        "control_name": "Training Data Source Verification",
        "category": "Data Source Verification",
        "description": "Verify the sources of training data to ensure its authenticity and reliability.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-2: Controlled Maintenance\nMA-3: Maintenance Tools\nMA-6: Timely Maintenance\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: Systems Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-13: Predictable Failure Prevention\nSI-15: Information Output Filtering\nSI-16: Memory Protection\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation, SCF: AST-02: Asset Inventories - Documenting the sources of training data in AI model asset inventories to maintain a record of their authenticity and reliability.\nCFG-02: System Hardening Through Baseline Configurations - Incorporating validation checks of training data sources into the baseline configurations for AI systems.\nMON-01: Continuous Monitoring - Continuously monitoring the sources of training data for AI models to ensure ongoing authenticity and reliability.\nDCH-01: Data Protection - Implementing measures to protect the integrity and authenticity of data sources used for AI training.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI model training projects with a focus on verifying the sources of training data.\nRSK-01: Risk Management Program - Incorporating training data source verification into the overall risk management strategy for AI systems.\nRSK-04: Risk Assessment - Assessing risk related to the sources of training data for AI models and ensuring their validity and reliability.\nTDA-01: Technology Development & Acquisition - Ensuring the authenticity and reliability of training data sources in the development and acquisition of AI technologies.",
        "explainability": "The Training Data Source Verification control describes the process of verifying the sources of training data used for AI model development. It provides a rationale for the verification methods and their role in ensuring the quality and trustworthiness of the training data.",
        "evidence": "Rationale Explanation: Data Source Verification Procedure: A document outlining the procedures and methods used to verify the sources of training data, including data provenance, authenticity checks, and data quality assessments.\n \nRationale for Verification: A report that explains the reasons for verifying training data sources, such as data trustworthiness, compliance with regulations, and mitigation of potential biases. \n\nVerification Results Report: A report summarizing the outcomes of the data source verification process, including any issues identified and how they were addressed. \n\nRationale Document: A comprehensive nontechnical report justifying the verification process and its significance for the AI system. | Responsibility Explanation: Data Source Verification Plan: A plan that outlines the process for verifying the sources of training data. \n\nData Source Verification Records: Records that document the verification of each data source used for training AI systems. \n\nData Authenticity Certificates: Certificates or attestations that confirm the authenticity and reliability of the training data sources. | Data Explanation: Training Data Source Verification Protocol, detailing the methods used to verify the authenticity and reliability of data sources for training purposes. | Fairness Explanation: Data Source Verification Procedures: Procedures for verifying the authenticity and reliability of training data sources, with a focus on fairness. \n\nSource Authenticity and Fairness Reports: Reports documenting the verification of data sources and the implications for fairness. \n\nTraining Data Authenticity Logs: Logs that record the verification of training data sources and any fairness-related observations. | Safety & Performance  Explanation: Deliverables would include a Training Data Source Verification Report, which documents the processes and results of authenticity checks, and a Data Source Profile Catalogue which provides detailed information about each data source used. \n\nAn Authenticity Assurance Checklist might also be prepared that outlines the criteria and standards for data source verification. | Impact Explanation: Data Source Verification Protocols: Detailed guidelines and procedures for verifying the authenticity and reliability of training data sources. \n\nVerification Reports: Comprehensive documentation of the verification process, findings, and any actions taken in response to identified issues. \n\nData Source Profiles: Detailed profiles of each data source, including origin, collection methods, and historical reliability."
    },
    {
        "control_name": "Regular Software Updates",
        "category": "Compliance with Principles",
        "description": "Implement regular software updates to fix vulnerabilities and security patches, reducing the risk of output integrity attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-3: Maintenance Tools\nMA-5: Maintenance Personnel\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-6: Monitoring Physical Access\nPE-9: Power Equipment and Cabling\nPS-3: Personnel Screening\nPS-7: External Personnel Security\nSA-9: External Application Partitioning\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity\nSI-8: Spam Protection\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-16: Memory Protection\nSI-17: Fail-Safe Procedures\nSI-19: De-Identification\nSI-20: Tainting\nSA-11: Developer Testing and Evaluation\nAT-2: Literacy Training and Awareness, SCF: CFG-02: System Hardening Through Baseline Configurations - Including regular software updates in the baseline configurations to ensure system integrity and security.\nMON-01: Continuous Monitoring - Continuously monitoring AI systems for compliance with regular software update schedules.\nMNT-01: Maintenance Operations - Regularly updating software as part of maintenance operations to address vulnerabilities and enhance security.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on implementing regular software updates for security and compliance.\nRSK-01: Risk Management Program - Incorporating the management of software updates into the overall risk management strategy for AI systems.\nRSK-04: Risk Assessment - Assessing risk related to software vulnerabilities and determining the necessity and frequency of software updates.\nTDA-01: Technology Development & Acquisition - Ensuring the inclusion of regular software update mechanisms in the development and acquisition of AI technologies.\nVPM-05: Software & Firmware Patching - Implementing a robust patch management process to update software and fix vulnerabilities regularly.",
        "explainability": "The Regular Software Updates control describes the process of keeping the AI system and its underlying software components up to date through regular updates. It provides a rationale for the importance of these updates in maintaining system security, performance, and compliance.",
        "evidence": "Rationale Explanation: Update Schedule and Process: A document specifying the schedule and procedures for updating the AI system and its software components, including the version control and release notes. \n\nRationale for Updates: A report explaining the reasons for regular software updates, such as security patching, bug fixes, feature enhancements, and compliance with evolving standards. \n\nUpdate History Log: A record of past updates, including the issues addressed and improvements made through each update. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of regular software updates for the AI system's security, performance, and compliance. | Responsibility Explanation: Software Update Schedule: A detailed timeline and procedure for regular software updates. \n\nUpdate Implementation Logs: Logs documenting each software update applied, including details on the nature of the update and who performed it. \n\nVulnerability Mitigation Reports: After-action reports assessing the effectiveness of updates in addressing specific vulnerabilities. | Data Explanation: Software Update Policy, which establishes the schedule, procedures, and responsibilities for applying updates and patches to the software. | Fairness Explanation: Software Update and Fairness Maintenance Schedule: A schedule that outlines regular software updates with considerations for maintaining fairness. \n\nUpdate Fairness Compliance Certificates: Certificates that confirm software updates have been implemented in a manner that maintains fairness. \n\nSoftware Vulnerability and Fairness Response Records: Records detailing responses to software vulnerabilities and their implications for fairness. | Safety & Performance  Explanation: Deliverables would include a Software Update Schedule outlining the frequency and scope of updates and an Update Implementation Log documenting each update, the vulnerabilities addressed, and any changes made. \n\nA Security Patch Efficacy Report could also be provided that evaluates the impact of the patches on system security. | Impact Explanation: Update Schedule and Protocols: A well-defined schedule and set of procedures for regular software updates, including guidelines for emergency patches. \n\nUpdate Logs: Detailed records of each update, including what was updated, the reason for the update, and any impact assessments. \n\nVulnerability and Patch Reports: Documentation of identified vulnerabilities and the corresponding patches applied."
    },
    {
        "control_name": "Continuous Monitoring",
        "category": "Effective Monitoring Mechanisms",
        "description": "Implement effective monitoring mechanisms to identify high risk to the rights and freedoms of data subjects.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-12: Information Location\nCM-13: Data Action Mapping\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-6: Monitoring Physical Access\nPE-9: Power Equipment and Cabling\nPS-3: Personnel Screening\nPS-7: External Personnel Security\nSA-9: External System Services\nSC-2: Separation of System and User Functionality\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-8: Spam Protection\nSC-12: Cryptographic Key Establishment and Management\nSA-11: Developer Testing and Evaluation, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with laws and regulations related to data subjects' rights through effective monitoring.\nCFG-02: System Hardening Through Baseline Configurations - Hardening AI systems with configurations that support continuous monitoring for data subject rights protection.\nMON-01: Continuous Monitoring - Continuously monitoring AI systems to ensure the protection of data subjects' rights and identification of high risk.\nDCH-01: Data Protection - Protecting data within AI systems through continuous monitoring, ensuring compliance with data subject rights.\nPRI-01: Data Privacy Program - Implementing a data privacy program that includes monitoring mechanisms to safeguard data subjects' rights in AI systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on continuous monitoring to protect data subjects' rights.\nRSK-01: Risk Management Program - Incorporating continuous monitoring into the AI systems' risk management program, focusing on data subject rights protection.\nRSK-04: Risk Assessment - Assessing risk to data subjects' rights and freedoms as part of AI system risk assessments.",
        "explainability": "The Continuous Monitoring control describes the ongoing monitoring process for the AI system's behavior and performance. It provides a rationale for the importance of continuous monitoring in ensuring the AI system's reliability, security, and adherence to ethical and legal standards.",
        "evidence": "Rationale Explanation: Monitoring Plan: A document detailing the plan for continuous monitoring, including the specific metrics, indicators, and tools used. \n\nRationale for Monitoring: A report explaining the reasons for continuous monitoring, such as identifying and addressing issues, ensuring security, and maintaining ethical and legal compliance. \n\nMonitoring Reports: Regular reports summarizing the findings and insights from continuous monitoring efforts, including any anomalies or deviations from expected behavior. \n\nRationale Document: A comprehensive nontechnical report justifying the significance of continuous monitoring for maintaining the AI system's reliability, security, and ethical and legal compliance. | Responsibility Explanation: Monitoring Strategy Document: A document outlining the strategy and techniques for continuous monitoring of high-risk AI systems.\n \nRisk Detection Logs: Logs that record all identified risk and the corresponding actions taken. \n\nMonitoring Effectiveness Reports: Regular reports evaluating the effectiveness of the monitoring mechanisms and any risk identified. | Data Explanation: Monitoring Mechanism Framework describing the tools, processes, and criteria used for continuous monitoring of AI systems to protect data subjects' rights and freedoms. | Fairness Explanation: Continuous Monitoring Framework for Fairness: A framework for the continuous monitoring of AI systems that emphasizes the detection and management of fairness risk.\n\nRights and Freedoms Monitoring Reports: Reports that monitor risk to the rights and freedoms of data subjects, with a focus on fairness. \n\nMonitoring System Fairness Audit Trails: Audit trails that demonstrate the fairness of the continuous monitoring systems in place. | Safety & Performance  Explanation: Deliverables would include a Continuous Monitoring Strategy Document outlining the approach and tools used for ongoing risk detection and a Data Subject Rights Protection Report summarizing the findings from the monitoring activities and actions taken to mitigate risk. \n\nAdditionally, an Incident Response Log would record any incidents affecting data subject rights and the response to these incidents. | Impact Explanation: Monitoring Strategy Documents: Detailed outlines of the monitoring mechanisms, including objectives, tools, and processes. \n\nRisk Identification Logs: Continuous logs recording identified risk to the rights and freedoms of data subjects, including the nature and severity of each risk. \n\nCompliance Reports: Regular reports detailing compliance with data protection laws and the effectiveness of monitoring mechanisms."
    },
    {
        "control_name": "Consolidated AI Technical Documentation",
        "category": "AI Model Management",
        "description": "Develop and maintain comprehensive technical documentation for high-risk AI systems. This documentation should cover the system's architecture, data flow, and any associated legal or regulatory considerations, ensuring a holistic view of the system.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 11: Technical Documentation, NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-9: Power Equipment and Cabling\nPE-20: Asset Monitoring and Tracking\nSA-9: External System Services\nSC-2: Separation of System and User Functionality\nSC-7: Boundary Protection\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-10: Information Input Validation\nSI-11: Error Handling\nSI-12: Information Management and Retention\nSI-15: Information Output Filtering\nSC-12: Cryptographic Key Establishment and Management, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with laws and regulations related to data subjects' rights through effective monitoring.\nCFG-02: System Hardening Through Baseline Configurations - Hardening AI systems with configurations that support continuous monitoring for data subject rights protection.\nMON-01: Continuous Monitoring - Continuously monitoring AI systems to ensure the protection of data subjects' rights and identification of high risk.\nDCH-01: Data Protection - Protecting data within AI systems through continuous monitoring, ensuring compliance with data subject rights.\nPRI-01: Data Privacy Program - Implementing a data privacy program that includes monitoring mechanisms to safeguard data subjects' rights in AI systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing AI projects with an emphasis on continuous monitoring to protect data subjects' rights.\nRSK-01: Risk Management Program - Incorporating continuous monitoring into the AI systems' risk management program, focusing on data subject rights protection.\nRSK-04: Risk Assessment - Assessing risk to data subjects' rights and freedoms as part of AI system risk assessments.",
        "explainability": "The Consolidated AI Documentation control describes the process of maintaining comprehensive and organized documentation for the AI system, including its development, training, deployment, and operation. It provides a rationale for the importance of consolidated documentation in ensuring transparency, accountability, and effective collaboration.",
        "evidence": "Rationale Explanation: Documentation Structure and Guidelines: A document outlining the structure and guidelines for creating and organizing AI system documentation, including sections for development, training, deployment, and operation. \n\nRationale for Consolidated Documentation: A report explaining the reasons for maintaining consolidated documentation, such as transparency, accountability, and facilitating collaboration among different teams. \n\nConsolidated Documentation Repository: A centralized repository or system for storing and managing the AI system's documentation, ensuring easy access and version control. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of consolidated documentation for transparency, accountability, and effective collaboration. | Responsibility Explanation: AI Documentation Framework: A framework defining the scope and structure of the comprehensive technical documentation. \n\nDocumentation Consolidation Logs: Logs that track the consolidation and updating of technical documentation. \n\nSystem Overview Reports: Reports providing a complete overview of the AI system’s architecture, data flows, and compliance with legal and regulatory requirements. | Data Explanation: Consolidated Technical Documentation Guide encompassing all necessary aspects of the AI system's design, operation, compliance, and impact assessment. | Fairness Explanation: AI System Documentation Fairness Guidelines: Guidelines that ensure all aspects of AI system documentation are comprehensive and fair. \n\nTechnical Documentation and Fairness Review Protocols: Protocols for reviewing AI system documentation to ensure it covers fairness considerations. \n\nDocumentation Consolidation and Fairness Certificates: Certificates that confirm the comprehensive documentation of AI systems includes thorough coverage of fairness aspects. | Safety & Performance  Explanation: Deliverables would include a Technical Documentation Manual, which extensively details the AI system's architecture, data handling processes, and compliance with legal and regulatory requirements. \n\nAdditionally, an AI System Architecture Diagram would visually represent the system's structure, and a Data Flow and Compliance Matrix would be created to track how data moves within the system and its adherence to legal standards. | Impact Explanation: Technical Documentation Manuals: In-depth manuals detailing the AI system's architecture, data flow, and compliance with legal and regulatory requirements. \n\nSystem Architecture Diagrams: Visual representations of the AI system's architecture and data flow. \n\nLegal and Regulatory Compliance Files: Documents covering all legal and regulatory considerations associated with the AI system, including data protection, ethical guidelines, and industry-specific regulations."
    },
    {
        "control_name": "AI System Transition Governance",
        "category": "AI Model Management",
        "description": "Establish robust transition governance for high-risk AI systems. This should encompass guidelines for the secure transfer of all technical documentation, ensuring continuity and integrity during system handovers or migrations.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-3: Maintenance Tools\nMA-5: Nonlocal Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-6: Monitoring Physical Access\nPE-9: Power Equipment and Cabling\nPM-23: Data Governance Body\nPS-3: Personnel Screening\nPS-7: External Personnel Security\nSA-9: External System Services\nSC-2: Separation of System and User Functionality\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSC-8: Transmission Confidentiality and Integrity\nSC-11: Trusted Path\nSC-16: Transmission of Security and Privacy Attributes\nSC-46: Cross Domain Policy Enforcement\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-6: Security and Privacy Function Verification\nSI-7: Software, Firmware and Information Integrity\nSI-8: Spam Protection\nSC-12: Cryptographic Key Establishment and Management\nSI-16: Memory Protection\nSI-17: Memory Management\nSI-20: Security Functionality Verification\nSR-6: Developer Security Testing, SCF: AST-02: Asset Inventories - Documenting AI system transitions in asset inventories to track changes and maintain integrity.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with regulations and contractual obligations during AI system transitions.\nCFG-01: Configuration Management Program - Ensuring secure and documented transitions in AI systems' configurations during handovers or migrations.\nMNT-07: Maintain Configuration Control During Maintenance - Maintaining configuration control during AI system transitions to ensure continuity and integrity.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Ensuring the AI system portfolio includes robust transition governance practices.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Managing transitions in AI projects with a focus on secure transfer and documentation continuity.\nRSK-01: Risk Management Program - Incorporating transition risk management into the overall strategy for AI systems.\nTDA-01: Technology Development & Acquisition - Ensuring the secure transition of AI systems is included in technology development and acquisition processes.",
        "explainability": "The AI System Transition Governance control describes the governance and oversight measures in place when transitioning the AI system from development to deployment and throughout its life cycle. It provides a rationale for the importance of transition governance in ensuring a smooth and accountable transition process.",
        "evidence": "Rationale Explanation: Transition Governance Plan: A document outlining the plan and strategies for governing the transition of the AI system, including roles and responsibilities, decision-making processes, and checkpoints. \n\nRationale for Transition Governance: A report explaining the reasons for having governance measures during the transition, such as risk management, quality assurance, and accountability. \n\nTransition Governance Records: Records of the decisions, actions, and outcomes of the transition governance, documenting how governance was applied in practice. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of transition governance for ensuring a smooth and accountable AI system transition. | Responsibility Explanation: Transition Governance Policy: A policy outlining the responsibilities and procedures for the secure transition of AI systems. \n\nTransition Records: Records documenting the transfer of technical documentation and other critical information during system handovers. \n\nTransition Security Audits: Audits to assess the security and integrity of the transition process. | Data Explanation: Transition Governance Framework outlining the processes for secure documentation transfer during AI system transitions, including handovers and migrations. | Fairness Explanation: Transition Governance Policy with Fairness Considerations: A policy that ensures the governance of AI system transitions includes fairness considerations. \n\nTransition Fairness Assurance Plan: A plan that outlines how transitions will uphold the fairness of AI system operations. \n\nGovernance and Fairness Compliance Records: Records demonstrating compliance with transition governance policies and fairness. | Safety & Performance  Explanation: Deliverables would include a Transition Governance Framework outlining the procedures and responsibilities for transitioning AI systems and a Secure Transfer Protocol Document specifying the methods used to safely transfer technical documentation. \n\nA Transition Log would also be maintained recording all handovers or migrations and detailing the transfer of information and system components. | Impact Explanation: Transition Governance Policies: Detailed policies and guidelines outlining the procedures for secure system transitions, including roles and responsibilities. \n\nDocumentation Transfer Protocols: Specific protocols for the secure and efficient transfer of technical documentation during system handovers. \n\nTransition Logs: Records of all transitions, including details on the transfer process, stakeholders involved, and any issues encountered."
    },
    {
        "control_name": "AI Change Log Maintenance",
        "category": "AI Model Management",
        "description": "Maintain comprehensive records of all changes made to high-risk AI systems. This should detail the nature of each change (whether predetermined or unplanned) and any assessments conducted in relation to the change.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-8: System Use Notification\nAU-2: Event Logging\nAU-3: Content of Audit Records\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nSA-9: External System Services\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-8: Spam Protection\nSC-12: Cryptographic Key Establishment and Management\nSI-12: Information Management and Retention\nSI-17: Fail-Safe Procedures\nSI-18: Personally Identifiable Information Quality Operations\nSI-23: Information Fragmentation\n, SCF: CHG-02: Configuration Change Control - Controlling and documenting all configuration changes to AI systems, including both predetermined and unplanned changes.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring change logs are maintained in compliance with relevant legal and regulatory requirements.\nCFG-01: Configuration Management Program - Maintaining comprehensive records of configuration changes in AI systems, including detailed documentation of each change.\nMON-01: Continuous Monitoring - Continuously monitoring AI systems to ensure that all changes are accurately recorded in change logs.\nMNT-04: Maintenance Tools - Utilizing maintenance tools to support the logging and tracking of changes in AI systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensuring project management practices include maintaining detailed logs of all changes to AI models.\nRSK-04: Risk Assessment - Conducting risk assessments for changes made to AI systems and documenting the findings in change logs.\nTDA-01: Technology Development & Acquisition - Documenting changes made during the development and acquisition phases of AI systems.",
        "explainability": "The AI Change Log Maintenance control describes the process of maintaining a change log that tracks updates, modifications, and changes made to the AI system over time. It provides a rationale for the importance of maintaining a change log to ensure transparency, accountability, and traceability of system changes.",
        "evidence": "Rationale Explanation: Change Log Structure and Guidelines: A document outlining the structure and guidelines for creating and maintaining a change log, including information on what changes should be recorded, the format, and accessibility. \n\nRationale for Change Log Maintenance: A report explaining the reasons for maintaining a change log, such as transparency, accountability, and traceability of system changes. \n\nChange Log Records: Records of all changes made to the AI system, including descriptions of changes, dates, reasons, and individuals responsible. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of change log maintenance for transparency, accountability, and traceability. | Responsibility Explanation: Change Log Policy: A policy that specifies how changes to AI systems should be documented. \n\nChange Logs: Comprehensive records of all changes made to the AI systems, including the nature of the change and associated impact assessments. \n\n Change Management Reports: Reports detailing the change management process and evaluating the impact of changes on system performance and risk profile. | Data Explanation: AI System Change Log, which records all changes made to the system, the reasons for each change, and details of any impact assessments or evaluations carried out. | Fairness Explanation: Change Log Fairness Protocol: A protocol ensuring change logs include fairness implications of each modification. \n\nChange Impact Fairness Reports: Reports that detail how changes to AI systems impact fairness. \n\nModification Assessment Fairness Summaries: Summaries of assessments conducted to determine the fairness impact of AI system changes. | Safety & Performance  Explanation: Deliverables would include an AI Change Log, which provides a chronological record of all changes made to the system, detailing the nature and purpose of each change. \n\nAdditionally, a Change Impact Assessment Report would be created for each change, analyzing its effects on the system's performance and safety. | Impact Explanation: Change Log Records: Detailed records of all changes made to the AI system, including the nature, purpose, and date of each change. \n\nImpact Assessment Reports: Documentation of assessments conducted to understand the implications of each change, covering aspects such as system performance, security, and compliance. \n\nVersion Control Documentation: Detailed records of different versions of the AI system, aligning changes with specific releases."
    },
    {
        "control_name": "AI Change Control Framework",
        "category": "AI Model Management",
        "description": "Implement a stringent AI change control framework for high-risk AI systems. This framework should involve a structured process for proposing, reviewing, approving, and documenting every modification, ensuring that changes align with industry best practices and compliance requirements.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nSA-9: External System Services\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-11: Clearing and Purging\nSC-12: Cryptographic Key Establishment and Management\nSI-21: Information Refresh, SCF: CHG-02: Configuration Change Control - Managing and controlling changes to AI systems through a structured process that includes review, approval, and documentation.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring the AI change control framework complies with relevant legal, regulatory, and contractual requirements.\nCFG-01: Configuration Management Program - Implementing a configuration management program that encompasses structured change control processes for AI systems.\nMNT-01: Maintenance Operations - Ensuring maintenance operations for AI systems adhere to the established change control framework.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating structured AI change control processes into project management practices, aligning with industry best practices.\nRSK-01: Risk Management Program - Including AI change control processes in the organization's risk management strategy to address potential risk associated with system modifications.\nRSK-04: Risk Assessment - Assessing risk associated with proposed changes to AI systems as part of the change control process.\nTDA-01: Technology Development & Acquisition - Incorporating change control mechanisms into the lifecycle of AI systems during development and acquisition.",
        "explainability": "The AI Change Control Framework describes the framework and processes in place for managing and controlling changes to the AI system, including updates, enhancements, and modifications. It provides a rationale for the importance of a change control framework in maintaining system stability, compliance, and reliability.",
        "evidence": "Rationale Explanation: Change Control Framework Guidelines: A document outlining the framework's guidelines, principles, and processes for controlling changes to the AI system, including change request procedures, approvals, and version control. \n\nRationale for Change Control Framework: A report explaining the reasons for having a change control framework, such as maintaining system stability, ensuring compliance, and enhancing reliability. \n\nChange Control Records: Records of all change requests, approvals, rejections, and their impact on the AI system, documenting how the framework is applied in practice. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of the change control framework for maintaining system stability, compliance, and reliability. | Responsibility Explanation: Change Control Framework Document: A comprehensive framework that outlines the structured process for managing changes to AI systems. \n\n Change Proposal Forms: Standardized forms for submitting proposals for changes, which capture the rationale and expected impact. \n\n Change Approval Records: Documentation of the review and approval process for each change, including details of the decision-making process and personnel involved. | Data Explanation: Change Control Framework Documentation defines the standardized procedures for change management within AI systems, including proposal submission, review protocols, approval mechanisms, and documentation requirements. | Fairness Explanation: Change Control and Fairness Management Framework: A framework that integrates fairness considerations into the AI change control process. \n\nBest Practices and Fairness Alignment Documentation: Documentation that aligns AI changes with industry best practices and fairness. \n\nCompliance and Fairness Control Records: Records of compliance with the change control framework that emphasize fairness. | Safety & Performance  Explanation: Deliverables would include a Change Control Framework Document outlining the procedures and criteria for managing changes and a Change Request and Approval Log recording every proposed change, the review process, and final decisions. \n\nAdditionally, a Compliance Alignment Record would be maintained documenting how each change adheres to industry standards and regulatory requirements. | Impact Explanation: Change Control Policy Documents: Detailed guidelines and procedures outlining the change control process, including roles, responsibilities, and workflows for proposing and implementing changes. \n\nChange Approval Records: Documentation of the review and approval process for each change, including details of the change, decision rationale, and approval authority. \n\nCompliance and Best Practice Alignment Reports: Analysis demonstrating how each change aligns with industry best practices and compliance requirements."
    },
    {
        "control_name": "AI System Registration Verification",
        "category": "AI Model Management",
        "description": "Implement a procedure to regularly verify and validate the registration details of high-risk AI systems in databases required by local authorities (e.g., EU database). This should include checks for accuracy, completeness, and any updates or changes made since the last verification",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-4: Information Flow Enforcement\nAC-8: System Use Notification\nAC-21: Information Sharing\nAU-6: Audit Record Review, Analysis, and Reporting\nCA-3: Information Exchange\nCA-6: Authorization\nCM-14: Signed Components\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-22: Component Marking\nRA-3 Risk Assessment\nSA-9: External System Services\nSC-5: Denial of Service Protection\nSC-8: Transmission Confidentiality and Integrity\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity, SCF: AST-02: Asset Inventories - Maintaining accurate and complete inventories of AI systems, aligning with EU database registration details.\nCHG-02: Configuration Change Control - Controlling and documenting changes in AI systems, ensuring alignment with their registered details in the EU database.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with EU regulations through regular verification of AI system registration details.\nCFG-01: Configuration Management Program - Implementing a configuration management program that includes regular verification of AI system registration details.\nMNT-01: Maintenance Operations - Including verification of AI system registration details in regular maintenance and update operations.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Regularly reviewing and verifying AI system registration details as part of portfolio management.\nRSK-04: Risk Assessment - Performing risk assessments to validate the accuracy and completeness of AI system registration details.\nTDA-01: Technology Development & Acquisition - Incorporating verification of registration details into the development and acquisition processes of AI systems.",
        "explainability": "The AI System Registration Verification control describes the process of verifying the registration and compliance status of the AI system with relevant regulatory authorities and standards bodies. It provides a rationale for the importance of registration verification in ensuring legal compliance, transparency, and adherence to industry standards.",
        "evidence": "Rationale Explanation: \nRegistration Verification Procedures: A document outlining the procedures and methods used to verify the registration and compliance status of the AI system with regulatory authorities and standards bodies. \n\nRationale for Verification: A report explaining the reasons for conducting registration verification, such as ensuring legal compliance, demonstrating transparency, and adherence to industry standards. \n\nVerification Results Report: A report summarizing the outcomes of the registration verification process, including any issues identified and how they were addressed. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of registration verification for legal compliance, transparency, and industry standards adherence. | Responsibility Explanation: Registration Verification Procedure: A procedure detailing the steps for verifying the registration details of high-risk AI systems. \n\nVerification Checklists: Checklists used to ensure each required detail is accurately captured and up to date in the registration. \n\nVerification Reports: Reports generated from the verification process documenting the findings and any necessary updates. | Data Explanation: The Registration Verification Protocol details the regularity of verification, the methods used to ensure the accuracy and completeness of registration details and the process for updating the EU database. | Fairness Explanation: Registration Verification Protocol with Fairness: A protocol that ensures registration verification includes fairness considerations. \n\nRegistration Accuracy and Fairness Review Reports: Reports that review the accuracy and fairness of AI system registration details. \n\nVerification and Fairness Update Logs: Logs that capture verification checks and updates made using a fairness perspective. | Safety & Performance  Explanation: Deliverables would include a Registration Verification Protocol detailing the procedures for validating AI system details in the EU database and a Registration Verification Report documenting the findings of each verification exercise. \n\nAn Updates and Amendments Log would also be maintained recording any changes made to the system's registration details. | Impact Explanation: Verification Protocol Documentation: Detailed procedures and guidelines for verifying the registration details of AI systems in the EU database. \n\nVerification Reports: Comprehensive reports documenting each verification process, findings, and any discrepancies identified. \n\nUpdate and Change Logs: Records of all updates or changes made to the AI system since the last verification, ensuring that the registration details remain current."
    },
    {
        "control_name": "AI System Documentation",
        "category": "AI Model Management",
        "description": "Maintain an updated inventory of high-risk AI systems, capturing essential details such as the deployer, system specifics, intended use, governance protocols, and assessment summaries.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 11: Technical Documentation, NIST 800-53: AC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-9: Power Equipment and Cabling\nPM-23: Data Governance Body\nSA-9: External System Services\nSI-4: System Monitoring\nSI-12: Information Management and Retention\nSI-16: Memory Protection\nSI-17: Fail-Safe Procedures, SCF: AST-02: Asset Inventories - Maintaining an updated inventory of high-risk AI systems, documenting details like deployer, system specifics, and intended use.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Documenting AI systems to ensure compliance with legal and regulatory requirements.\nCFG-01: Configuration Management Program - Including AI systems in the configuration management program, ensuring detailed documentation of system specifics and governance.\nIAO-03: System Security & Privacy Plan (SSPP) - Including AI system specifics, governance protocols, and assessments in the SSPP.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI system portfolios with comprehensive documentation of each system.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensuring project management practices include thorough documentation of AI systems.\nRSK-04: Risk Assessment - Documenting risk assessments summaries for high-risk AI systems in the system inventory.\nTDA-01: Technology Development & Acquisition - Documenting AI system specifics during development and acquisition processes.",
        "explainability": "The AI System Documentation control describes the process of creating and maintaining comprehensive documentation for the AI system, covering its architecture, design, components, and operational procedures. It provides a rationale for the importance of AI system documentation in ensuring transparency, understanding, and efficient operation.",
        "evidence": "Rationale Explanation: Documentation Structure and Guidelines: A document outlining the structure and guidelines for creating and maintaining AI system documentation, including sections for architecture, design, components, and operational procedures. \n\nRationale for Documentation: A report explaining the reasons for creating and maintaining comprehensive documentation, such as ensuring transparency, promoting understanding, and enabling efficient operation. \n\nDocumentation Repository: A centralized repository or system for storing and managing the AI system's documentation, ensuring easy access and version control. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI system documentation for transparency, understanding, and efficient operation. | Responsibility Explanation: AI System Inventory: An inventory that captures essential details of all high-risk AI systems, including documentation on the deployer, system specifics, and intended use. \n\nGovernance Documentation: Documentation of the governance protocols associated with each high-risk AI system. \n\n Assessment Summaries: Summaries of assessments conducted on the AI systems that capture key findings and recommendations. | Data Explanation: AI System Inventory Documentation includes comprehensive records of all high-risk AI systems, their deployment details, design specifics, intended use cases, governance measures in place, and summaries of any assessments or audits conducted. | Fairness Explanation: Comprehensive Fairness Documentation Protocol: A protocol for maintaining detailed AI system documentation that supports fairness. \n\nSystem Inventory and Fairness Alignment Records: Records that ensure AI system specifics are aligned with fairness requirements. \n\nDocumentation and Fairness Assurance Reports: Reports that assure stakeholders the AI system documentation reflects a commitment to fairness. | Safety & Performance  Explanation: Deliverables would include an AI System Inventory Document, which contains detailed records of each high-risk AI system, including specifications, deployment information, and usage details. \n\nA Governance and Assessment Summary for each system would also be maintained which provides an overview of the governance protocols in place and summaries of any assessments conducted. | Impact Explanation: AI System Inventory: A comprehensive database or document listing all high-risk AI systems, including details such as deployer information, system specifics, intended use, and governance protocols. \n\nSystem Usage and Assessment Summaries: Summaries or reports detailing the use of each AI system, along with summaries of any assessments or audits conducted. \n\nGovernance Protocol Documents: Detailed documentation of the governance protocols associated with each AI system."
    },
    {
        "control_name": "AI System Identification",
        "category": "AI Model Versioning",
        "description": "Implement procedures to capture and maintain unique identification details for each high-risk AI system.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-6: Least Privilege\nAC-8: System Use Notification\nAC-14: Permitted Actions Without Identification or Authorization\nAC-24: Access Control Decisions\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nSA-9: External System Services\nSI-4: System Monitoring\nSI-12: Information Management and Retention\nSI-16: Memory Protection\nSI-17: Fail-Safe Procedures, SCF: AST-02: Asset Inventories - Keeping an inventory of AI systems with unique identification details for each system.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring AI systems are identified and documented as per compliance requirements.\nCFG-01: Configuration Management Program - Managing and documenting the unique identifiers of AI systems as part of configuration management.\nMNT-01: Maintenance Operations - Ensuring AI systems' unique identifiers are considered and maintained during maintenance activities.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing portfolios of AI systems with clear identification for each system.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating the identification of AI systems into project management practices.\nRSK-04: Risk Assessment - Including unique identifiers of AI systems in risk assessments to ensure accurate tracking.\nTDA-01: Technology Development & Acquisition - Identifying AI systems uniquely during their development and acquisition.",
        "explainability": "The AI System Identification control describes the process of identifying and labeling the AI system and its components to ensure clear identification and traceability. It provides a rationale for the importance of AI system identification in enabling accountability, traceability, and effective management.",
        "evidence": "Rationale Explanation: Identification Framework: A document outlining the framework and guidelines for labeling and identifying the AI system and its components, including naming conventions and version tracking. \n\nRationale for Identification: A report explaining the reasons for implementing AI system identification, such as enabling accountability, traceability, and effective management. \n\nIdentification Records: Records of all identifications and labeling efforts, documenting how the framework is applied in practice. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI system identification for accountability, traceability, and effective management. | Responsibility Explanation: Identification Protocol: A protocol that outlines how high-risk AI systems will be uniquely identified and recorded. \n\nIdentification Records: Records that detail the unique identification details for each AI system. \n\nSystem Identification Logs: Logs that track the creation, update, and verification of AI system identifiers. | Data Explanation: The AI System Identification Protocol specifies the method for assigning and maintaining unique identifiers for each AI system. | Fairness Explanation: AI Identification and Fairness Process: A process for capturing AI system identification details that considers fairness implications. \n\nIdentification Fairness Certification Process: A certification process that confirms the fairness of AI system identification procedures. \n\nUnique Identifier Fairness Compliance Log: A log that records the compliance of AI system identifiers with fairness standards. | Safety & Performance  Explanation: Deliverables would include an AI System Identification Protocol outlining the process for assigning and recording unique identifiers and an AI System Registry, which is a comprehensive database containing the identification details for each high-risk AI system. | Impact Explanation: Identification Protocol Documentation: Detailed guidelines and procedures for assigning and maintaining unique identification details for each AI system. \n\nAI System Identification Records: A database or register containing the unique identification details of each AI system, including any relevant characteristics or specifications. \n\nSystem Tracking Logs: Logs that record the assignment and any changes to the identification details over time."
    },
    {
        "control_name": "Institutional AI Registration",
        "category": "AI Model Management",
        "description": "Mandate the registration of every high-risk AI system in the designated database, ensuring accuracy and timeliness of entries. The organization also ensures that personnel are familiar with the database's interface and functionalities.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAC-8: System Use Notification\nAC-14: Permitted Actions Without Identification or Authorization\nAC-24: Access Control Decisions\nAT-2: Literacy Training and Awareness\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPM-14: Testing, Training and Monitoring\nPM-25: Minimization of Personally Identifiable Information Used in Testing, Training and Research\nSA-9: External System Services\nSI-4: System Monitoring\nSI-12: Information Management and Retention\nSI-16: Memory Protection\nSI-17: Fail-Safe Procedures, SCF: AST-02: Asset Inventories - Registering and maintaining high-risk AI systems in asset inventories, ensuring accurate and timely entries in the designated database.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with regulations requiring the registration of high-risk AI systems in designated databases.\nCFG-01: Configuration Management Program - Including AI systems in the configuration management program, with a requirement for database registration.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI system portfolios with a focus on mandatory registration and database accuracy.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensuring the registration of AI systems as part of project management processes and database familiarization for personnel.\nRSK-01: Risk Management Program - Identifying risk associated with the non-registration or inaccurate registration of AI systems.\nTDA-01: Technology Development & Acquisition - Including the registration of developed or acquired AI systems in the designated database as part of acquisition protocols.",
        "explainability": "The Institutional AI Registration control describes the process of registering the AI system within the institution or organization, ensuring that it is properly documented and officially recognized. It provides a rationale for the importance of institutional AI registration in establishing accountability, ownership, and organizational oversight.",
        "evidence": "Rationale Explanation: Registration Procedure and Guidelines: A document outlining the procedures and guidelines for registering the AI system within the institution or organization, including the documentation required, contacts, and approval processes. \n\nRationale for Registration: A report explaining the reasons for institutional AI registration, such as establishing accountability, ownership, and organizational oversight. \n\nRegistration Records: Records of all registration efforts documenting how the procedures and guidelines are applied in practice. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of institutional AI registration for accountability, ownership, and organizational oversight. | Responsibility Explanation: Registration Protocol: A protocol outlining the process and timeline for registering high-risk AI systems. \n\nRegistration Records: Formal records of each system's registration details. \n\nDatabase User Guide: A guide or manual for personnel on how to use the database interface and functionalities for registration purposes. | Data Explanation: A Institutional Registration Policy includes the process for registering AI systems, ensuring data accuracy, and training for personnel on database use. | Fairness Explanation: Institutional Registration Fairness Guidelines: Guidelines to ensure the institutional registration of AI systems is carried out fairly. \n\nDatabase Entry Fairness and Timeliness Report: A report on the fairness and timeliness of AI system registration entries. \n\nRegistration Interface and Fairness Training Materials: Training materials that teach personnel to use the registration database in a manner that upholds fairness. | Safety & Performance  Explanation: Deliverables would include an AI System Registration Procedure Document outlining the process and guidelines for registering AI systems and a Database Training Manual, which provides instructions on using the database's interface and functionalities. \n\nA Registration Compliance Report would also be generated to document the status and accuracy of the AI systems registered in the database. | Impact Explanation: AI System Registration Guidelines: Detailed instructions and protocols for registering AI systems in the designated database. \n\nRegistration Records: Comprehensive records in the database detailing each AI system's specifics, including updates and changes. \n\nTraining Materials for Database Use: Resources and guides to train personnel in using the database effectively."
    },
    {
        "control_name": "Extended Requirements for High-Risk AI",
        "category": "AI Model Management",
        "description": "Ensure requirements for high-risk AI systems consider the environment, democracy, and rule of law. Emphasis is placed on the quality and relevance of datasets and information provided to deployers.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-6: Monitoring Physical Access\nPE-9: Power Equipment and Cabling\nSA-9: External System Services\nSC-2: Separation of System and User Functionality\nSC-7: Boundary Protection\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-20: Tainting\nSI-22: Information Diversity\nPT-2: Authority to Process Personally Identifiable Information\nPT-3: Personally Identifiable Information Processing Purpose\nRA-8 Privacy Impact Assessments\nRA-9: Criticality Analysis\nSA-15: Development Process, Standards, and Tools, SCF: AST-02: Asset Inventories - Ensuring high-risk AI systems are properly inventoried, with an emphasis on dataset quality and relevance.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with laws and regulations related to the environment, democracy, and rule of law in the context of AI systems.\nDCH-01: Data Protection - Focusing on the protection and quality of datasets used in high-risk AI systems.\nIAC-01: Identity & Access Management (IAM) - Managing access to high-risk AI systems and datasets, ensuring they meet extended requirements.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI system portfolios with an emphasis on dataset quality and legal and ethical considerations.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating these extended requirements in the management of AI projects.\nRSK-04: Risk Assessment - Assessing risk associated with high-risk AI systems, including their impact on the environment, democracy, and rule of law.\nTDA-01: Technology Development & Acquisition - Considering these extended requirements during the development and acquisition of AI systems.",
        "explainability": "The Extended Requirements for High-Risk AI control describes the additional requirements and measures that are specifically applied to high-risk AI systems. It provides a rationale for the importance of these extended requirements in addressing potential risk, ensuring safety, and meeting regulatory compliance.",
        "evidence": "Rationale Explanation: High-Risk AI Requirements Documentation: A document outlining the specific extended requirements for high-risk AI systems, including safety, transparency, accountability, and regulatory compliance. \n\nRationale for Extended Requirements: A report explaining the reasons for applying extended requirements to high-risk AI systems, such as addressing potential risk, risk, ensuring safety, and meeting regulatory standards. \n\nCompliance Records: Records of the compliance efforts with the extended requirements, documenting how they are implemented and adhered to in practice. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of extended requirements for high-risk AI in addressing potential risk, risk, ensuring safety, and meeting regulatory standards. | Responsibility Explanation: Extended Requirements Framework: A document outlining all additional requirements for high-risk AI systems beyond technical performance, including societal and legal considerations. \n\nDataset Quality and Relevance Reports: Reports assessing the datasets used for high-risk AI systems and their sourcing, quality, and relevance. \n\nInformation Provision Protocols: Protocols detailing the information to be provided to deployers, ensuring it they meets the extended requirements. | Data Explanation: Extended Requirement Documentation encompasses guidelines on how high-risk AI systems should be designed and deployed with respect to environmental, democratic, and legal considerations. | Fairness Explanation: Extended Fairness Requirements Protocol: A protocol outlining requirements that consider fairness in terms of societal impact. \n\nDataset Quality and Democracy Impact Report: A report that links dataset quality and information relevance to their impact on fairness within democratic and legal frameworks. \n\nAI System Democracy and Fairness Logs: Logs that document how AI systems uphold democracy and the rule of law in a fair manner. | Safety & Performance  Explanation: Deliverables would include a Requirements Compliance Document, which details how the AI system meets environmental, democratic, and legal standards and a Data Quality and Relevance Report assessing the datasets used for training and operation. An AI System Impact Analysis might also be prepared to evaluate the system's effects on the environment, democracy, and legal norms. | Impact Explanation: Extended Requirement Documentation: Detailed guidelines outlining the additional requirements for high-risk AI systems, including considerations for environmental impact, democratic values, and legal compliance. \n\nDataset Quality and Relevance Reports: Documentation evaluating the quality and relevance of datasets used in high-risk AI systems. \n\nEthical Compliance Certificates: Certifications or attestations that the AI system meets extended ethical standards."
    },
    {
        "control_name": "Identification of High-Risk AI Systems",
        "category": "AI Model Management",
        "description": "Ensure procedures are in place to identify and classify AI systems as high-risk based on their potential impacts and use cases.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-8: System Component Inventory\nCA-3: Information Exchange\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPS-3: Personnel Screening\nPS-7: External Personnel Security\nSA-9: External System Services\nSC-2: Separation of System and User Functionality\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-8: Spam Protection\nSI-16: Memory Protection\nSA-11: Developer Testing and Evaluation \nAT-2: Literacy Training and Awareness\nRA-1: Risk Assessment Policy and Procedures\nRA-3: Risk Assessment \n, SCF: AST-02: Asset Inventories - Maintaining inventories of AI systems and classifying them based on risk assessments.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring AI systems are classified as high-risk in compliance with relevant regulations and standards.\nCFG-01: Configuration Management Program - Using configuration management processes to track and classify high-risk AI systems.\nIAC-01: Identity & Access Management (IAM) - Implementing access controls based on the classification of AI systems as high-risk.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing AI system portfolios with a focus on identifying high-risk systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including the identification and classification of high-risk AI systems in project management practices.\nRSK-04: Risk Assessment - Conducting risk assessments to identify and classify AI systems as high-risk based on potential impacts and use cases.\nTDA-01: Technology Development & Acquisition - Identifying and classifying AI systems as high-risk during development and acquisition phases.",
        "explainability": "The Identification of High-Risk AI Systems control describes the process of identifying and categorizing AI systems as high risk based on predefined criteria. It provides a rationale for the importance of this identification process in targeting additional safety, compliance, and oversight measures.",
        "evidence": "Rationale Explanation: High-Risk AI Identification Criteria: A document outlining the criteria and factors used to identify and categorize AI systems as high-risk, including data sensitivity, potential impacts, and usage scenarios. \n\nRationale for Identification: A report explaining the reasons for identifying AI systems as high-risk, such as the need for additional safety, compliance, and oversight measures. \n\nIdentification Records: Records of all AI systems identified as high-risk documenting how the identification criteria are applied in practice. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of identifying high-risk AI systems for safety, compliance, and oversight. | Responsibility Explanation: High-Risk Identification Protocol: A protocol that outlines the criteria and process for identifying high-risk AI systems. \n\nRisk Impact Assessment Tools: Tools or checklists used to analyze potential impacts and classify AI systems accordingly. \n\nHigh-Risk Classification Documentation: Documentation that captures the classification process and rationale for each AI system deemed high-risk. | Data Explanation: High-Risk Identification Protocol outlines the criteria and process for determining whether an AI system is considered high risk. | Fairness Explanation: High-Risk AI Identification Process: A process that identifies high-risk AI systems with a focus on their societal impact and fairness. \n\nImpact-Based Fairness Classification Guidelines: Guidelines that classify AI systems based on their potential impact on fairness. \n\nHigh-Risk AI Fairness Identification Records: Records documenting the identification of high-risk AI systems and their implications for fairness. | Safety & Performance  Explanation: Deliverables would include a High-Risk AI Identification Protocol, which outlines the criteria and process for classifying AI systems and a Risk Assessment Report for each AI system documenting its evaluation and classification as high risk. Additionally, a High-Risk AI System Registry would be maintained, listing all AI systems identified as high risk. | Impact Explanation: Risk Identification Protocols: Detailed guidelines for assessing and classifying AI systems as high risk, including criteria and procedures. \n\nRisk Classification Reports: Documentation of the assessment and classification process for each AI system, detailing its potential impacts and rationale for its risk level. \n\nUse Case Analysis Documents: Analyses of AI systems' use cases to determine their potential impacts and risk classification."
    },
    {
        "control_name": "High-Risk AI System Registry",
        "category": "AI Model Management",
        "description": "Maintain a registry of high-risk AI systems, detailing their sectors and compliance status with relevant legislation.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-9: Power Equipment and Cabling\nSA-9: External System Services\nSI-4: System Monitoring\nSI-12: Information Management and Retention\nSI-16: Memory Protection\nSI-17: Fail-Safe Procedures, SCF: AST-02: Asset Inventories - Developing and maintaining an inventory for high-risk AI systems, including sector-specific details and compliance status.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring the AI systems registry reflects compliance with relevant legislation.\nCFG-01: Configuration Management Program - Incorporating high-risk AI systems into the configuration management program with registry updates.\nIAC-01: Identity & Access Management (IAM) - Managing access to the high-risk AI system registry, maintaining its integrity and security.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of high-risk AI systems and ensuring accurate registry entries.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including registry updates in project management processes for high-risk AI systems.\nRSK-04: Risk Assessment - Using risk assessments to inform the registry about the compliance and sector-specific aspects of high-risk AI systems.\nTDA-01: Technology Development & Acquisition - Ensuring that new and updated high-risk AI systems are accurately registered.",
        "explainability": "The High-Risk AI System Registry control describes the establishment and maintenance of a registry for high-risk AI systems within an organization or regulatory authority. It provides a rationale for the importance of the registry in ensuring oversight, compliance, and accountability for high-risk AI systems.",
        "evidence": "Rationale Explanation: Registry Establishment Guidelines: A document outlining the guidelines and procedures for establishing and maintaining the High-Risk AI System Registry, including criteria for inclusion and maintenance processes. \n\nRationale for the Registry: A report explaining the reasons for establishing and maintaining the registry, such as enhancing oversight, ensuring compliance, and accountability for high-risk AI systems. \n\nRegistry Records: Records of all high-risk AI systems included in the registry, documenting the criteria used for inclusion and any relevant compliance or oversight actions. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of the High-Risk AI System Registry for oversight, compliance, and accountability. | Responsibility Explanation: High-Risk AI Registry: A registry that lists all identified high-risk AI systems, categorized by sector and compliance status. \n\nCompliance Status Reports: Reports detailing each high-risk AI system’s compliance with relevant legislation. \n\nRegistry Maintenance Logs: Logs that track updates and changes to the high-risk AI system registry. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nData Inventory: A comprehensive inventory of data sources used to compile the registry, specifying the types of data, data providers, and data collection methods. \n\nData Usage Documentation: Detailed documentation describing how the data is utilized to assess the risk level of AI systems and maintain compliance records. \n\nData Governance Policy: A policy outlining the rules, procedures, and responsibilities for managing and updating the registry data. | Fairness Explanation: High-Risk AI Registry Fairness Protocol: A protocol for registry maintenance that ensures high-risk AI systems are documented with fairness considerations. \n\nCompliance and Fairness Registry Reports: Reports that detail the compliance status of high-risk AI systems and their adherence to fairness. \n\nSector-Specific Fairness Compliance Logs: Logs that record sector-specific compliance with fairness-related legislation for high-risk AI systems. | Safety & Performance  Explanation: Deliverables would include the High-Risk AI System Registry itself containing detailed entries for each system, including sector information and compliance status. Additionally, a Compliance Tracking Report for each AI system would be maintained documenting ongoing adherence to relevant laws and regulations. | Impact Explanation: AI System Registry Database: A comprehensive database listing all high-risk AI systems, including details about their sectors, deployment contexts, and compliance status with relevant legislation. \n\nCompliance Documentation: Records of each AI system's compliance with applicable laws and regulations. \n\nSector-Specific AI System Overviews: Summaries or profiles of high-risk AI systems categorized by sector, providing insight into their specific applications and impact areas."
    },
    {
        "control_name": "Risk Classification for Law Enforcement AI",
        "category": "AI Model Management",
        "description": "Define criteria for classifying AI tools used in law enforcement, such as polygraphs and profiling tools, emphasizing their potential impact on individual rights.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: AC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-8: System Component Inventory\nCA-3: Information Exchange\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPS-7: External Personnel Security\nSA-9: External System Services\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSC-12: Cryptographic Key Establishment and Management\nSI-16: Memory Protection, SCF: AST-02: Asset Inventories - Maintaining an inventory of law enforcement AI tools classified by risk to individual rights.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring AI tools used in law enforcement comply with legal standards protecting individual rights.\nIAC-20: Access Enforcement - Enforcing access controls on law enforcement AI tools based on their risk classification.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of law enforcement AI tools with a focus on their risk classification.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including the risk classification criteria in project management related to law enforcement AI tools.\nRSK-01: Risk Management Program - Incorporating the risk classification of law enforcement AI tools into the broader risk management program.\nRSK-03: Risk Identification - Identifying risk associated with law enforcement AI tools, emphasizing their potential impact on individual rights.\nRSK-04: Risk Assessment - Conducting detailed assessments of law enforcement AI tools to classify them based on risk to individual rights.",
        "explainability": "The Risk Classification for Law Enforcement AI control describes the process of classifying AI systems used in law enforcement based on predefined risk categories. It provides a rationale for the importance of this classification in targeting appropriate oversight, transparency, and accountability measures.",
        "evidence": "Rationale Explanation: Risk Classification Criteria: A document outlining the criteria and factors used to classify law enforcement AI systems into different risk categories, including potential for bias, impact on individual rights, and public safety implications.\n \nRationale for Classification: A report explaining the reasons for classifying law enforcement AI systems based on risk, such as the need for targeted oversight, transparency, and accountability measures. \n\nClassification Records: Records of all law enforcement AI systems classified based on risk, documenting the criteria used and any relevant oversight or transparency actions. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of risk classification for law enforcement AI in enhancing oversight, transparency, and accountability. | Responsibility Explanation: Law Enforcement AI Classification Criteria: A set of criteria tailored to classify AI tools in law enforcement based on their potential impact on rights. \n\nImpact Assessment Reports for Law Enforcement AI: Reports that assess the specific impacts of AI tools used in law enforcement. \n\nClassification Decision Records: Records documenting the decision-making process for classifying law enforcement AI tools. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nRisk Classification Framework: A document outlining the specific criteria and parameters used to classify AI tools in law enforcement based on their potential impact on individual rights, including factors such as data sensitivity, surveillance capabilities, and potential for bias. \n\nRisk Assessment Reports: Reports assessing the risk level of specific AI tools, using the established criteria, and detailing their impact on individual rights. \n\nPublic Disclosure Protocol: A protocol specifying how and when the classification of AI tools will be publicly disclosed to ensure transparency. | Fairness Explanation: Law Enforcement AI Classification Standards: Standards that define criteria for classifying law enforcement AI tools, emphasizing fairness and individual rights. \n\nRights Impact and Fairness Classification Report: A report that evaluates law enforcement AI tools for their impact on rights and fairness. \n\nPolygraph and Profiling Tool Fairness Evaluation: Evaluations of specific tools like polygraphs for their potential impact on fairness. | Safety & Performance  Explanation: Deliverables would include a Law Enforcement AI Risk Classification Framework detailing the criteria used to assess the impact of AI tools on individual rights and a Classification Report for each tool documenting its evaluation against these criteria. \n\nAdditionally, an AI Tool Impact Analysis may be prepared assessing potential risk to individual rights. | Impact Explanation: Classification Criteria Documentation: Detailed guidelines and criteria for classifying law enforcement AI tools based on their potential impact on individual rights. \n\nRisk Assessment Reports: Comprehensive evaluations of each AI tool used in law enforcement, documenting their risk classification and potential impacts. \n\nLaw Enforcement AI Tool Catalog: A catalog or database listing all AI tools used in law enforcement along with their risk classification."
    },
    {
        "control_name": "Mandatory AI System Registration",
        "category": "AI Model Management",
        "description": "Define and implement mandatory registration protocols for deployers of high-risk AI systems, emphasizing timely updates and accuracy.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-5: Separation of Duties\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-3: Configuration Change Control\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nCA-3: Information Exchange\nMA-2: Controlled Maintenance\nMA-6: Timely Maintenance\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-6: Monitoring Physical Access\nPE-9: Power Equipment and Cabling\nSA-9: External System Services\nSC-2: Separation of System and User Functionality\nSC-5: Denial of Service Protection\nSC-7: Boundary Protection\nSC-18: Mobile Code\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware and Information Integrity\nSI-8: Spam Protection\nSC-12: Cryptographic Key Establishment and Management\nSI-16: Memory Protection\n, SCF: AST-02: Asset Inventories - Including high-risk AI systems in asset inventories, with mandatory registration protocols ensuring accuracy and timeliness.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with legal requirements for mandatory registration of high-risk AI systems.\nCFG-01: Configuration Management Program - Integrating mandatory AI system registration into the configuration management process.\nIAC-01: Identity & Access Management (IAM) - Managing access based on the registration status of high-risk AI systems.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing portfolios of high-risk AI systems with mandatory registration requirements.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Implementing mandatory registration protocols in project management for high-risk AI systems.\nRSK-04: Risk Assessment - Assessing risk related to non-compliance with mandatory registration protocols for high-risk AI systems.\nTDA-01: Technology Development & Acquisition - Ensuring new high-risk AI systems undergo mandatory registration during development and acquisition.",
        "explainability": "The Mandatory AI System Registration control describes the requirement for all AI systems within an organization or jurisdiction to be registered with the appropriate regulatory authority. It provides a rationale for the importance of mandatory registration in ensuring oversight, compliance, and accountability for all AI systems.",
        "evidence": "Rationale Explanation: Mandatory Registration Guidelines: A document outlining the guidelines and procedures for mandatory registration of AI systems, including registration criteria, deadlines, and documentation requirements. \n\nRationale for Mandatory Registration: A report explaining the reasons for implementing mandatory registration, such as ensuring oversight, compliance, and accountability for all AI systems. \n\nRegistration Records: Records of all AI systems that have undergone mandatory registration, documenting the registration criteria, deadlines, and compliance actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of mandatory AI system registration in enhancing oversight, ensuring compliance, and accountability. | Responsibility Explanation: Mandatory Registration Protocol: A protocol outlining the process and requirements for mandatory registration of high-risk AI systems. \n\nRegistration Compliance Checklists: Checklists to ensure all registration requirements are met and the process is followed. \n\nRegistration Accuracy Certificates: Certificates confirming the accuracy of the registration details entered into the designated database. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nRegistration Guidelines: A document outlining the registration process, requirements, and data to be provided by deployers of high-risk AI systems. \n\nRegistration Database: A centralized database or registry that stores information about registered AI systems, including their specifications, usage, and responsible parties. \n\nData Accuracy Verification Mechanism: A mechanism or tool for verifying the accuracy of the information provided during registration and for updating it as needed. | Fairness Explanation: Mandatory Registration Fairness Protocol: A protocol that mandates the registration of high-risk AI systems, emphasizing fairness in system accountability. \n\nRegistration Timeliness and Fairness Documentation: Documentation ensuring the timely registration of AI systems, with an emphasis on fairness. \n\nRegistration Protocol Fairness Training Materials: Training materials that teach the importance of fair and accurate registration for high-risk AI systems. | Safety & Performance  Explanation: Deliverables would include a Mandatory Registration Protocol Document outlining the procedures for registration and a High-Risk AI Systems Registry, which is a comprehensive database of all registered systems. \n\nAn AI System Registration Compliance Report would also be generated documenting adherence to the registration requirements. | Impact Explanation: Registration Protocol Guidelines: Comprehensive guidelines detailing the mandatory registration process for high-risk AI systems, including timelines and requirements for updates. \n\nAI System Registration Records: Official records of all registered high-risk AI systems, maintained in a central database or registry. \n\nAccuracy Verification Documents: Documentation of the processes used to verify the accuracy of registration information."
    },
    {
        "control_name": "Life Cycle Performance Assurance for High-Risk AI Systems",
        "category": "AI Model Management",
        "description": "Implement monitoring and evaluation mechanisms to ensure that high-risk AI systems perform consistently and as intended from deployment through decommissioning.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 9: Risk Management System\nRecital 65: Emphasizes continuous risk management system throughout a high-risk AI system.\nRecital 74: Emphasizes the need for high-risk AI systems to meet an appropriate level of accuracy, robustness and cybersecurity., NIST 800-53: RA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nRA-8: Privacy Impact Assessments\nSI-4: System Monitoring\nSR-4: Provenance\nCA-1: Assessment, Authorization, and Monitoring Policies and Procedures\nCA-7: Continuous Monitoring\nIR-4: Incident Monitoring\nPE-20: Asset Monitoring and Tracking\nPM-14: Testing, Training, and Monitoring\nPM-31: Continuous Monitoring Strategy\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-8: System Use Notification\nCM-8: System Component Inventory\nCA-3: Information Exchange\nPE-3: Physical Access Control\nPE-4: Access Control for Transmission\nPE-5: Access Control for Output Devices\nPE-6: Monitoring Physical Access\nSA-9: External System Services\nSC-18: Mobile Code, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring high-risk AI systems comply with relevant laws and regulations throughout their lifecycle.\nCFG-01: Configuration Management Program - Managing the configuration of high-risk AI systems to ensure performance consistency from deployment to decommissioning.\nMON-01: Continuous Monitoring - Continuous monitoring of high-risk AI systems to ensure consistent and intended performance throughout their lifecycle.\nIAC-15: Account Management - Managing user accounts and access to high-risk AI systems to ensure they are used as intended.\nMNT-01: Maintenance Operations - Regular maintenance to ensure high-risk AI systems are performing as intended throughout their life cycle.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of high-risk AI systems, including performance assurance throughout their lifecycle.\nRSK-04: Risk Assessment - Risk assessments to evaluate the performance and impact of high-risk AI systems during their lifecycle.\nTDA-01: Technology Development & Acquisition - Incorporating lifecycle performance assurance in the development and acquisition of high-risk AI systems.",
        "explainability": "The Life Cycle Performance Assurance for High-Risk AI Systems control describes the process of assuring the performance of high-risk AI systems throughout their life cycle, including development, deployment, and operation. It provides a rationale for the importance of life cycle performance assurance in mitigating risk, ensuring reliability, and adhering to regulatory requirements.",
        "evidence": "Rationale Explanation: Performance Assurance Plan: A document outlining the plan and strategies for assuring the performance of high-risk AI systems throughout their life cycle, including testing procedures, monitoring, and compliance measures. \n\nRationale for Performance Assurance: A report explaining the reasons for assuring the performance of high-risk AI systems throughout their life cycle, such as risk mitigation, reliability, and regulatory compliance. \n\nAssurance Records: Records of all performance assurance efforts for high-risk AI systems, documenting the testing results, monitoring findings, and compliance actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of life cycle performance assurance for high-risk AI systems in mitigating 5 risk, ensuring reliability, and adhering to regulatory requirements. | Responsibility Explanation: Performance Assurance Plan: A comprehensive plan that outlines the monitoring and evaluation mechanisms for high-risk AI systems throughout their life cycle.\n\nPerformance Monitoring Logs: Logs that capture the ongoing monitoring activities and any performance issues identified. \n\nEvaluation Reports: Periodic reports that assess the high-risk AI systems' performance, from deployment to decommissioning. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nPerformance Monitoring Plan: A comprehensive plan outlining the key performance indicators (KPIs) and metrics used to evaluate the AI system's performance during its life cycle.\n\nData Collection and Storage Procedures: Detailed procedures for collecting, storing, and managing data related to the AI system's behavior, decisions, and performance. \n\nCompliance Reports: Regular reports summarizing the AI system's compliance with predefined standards and objectives. | Fairness Explanation: Life Cycle Performance and Fairness Assurance Protocol: A protocol that outlines performance assurance measures throughout the AI life cycle, with an emphasis on fairness. \n\nPerformance Consistency and Fairness Review Guidelines: Guidelines for reviewing AI system performance for consistency and fairness. \n\nDecommissioning Fairness Impact Reports: Reports that evaluate the fairness implications of AI system decommissioning. | Safety & Performance  Explanation: Deliverables would include a Life Cycle Performance Assurance Plan, which outlines the strategies for ongoing monitoring and evaluation and a System Performance Report documenting the AI system's performance at various stages of its life cycle. \n\nAdditionally, a Decommissioning Assessment would be prepared to evaluate the system's performance and impact at the end of its life cycle. | Impact Explanation: Impact Assessment Report: Detailed report outlining the potential positive and negative impacts of the AI system. \n\nSocietal Impact Statement: A document that assesses the broader societal implications of the AI system."
    },
    {
        "control_name": "Life Cycle-Centric AI Risk Management",
        "category": "AI Model Management",
        "description": "Implement a risk management framework that considers the entire life cycle of high-risk AI systems, from development to decommissioning.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 9: Risk Management System\nRecital 65: Emphasizes continuous risk management system throughout a high-risk AI system.\nRecital 74: Emphasizes the need for high-risk AI systems to meet an appropriate level of accuracy, robustness and cybersecurity., NIST 800-53: RA-1: Risk Assessment Policy and Procedures\nRA-2: Security Categorization\nRA-3: Risk Assessment\nRA-4: Risk Assessment Update\nRA-5: Vulnerability Monitoring and Scanning\nRA-6: Technical Surveillance Countermeasures Survey\nRA-7: Risk Response\nPM-9: Risk Management Strategy, SCF:  CFG-01: Configuration Management Program - Managing configurations across the lifecycle of high-risk AI systems to mitigate risk.\nMNT-01: Maintenance Operations - Incorporating risk management in the maintenance operations of high-risk AI systems.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of high-risk AI systems with a focus on lifecycle-centric risk management.\nRSK-01: Risk Management Program - Implementing a comprehensive risk management program that addresses all lifecycle stages of high-risk AI systems.\nRSK-03: Risk Identification - Identifying potential risk associated with high-risk AI systems throughout their life cycle.\nRSK-04: Risk Assessment - Conducting thorough risk assessments at different stages of the AI system's lifecycle, from development to decommissioning.\nRSK-06: Risk Remediation - Remediation strategies to address identified risk during the AI system's lifecycle.\nTDA-01: Technology Development & Acquisition - Ensuring risk management is a core aspect of the development and acquisition of high-risk AI systems.",
        "explainability": "The Life Cycle-Centric AI Risk Management control describes the approach of managing AI-related risk throughout the entire life cycle of AI systems, including development, training, deployment, and operation. It provides a rationale for the importance of life cycle-centric risk management in identifying, mitigating, and monitoring risk to ensure safety, compliance, and reliability.",
        "evidence": "Rationale Explanation: Risk Management Framework: A document outlining the framework and strategies for managing AI-related risk throughout the life cycle, including risk identification, assessment, mitigation, and monitoring processes. \n\nRationale for Risk Management: A report explaining the reasons for adopting a life cycle-centric approach to AI risk management, such as the need for safety, compliance, and reliability. \n\nRisk Management Records: Records of all risk management activities and their outcomes, documenting how the framework is applied in practice. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of lifecycle-centric AI risk management in identifying, mitigating, and monitoring risk for safety, compliance, and reliability. | Responsibility Explanation: Life Cycle Risk Management Framework: A document outlining the approach to managing risk throughout the AI system's life cycle.\n\nLife Cycle Risk Assessments: Periodic assessments that identify and evaluate risk at different stages of the AI system's life cycle.\n \nRisk Mitigation Action Plans: Plans detailing mitigation strategies for identified risk at each life cycle stage. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nRisk Management Plan: A comprehensive plan outlining the methodologies and processes for identifying, assessing, and mitigating risk associated with high-risk AI systems across their life cycle.\n\nRisk Assessment Reports: Reports summarizing the outcomes of risk assessments conducted at various stages of the AI system's life cycle, including development, deployment, and decommissioning. \n\nRisk Register: A dynamic record of identified risk and its potential impacts and mitigation strategies, regularly updated as the AI system evolves. | Fairness Explanation: Life Cycle Fairness Risk Assessment Report: A detailed report that assesses fairness at every life cycle stage of the AI system. \n\nBias Mitigation Life Cycle Documentation: Comprehensive documentation that describes measures taken to mitigate bias over the AI system's life cycle.\n\nFairness Assurance Protocol for Decommissioning: A specific protocol to ensure fairness is maintained when AI systems are decommissioned. | Safety & Performance  Explanation: Deliverables would include a Life Cycle-Centric Risk Management Plan documenting the risk management strategies and practices throughout the AI system's life cycle and a Life Cycle Risk Assessment Report detailing the identified risk and mitigation strategies at each life cycle stage. \n\nAn AI System Risk Registry could also be maintained, recording all identified risk and its their management over time. | Impact Explanation: Risk Management Plan: A comprehensive document outlining the strategies for identifying, assessing, and mitigating risk throughout the AI system's life cycle. \n\nRisk Register: A detailed log of identified risk, their potential impacts, and planned mitigation measures."
    },
    {
        "control_name": "Regular Review of AI Risk Criteria",
        "category": "AI Model Management",
        "description": "Implement procedures to regularly review and stay informed of the evolving criteria set by regulatory bodies for high-risk AI system classification.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: SI-1: System and Information Integrity Policy and Procedures\nPS-1: Personnel Security Policy and Procedures\nPT-6: System of Records Notice\nSA-1: System and Services Acquisition Policy and Procedures\nAT-1: Awareness and Training Policy and Procedures\nAT-2: Literacy Training and Awareness\nSC-1: System and Communications Protection Policy and Procedures\nMA-3: Maintenance Tools\n\n\n, SCF: AST-02: Asset Inventories - Updating asset inventories to reflect the latest classification of high-risk AI systems based on revised risk criteria.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Regularly reviewing compliance with evolving regulatory criteria for high-risk AI system classification.\nCFG-01: Configuration Management Program - Updating the configuration management program based on the latest AI risk criteria and regulatory guidelines.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of high-risk AI systems with a focus on regularly reviewing AI risk criteria.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including the review of AI risk criteria in project management processes for high-risk AI systems.\nRSK-03: Risk Identification - Identifying risk associated with non-compliance or outdated criteria in the classification of high-risk AI systems.\nRSK-04: Risk Assessment - Conducting risk assessments that incorporate the latest high-risk AI system classification criteria from regulatory bodies.\nTDA-01: Technology Development & Acquisition - Ensuring the development and acquisition of high-risk AI systems are aligned with current regulatory risk criteria.",
        "explainability": "The Regular Review of AI Risk Criteria control describes the practice of regularly reviewing and updating the criteria and factors used to assess AI-related risk. It provides a rationale for the importance of regular reviews to ensure the relevancy and effectiveness of risk assessment for safety, compliance, and reliability.",
        "evidence": "Rationale Explanation: Risk Criteria Review Guidelines: A document outlining the guidelines and procedures for reviewing and updating AI risk assessment criteria, including the frequency of reviews, stakeholders involved, and criteria modification processes.\n \nRationale for Regular Review: A report explaining the reasons for conducting regular reviews of AI risk criteria, such as the need for ensuring relevancy and effectiveness in assessing risk for safety, compliance, and reliability. \n\nReview Records: Records of all reviews of AI risk criteria, documenting the modifications made and the rationale behind each change. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of regular review of AI risk criteria for safety, compliance, and reliability. | Responsibility Explanation: AI Risk Criteria Review Protocol: A protocol outlining how and when the AI risk criteria will be reviewed. \n\nRegulatory Update Records: Records of changes and updates in regulatory criteria pertaining to high-risk AI systems. \n\nCriteria Compliance Reports: Reports detailing the organization's compliance with the current regulatory criteria for high-risk AI systems. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nRegulatory Criteria Documentation: Documentation of the current criteria set by relevant regulatory bodies for classifying high-risk AI systems. \n\nReview Procedures: Detailed procedures outlining how and when the organization reviews and updates its understanding of regulatory criteria. \n\nCompliance Reports: Reports summarizing the organization's compliance with evolving regulatory criteria. | Fairness Explanation: Evolving Fairness Criteria Guideline Document: A guideline that outlines the evolving nature of fairness criteria and how they should be regularly reviewed. \n\nAI Risk Reclassification Impact Analysis: An in-depth analysis that examines the implications of risk reclassification on the fairness of AI system operations. \n\nRegulatory Fairness Criteria Update Manual: A manual that provides up-to-date regulatory criteria on fairness, ensuring AI systems are classified in accordance with the latest standards. | Safety & Performance  Explanation: Deliverables would include a Regulatory Criteria Review Protocol outlining the process for staying updated on changes in high-risk AI system classification criteria and a Regulatory Compliance Update Report documenting how the latest regulatory criteria have been incorporated into the organization's risk assessment and management practices. | Impact Explanation: Updated Risk Criteria Document: A revised document outlining the criteria used to assess risk associated with the AI system. \n\nReview Summary Report: A summary report detailing the findings and changes made during the review of AI risk criteria."
    },
    {
        "control_name": "High-Risk AI System Registration",
        "category": "AI Model Management",
        "description": "Deployers must register their use of specific high-risk AI systems in the designated country (e.g., mainland China, EU) database, ensuring transparency and traceability.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nCA-3: Information Exchange\nCP-11: Alternate Communications Protocols\nSI-1: System and Information Integrity Policy and Procedures\nSI-7: Software, Firmware and Information Integrity\n, SCF: AST-02: Asset Inventories - Including high-risk AI systems in asset inventories and ensuring their registration in the EU database for transparency.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Complying with EU regulations requiring the registration of high-risk AI systems.\nCFG-01: Configuration Management Program - Ensuring high-risk AI systems are correctly configured and registered as per EU guidelines.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of high-risk AI systems, including adherence to registration requirements.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including registration protocols in project management for high-risk AI systems.\nRSK-01: Risk Management Program - Incorporating high-risk AI system registration into the organization’s overall risk management strategy.\nRSK-04: Risk Assessment - Assessing risk related to non-compliance with high-risk AI system registration protocols.\nTDA-01: Technology Development & Acquisition - Incorporating registration requirements in the development and acquisition of high-risk AI systems.",
        "explainability": "The High-risk AI System Registration control describes the process of registering high-risk AI systems with the appropriate regulatory authority or oversight body. It provides a rationale for the importance of registration in ensuring transparency, accountability, and regulatory compliance for high-risk AI systems.",
        "evidence": "Rationale Explanation: Registration Procedure and Guidelines: A document outlining the procedures and guidelines for registering high-risk AI systems, including registration criteria, documentation requirements, and contacts. \n\nRationale for Registration: A report explaining the reasons for requiring registration of high-risk AI systems, such as transparency, accountability, and regulatory compliance. \n\nRegistration Records: Records of all high-risk AI systems that have undergone registration, documenting the registration criteria met, documentation submitted, and compliance actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of high-risk AI system registration for transparency, accountability, and regulatory compliance. | Responsibility Explanation: High-Risk AI Registration Protocol: A protocol that outlines the process for deployers to register high-risk AI systems. \n\nRegistration Compliance Checklist: A checklist to ensure that all steps and requirements for registration are completed. \n\nRegistration Confirmation Documents: Documents that confirm a deployer has registered the AI system and provide proof of registration. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nRegistration Guidelines: A document outlining the registration process, data requirements, and procedures for deployers to follow when registering high-risk AI systems. \n\nRegistration Records: Detailed records of each registered high-risk AI system, including information on its purpose, data sources, intended use, and responsible parties. \n\nData Validation Procedures: Procedures for validating the accuracy and completeness of the registration data. | Fairness Explanation: High-Risk AI Registration Fairness Checklist: A checklist that ensures all high-risk AI system registrations are completed with fairness as a key criterion. \n\nTransparency and Fairness Compliance Ledger: A ledger that tracks the compliance of high-risk AI system registrations with established fairness protocols. \n\nRegistration Fairness Verification Forms: Forms used to verify the fairness of the registration process for high-risk AI systems. | Safety & Performance  Explanation: Deliverables would include a High-Risk AI System Registration Guide detailing the process for registration in the EU database and a Registration Compliance Document documenting the successful registration of each system. \n\nAn AI System Traceability Report would also be prepared showcasing how the registration process enhances the system's traceability. | Impact Explanation: AI System Registration Documentation: Detailed documentation providing information on the high-risk AI system, including its purpose, functionality, and potential impacts. \n\nRegistration Report: A report summarizing the key details of the registered high-risk AI system and its compliance with regulatory requirements."
    },
    {
        "control_name": "Residual Risk Communication",
        "category": "AI Model Management",
        "description": "Establish a communication protocol to inform deployers about the residual risk associated with high-risk AI systems and the rationale behind system decisions.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 6: Classification Rules for High-Risk AI Systems\nArticle 9: Risk Management System, NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-8: System Use Notification\nCA-3: Information Exchange\nCP-11: Alternate Communications Protocols\nSA-9: External System Services\nSI-7: Software, Firmware and Information Integrity\nMP-5: Media Transport\n, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with regulations that mandate residual risk communication.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of high-risk AI systems with a focus on residual risk communication.\nRSK-01: Risk Management Program - Integrating residual risk communication into the overall risk management strategy for AI systems.\nRSK-04: Risk Assessment - Conducting risk assessments that include evaluating and communicating residual risk of high-risk AI systems.\nRSK-06: Risk Remediation - Communicating residual risk and remediation strategies to deployers of high-risk AI systems.",
        "explainability": "The Residual Risk Communication control describes the process of communicating and documenting residual risk associated with AI systems after risk mitigation efforts have been applied. It provides a rationale for the importance of residual risk communication in ensuring transparency, awareness, and informed decision making regarding AI system risk.",
        "evidence": "Rationale Explanation: Residual Risk Communication Plan: A document outlining the plan for communicating and documenting residual risk, risk, including the timing, audience, and methods of communication. \n\nRationale for Communication: A report explaining the reasons for communicating residual risk, risk, such as transparency, awareness, and informed decision-making. decision making.\n\nResidual Risk Records: Records of all residual risk  communicated and documented, including descriptions of risk, risk, mitigation efforts, and any ongoing concerns. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of residual risk communication for transparency, awareness, and informed decision-making. decision making. | Responsibility Explanation: Residual Risk Communication Plan: A plan that outlines how and when residual risk will be communicated to deployers. \n\nRisk Communication Records: Records that document each instance of communication regarding residual risk. \n\nRisk Rationale Documentation: Documentation that provides the rationale for decisions related to the risk of high-risk AI systems. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nCommunication Protocol: A documented protocol outlining how and when residual risk will be communicated to deployers, including the format and channels of communication. \n\nResidual Risk Reports: Reports summarizing the residual risk associated with high-risk AI systems, along with explanations of the underlying rationale and decision-making processes. \n\nDecision Rationale Documentation: Detailed documentation providing the rationale behind system decisions, including design choices, data sources, and risk assessments. | Fairness Explanation: Residual Risk and Fairness Disclosure Templates: Templates for disclosing residual risk that are designed to clearly communicate any remaining fairness concerns. \n\nFairness-Oriented Risk Rationale Explanations: Detailed explanations that outline the rationale behind the residual risk from a fairness perspective. \n\nDeployer Fairness Communication Protocols: Protocols that standardize the communication of residual risk to deployers with an emphasis on fairness. | Safety & Performance  Explanation: Deliverables would include a Residual Risk Communication Plan detailing the approach for informing deployers about this risk and these decision-making processes and a Risk and Rationale Documentation, which provides comprehensive information on the identified residual risk and the logic behind AI decisions. | Impact Explanation: Residual Risk Communication Plan: A documented plan outlining how residual risk will be communicated to stakeholders. \n\nResidual Risk Report: A report detailing the identified residual risk, their potential impacts, and the steps taken to communicate and manage them."
    },
    {
        "control_name": "Conformity Documentation Provision",
        "category": "AI Model Management",
        "description": "Establish a protocol to promptly provide required documentation, where applicable, to national authorities (e.g., the AI Office or the EU Commission) to demonstrate the conformity of high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 11: Technical Documentation, NIST 800-53: CA-3: Information Exchange\nMP-1: Media Protection Policy and Procedures\nMP-5: Media Transport\nCP-11: Alternate Communications Protocols\nSI-1: System and Information Integrity Policy and Procedures\nPS-1: Personnel Security Policy and Procedures\nPT-6: System of Records Notice\nSA-1: System and Services Acquisition Policy and Procedures\nAT-1: Awareness and Training Policy and Procedures\nAT-2: Literacy Training and Awareness\nSC-1: System and Communications Protection Policy and Procedures\nMA-3: Maintenance Tools\n\n\n, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with regulations that mandate residual risk communication.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Managing a portfolio of high-risk AI systems with a focus on residual risk communication.\nRSK-01: Risk Management Program - Integrating residual risk communication into the overall risk management strategy for AI systems.\nRSK-04: Risk Assessment - Conducting risk assessments that include evaluating and communicating residual risk of high-risk AI systems.",
        "explainability": "The Conformity Documentation Provision control describes the process of providing and maintaining conformity documentation for AI systems, including certificates, compliance reports, and conformity statements. It provides a rationale for the importance of conformity documentation in ensuring compliance, transparency, and accountability.",
        "evidence": "Rationale Explanation: Conformity Documentation Guidelines: A document outlining the guidelines and procedures for providing and maintaining conformity documentation for AI systems, including the content, format, and record-keeping requirements. \n\nRationale for Conformity Documentation: A report explaining the reasons for providing conformity documentation, such as ensuring compliance, transparency, and accountability. \n\nConformity Records: Records of all conformity documentation provided, including certificates, compliance reports, and conformity statements, documenting the details and any compliance actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of conformity documentation provision for compliance, transparency, and accountability. | Responsibility Explanation: Conformity Documentation Protocol: A protocol that outlines the steps to provide conformity documentation upon request. \n\nDocumentation Provision Logs: Logs that track each request for and provision of conformity documentation. \n\nConformity Certification Records: Records or certificates that provide evidence of the AI system's conformity with relevant standards. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nDocumentation Protocol: A documented protocol outlining the procedures, timelines, and channels for providing required documentation to national authorities, the AI Office, or the EU Commission. \n\nConformity Documentation: Comprehensive documentation demonstrating the conformity of high-risk AI systems, including technical specifications, risk assessments, data sources, and compliance reports. \n\nCompliance Verification Records: Records of the verification process to ensure that the provided documentation aligns with regulatory requirements. | Fairness Explanation: Fairness-Conformity Documentation Procedures: Procedures for the provision of documentation that attest to the conformity of AI systems with fairness standards. \n\nAuthority Fairness Documentation Submission Guides: Guides that outline the process for submitting documentation to authorities to demonstrate fairness conformity. \n\nConformity and Fairness Evidence Folders: Folders that collect and organize evidence of conformity with fairness standards for high-risk AI systems. | Safety & Performance  Explanation: Deliverables would include a Conformity Documentation Protocol outlining the procedures for compiling and submitting documentation and a Conformity Documentation Package, which contains all required documents proving the AI system's compliance with regulatory standards. \n\nA Submission Tracking Log would also be maintained to record all instances of documentation provision to authorities. | Impact Explanation: Conformity Documentation: Comprehensive documentation showcasing the AI system's compliance with relevant standards, regulations, and ethical guidelines. \n\nConformity Statement: A formal statement confirming the AI system's adherence to specified criteria."
    },
    {
        "control_name": "AI System Registration",
        "category": "AI Model Management",
        "description": "Implement a standardized registration process for high-risk AI systems as required by local authorities (e.g., the EU database), capturing essential system details and compliance status.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: MP-1: Media Protection Policy and Procedures\nMP-5: Media Transport\nCP-11: Alternate Communication Protocols\nSI-1: System and Information Integrity Policy and Procedures\nPS-1: Personnel Security Policy and Procedures\nPT-6: System of Records Notice\nSA-1: System and Services Acquisition Policy and Procedures\nAT-1: Awareness and Training Policy and Procedures\nAT-2: Literacy Training and Awareness\nSC-1: System and Communications Protection Policy and Procedures\nMA-3: Maintenance Tools\n\n\n, SCF: AST-02: Asset Inventories - Including high-risk AI systems in the organization's asset inventories and ensuring their registration in the EU database.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with EU regulatory requirements for the registration of high-risk AI systems.\nCFG-01: Configuration Management Program - Managing and documenting configurations of high-risk AI systems as part of the standardized registration process.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Incorporating high-risk AI system registration requirements into portfolio management practices.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Integrating high-risk AI system registration requirements into project management practices.\nRSK-01: Risk Management Program - Ensuring that the risk management program accounts for the registration of high-risk AI systems.\nRSK-04: Risk Assessment - Assessing the risk implications of inaccuracies or non-compliance in the AI system registration process.\nTDA-01: Technology Development & Acquisition - Including registration requirements in the protocols for developing and acquiring high-risk AI systems.",
        "explainability": "The AI System Registration control describes the process of registering AI systems within an organization or regulatory framework. It provides a rationale for the importance of AI system registration in ensuring transparency, accountability, and compliance with regulatory requirements.",
        "evidence": "Rationale Explanation: Registration Procedures and Guidelines: A document outlining the procedures and guidelines for registering AI systems, including registration criteria, documentation requirements, and contacts. \n\nRationale for Registration: A report explaining the reasons for registering AI systems, such as ensuring transparency, accountability, and regulatory compliance. \n\nRegistration Records: Records of all AI systems that have undergone registration, including details of the registration process, documentation submitted, and any compliance actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI system registration for transparency, accountability, and regulatory compliance. | Responsibility Explanation: AI System Registration Guidelines: A set of guidelines detailing the registration process and the types of information required for the EU database. \n\nRegistration Completion Certificates: Certificates issued upon successful registration, indicating compliance and entry into the database. \n\nCompliance Status Reports: Reports that detail the AI system's compliance with EU regulations at the time of registration. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nRegistration Guidelines: Documented guidelines outlining the standardized process for registering high-risk AI systems in the EU database. \n\nRegistration Forms: Standardized forms or templates for capturing essential system details, including purpose, data sources, intended use, and compliance status. \n\nCompliance Documentation: Documentation demonstrating the AI system's compliance with relevant EU regulations and standards. | Fairness Explanation: Standardized AI Registration Fairness Guidelines: Guidelines that establish a standardized registration process for AI systems with an emphasis on fairness. \n\nFairness-Accurate AI System Registry Manuals: Manuals that detail the standardized process for registering AI systems, focusing on accuracy and fairness. \n\nCompliance and Fairness Registration Checklists: Checklists that guide the registration of AI systems, ensuring compliance with fairness standards. | Safety & Performance  Explanation: Deliverables would include a Standardized Registration Procedure Document outlining the steps and requirements for registering AI systems in the EU database and a High-Risk AI System Registration Record documenting the details and compliance status of each system registered. | Impact Explanation: AI System Registration Form: A standardized form capturing essential information about the AI system, including its purpose, functionality, and potential risk. \n\nRegistration Certificate: A formal certificate issued upon successful completion of the AI system registration process."
    },
    {
        "control_name": "Voluntary AI System Registration",
        "category": "AI Model Management",
        "description": "Provide a mechanism for deployers outside of specific categories to voluntarily register their high-risk AI system usage, if applicable (e.g., EU database).",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-8: System Use Notification\nCA-3: Information Exchange\nCP-11: Alternate Communication Protocols\nSA-9: External System Services\nSI-7: Software, Firmware and Information Integrity\nMP-5: Media Transport\n, SCF: AST-02: Asset Inventories - Including voluntarily registered high-risk AI systems in organizational asset inventories.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Encouraging voluntary compliance with the EU database registration process for high-risk AI systems.\nCFG-01: Configuration Management Program - Documenting configurations of voluntarily registered high-risk AI systems in compliance with the EU database requirements.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Integrating voluntary AI system registration into portfolio management processes.\nRSK-04: Risk Assessment - Assessing risk related to the voluntary registration of high-risk AI systems and ensuring data accuracy.\nTDA-01: Technology Development & Acquisition - Considering voluntary registration requirements during the development and acquisition of AI systems.",
        "explainability": "The Voluntary AI System Registration control describes the option for AI system owners to voluntarily register their systems within an organization or regulatory framework. It provides a rationale for the importance of voluntary registration in promoting transparency, accountability, and fostering a culture of responsible AI usage.",
        "evidence": "Rationale Explanation: Voluntary Registration Guidelines: A document outlining the guidelines and procedures for voluntary registration of AI systems, including the benefits of registration and the process for participation. \n\nRationale for Voluntary Registration: A report explaining the reasons for offering voluntary registration, such as promoting transparency, accountability, and responsible AI usage. \n\nVoluntary Registration Records: Records of AI systems that have undergone voluntary registration, documenting the registration process and any benefits realized.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of voluntary AI system registration for transparency, accountability, and responsible AI usage. | Responsibility Explanation: Voluntary Registration Protocol: A protocol that outlines the process for voluntary registration of high-risk AI systems. \n\nVoluntary Registration Records: Records of all AI systems registered voluntarily, capturing the date and details of registration. \n\nVoluntary Registration Reports: Reports that summarize the voluntary registration activities and any trends or insights gleaned. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nVoluntary Registration Process: A documented process outlining the steps and procedures for deployers to voluntarily register their high-risk AI systems in the EU database. \n\nVoluntary Registration Forms: Standardized forms or templates for capturing essential system details and compliance status for voluntarily registered AI systems. \n\n Compliance Documentation: Documentation demonstrating the AI system's compliance with relevant EU regulations and standards. | Fairness Explanation: Voluntary Registration Fairness Framework: A set of guidelines that detail how voluntary registration contributes to fairness. \n\nInclusive Registry Participation Reports: Reports on the participation in the voluntary registry, analyzed for fairness implications. \n\n Voluntary Registration Impact Analysis: An analysis of how the voluntary registration of AI systems affects fairness across the AI ecosystem. | Safety & Performance  Explanation: Deliverables would include a Voluntary Registration Framework outlining the process and incentives for voluntary registration and a Voluntary AI System Registry in the EU database documenting voluntarily registered systems and their details. \n\nA Voluntary Registration Participation Report might also be generated to track  the engagement of deployers with the voluntary registration process. | Impact Explanation: Voluntary Registration Portal: An online portal or mechanism where developers can voluntarily register their AI systems. \n\nVoluntary Registration Confirmation: A confirmation acknowledgment or receipt provided to developers upon successful voluntary registration."
    },
    {
        "control_name": "Decision Verification",
        "category": "AI Model Management",
        "description": "Establish a protocol where decisions made by high-risk AI systems undergo a verification process by at least two competent, trained, and authorized individuals.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: CP-11: Alternate Communications Protocols\nCA-3: Information Exchange\nPS-6: Access Agreements\nAT-1: Awareness and Training Policy and Procedures\nAT-2: Literacy Training and Awareness\nAT-4: Training Records, SCF: CHG-03: Security Impact Analysis for Changes - Analyzing the security impact of AI decisions and the verification process.\nCFG-02: System Hardening Through Baseline Configurations - Implementing configurations to support the decision verification process in AI systems.\nMON-01: Continuous Monitoring - Continuously monitoring the AI decision-making and verification process.\nHRS-03: Roles & Responsibilities - Defining roles and responsibilities for individuals involved in verifying decisions made by high-risk AI systems.\nHRS-03.1: User Awareness - Training and raising awareness among individuals responsible for verifying AI system decisions.\nHRS-03.2: Competency Requirements for Security-Related Positions - Ensuring individuals verifying AI decisions have the necessary skills and training.\nIAC-20: Access Enforcement - Enforcing access controls to ensure only authorized individuals can verify AI decisions.\nRSK-06: Risk Remediation - Addressing risk associated with the AI decision-making process and its verification.",
        "explainability": "The Decision Verification control describes the process of verifying AI-driven decisions through a set of procedures and criteria. It provides a rationale for the importance of decision verification in ensuring accountability, transparency, and the reliability of AI systems.",
        "evidence": "Rationale Explanation: Decision Verification Procedures: A document outlining the procedures and criteria for verifying AI-driven decisions, including the frequency of verification, the verification process, and reporting requirements. \n\nRationale for Verification: A report explaining the reasons for decision verification, such as ensuring accountability, transparency, and the reliability of AI systems.\n \nVerification Records: Records of all decision verifications, documenting the verification process, results, and any actions taken based on verification findings. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of decision verification for accountability, transparency, and the reliability of AI systems. | Responsibility Explanation: Decision Verification Protocol: A detailed protocol for how decisions by high-risk AI systems should be verified by human overseers. \n\n Verification Activity Logs: Logs that document each verification activity, including details of the personnel involved and the outcome. \n\n Decision Verification Certificates: Documents that certify a decision has been verified by the required number of competent individuals. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nVerification Protocol: A documented protocol outlining the procedures and criteria for verifying AI system decisions, including the roles and responsibilities of authorized individuals. \n\nVerification Records: Detailed records of the verification process, including decision outcomes, reviewer identities, and any modifications made. | Fairness Explanation: Decision Verification Protocol: A protocol establishing how decisions are to be verified for fairness. \n\nVerified Decision Fairness Certification: Certification that decisions made by AI systems have been verified for fairness. \n\nDecision Verification Training and Authorization Records: Records of training and authorization of personnel involved in decision verification, with a focus on fairness. | Safety & Performance  Explanation: Deliverables would include a Decision Verification Protocol outlining the steps and criteria for the verification process and a Verification Personnel Training Manual detailing the training required for individuals involved in the verification process. \n\nA Decision Verification Log would also be maintained documenting each decision reviewed and the outcomes of the verification process. | Impact Explanation: Decision Verification Plan: A documented plan outlining the strategies and criteria for verifying AI system decisions. \n\nVerification Reports: Regular reports summarizing the outcomes of decision verification activities and any corrective actions taken."
    },
    {
        "control_name": "AI System Dependency Documentation",
        "category": "AI Model Management",
        "description": "Develop an exhaustive documentation framework that captures the intricate interactions of AI systems with other software, hardware, or AI modules. This documentation should highlight interdependencies, dataflow diagrams, and potential points of vulnerability.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 11: Technical Documentation, NIST 800-53: AC-2: Account Management\nAC-4: Information Flow Enforcement\nAC-21: Information Sharing\nAC-23: Data Mining Protection\nAC-24: Access Control Decisions\nAC-25: Reference Monitor\nCA-3: Information Exchange\nCA-7: Continuous Monitoring\nCA-9: Internal Systems Connections\nCM-2: Baseline Configuration\nCM-6: Configuration Settings\nCM-8: System Component Inventory\nCM-12: Information Location\nCM-13: Data Action Mapping\nCM-14: Signed Components\nCP-9: System Backup\nCP-10: System Recovery and Reconstitution\nIA-3: Device Identification and Authentication\nIA-7: Cryptographic Module Authentication\nIA-9: Service Identification and Authentication\nIA-12: Identity Proofing\nMA-2: Controlled Maintenance\nMP-4: Media Storage\nPE-3: Physical Access Control\nPL-2: System Security and Privacy Plans\nPL-8: Security and Privacy Architectures\nPL-10: Baseline Selection\nPL-11: Baseline Tailoring\nSA-9: External System Services\nSA-10: Developer Testing and Evaluation\nSA-23: Specialization\nSC-13: Cryptographic Protection\nSC-18: Mobile Code\nSC-28: Protection of Information at Rest\nSC-32: System Partitioning \nSC-36: Distributed Processing and Storage\nSC-49: Hardware-Enforced Separation and Policy Enforcement\n\n, SCF: AST-04: Network Diagrams & Data Flow Diagrams (DFDs) - Creating network and data flow diagrams to illustrate AI system dependencies and interactions.\nCFG-01: Configuration Management Program - Managing and documenting the configuration of AI systems, including their dependencies.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including dependency documentation considerations in project management practices for AI systems.\nRSK-01: Risk Management Program - Integrating dependency documentation into the risk management program for AI systems.\nRSK-04: Risk Assessment - Assessing risk associated with AI system dependencies and potential vulnerabilities.\nTDA-01: Technology Development & Acquisition - Including dependency documentation in the development and acquisition of AI systems.",
        "explainability": "The AI System Dependency Documentation control describes the process of documenting the dependencies of an AI system on external components, data sources, and services. It provides a rationale for the importance of dependency documentation in ensuring transparency, risk assessment, and effective troubleshooting of AI systems.",
        "evidence": "Rationale Explanation: Dependency Documentation Guidelines: A document outlining the guidelines and procedures for documenting dependencies of AI systems, including the types of dependencies, documentation format, and update procedures. \n\nRationale for Dependency Documentation: A report explaining the reasons for documenting dependencies, such as ensuring transparency, risk assessment, and effective troubleshooting.\n \nDependency Records: Records of all AI system dependencies, documenting the types of dependencies, sources, and any changes or updates made to the dependencies.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of AI system dependency documentation for transparency, risk assessment, and effective troubleshooting. | Responsibility Explanation: Dependency Documentation Framework: A framework defining how dependencies should be documented, including data flows and points of potential vulnerability. \n\nInterdependency Maps: Visual maps or diagrams that detail the interconnections between AI systems and other components. \n\nDependency Audit Reports: Reports from audits that assess the comprehensiveness of the dependency documentation and identify any areas of risk. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nDocumentation Framework: A comprehensive framework outlining the structure and components of the documentation, including data flow diagrams, interdependencies, and vulnerability points. \n\nDependency Documentation: Detailed documentation that captures the interactions of the AI system with other software, hardware, or AI modules, including data sources and data flow pathways. \n\nVulnerability Assessment: Documentation highlighting potential points of vulnerability and security risk associated with system dependencies. | Fairness Explanation: Dependency Documentation Fairness Guidelines: Guidelines that dictate how AI system dependencies should be documented with fairness in mind. \n\nInterdependency Analysis and Fairness Report: A report that analyzes AI system interdependencies for potential fairness implications. \n\nData Flow and Vulnerability Fairness Documentation: Documentation that highlights data flows and vulnerabilities in AI systems, with a focus on maintaining fairness. | Safety & Performance  Explanation: Comprehensive dependency documentation that includes:\n \nData Flow Diagrams (DFDs): Mapping out the data exchange within the system components. \n\nInterdependency Matrices: Outlining the relationships between different software, hardware, or AI modules. \n\nVulnerability Reports: Identifying potential points of failure or security risk.\n\nPerformance Benchmarking Reports: Detailing the system’s efficiency and response times under various conditions. | Impact Explanation: Dependency Documentation: A comprehensive document listing all dependencies, including software libraries, data sources, and external services, along with version information and usage details. \n\nDependency Impact Assessment: An assessment report outlining the potential impact of dependencies on the AI system's performance and decision making."
    },
    {
        "control_name": "Standardized AI Reporting API",
        "category": "AI Model Management",
        "description": "Develop an API that enforces a standardized format for AI reporting, ensuring data consistency and facilitating easier aggregation and analysis of AI system data.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-2: Account Management\nAC-4: Information Flow Enforcement\nAC-21: Information Sharing\nAC-23: Data Mining Protection\nAU-2: Event Logging\nAU-6: Audit Records Review, Analysis, and Reporting\nCA-3: Information Sharing\nCA-9: Internal System Connections\nMA-3: Maintenance Tools\nPL-9: Central Management\nPM-3: Information Security and Privacy Resources, SCF: AST-02: Asset Inventories - Including standardized AI reporting APIs in organizational asset inventories for better management and oversight.\nCHG-02: Configuration Change Control - Managing changes to the AI reporting API configuration to maintain standardization and consistency.\nCFG-01: Configuration Management Program - Managing and standardizing configurations for AI reporting APIs to ensure data consistency.\nMON-01: Continuous Monitoring - Continuously monitoring the effectiveness and compliance of the AI reporting APIs.\nIAC-05: Identification & Authentication for Third Party Systems & Services - Managing identification and authentication for access to the AI reporting API.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Integrating standardized AI reporting API requirements into project management practices.\nTDA-01: Technology Development & Acquisition - Considering the need for standardized reporting APIs in the development and acquisition of AI systems.",
        "explainability": "The Standardized AI Reporting API control describes the implementation of a standardized application programming interface (API) for reporting and monitoring AI system performance and issues. It provides a rationale for the importance of a standardized reporting API in ensuring consistency, transparency, and ease of reporting for AI systems.",
        "evidence": "Rationale Explanation: Reporting API Specification: A document outlining the technical specifications for the standardized reporting API, including endpoints, data formats, and authentication mechanisms. \n\nRationale for Standardization: A report explaining the reasons for implementing a standardized reporting API, such as ensuring consistency, transparency, and ease of reporting for AI systems. \n\nAPI Documentation: Documentation of the standardized reporting API, including usage instructions, code samples, and error handling guidelines. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of a standardized reporting API for consistency, transparency, and ease of reporting for AI systems. | Responsibility Explanation: Reporting API Specification: A specification document for the API, detailing the standardized format for AI reporting. \n\nAPI Implementation Records: Records of the API's implementation across different systems, ensuring standardized reporting. \n\nAPI Usage Reports: Reports that summarize how the API is being used and the consistency of the data being reported. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nAPI Documentation: Comprehensive documentation outlining the standardized format and specifications for AI reporting using the API. \n\nStandardized AI Reports: AI reports generated using the API, adhering to the standardized format and containing relevant data on AI system behavior and performance. \n\nData Consistency Guidelines: Guidelines for maintaining data consistency and quality when using the API for reporting. | Fairness Explanation: Reporting API Fairness Standards: Standards for a reporting API that ensures fairness in AI system reporting. \n\nStandardized Reporting Fairness Compliance Certificates: Certificates that confirm compliance with fairness standards in AI reporting. \n\nAPI Development and Fairness Assessment Reports: Reports that assess the development of the reporting API for its ability to enforce fairness in reporting. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAPI Development Specification Document: Outlining the standards for data formats, endpoints, and communication protocols. \n\nIntegration Test Reports: Verifying the API’s compatibility with different AI systems and platforms. \n\nAPI Documentation: Providing a detailed guide for developers on how to use the standardized reporting API. \n\nChange Log Files: Record all modifications made to the API, ensuring traceability. | Impact Explanation: Reporting API Specification: A documented specification outlining the standardized format and structure of the AI reporting interface. \n\nAPI Usage Guidelines: Guidelines for developers on how to integrate and use the standardized reporting API effectively."
    },
    {
        "control_name": "AI Resilience Strategy Evaluation",
        "category": "AI Model Management",
        "description": "Evaluate and update AI system resilience strategies, incorporating real-world testing environments and feedback loops periodically. This should include periodic stress tests, vulnerability assessments, and penetration tests.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-4: Information Flow Enforcement\nAC-11: Device Lock\nAC-23: Data Mining Protection\nAT-2: Literacy Training and Awareness\nCA-2: Control Assessments\nCA-8: Penetration Testing\nCP-9: System Backup\nCP-10: System Recovery and Reconstitution\nCP-13: Alternate Security Mechanisms\nIR-2: Incident Response Training\nIR-3: Incident Response Testing\nIR-8: Incident Response Plan\nIR-9: Information Spillage Response\nPM-12: Insider Threat Program\nPM-16: Threat Awareness Program\nRA-5: Vulnerability Monitoring and Scanning\nSI-5: Security Alerts, Advisories, and Directives\nSI-11: Error Handling\nSI-17: Fail-Safe Procedures\n\n, SCF: CHG-03: Security Impact Analysis for Changes - Analyzing the impact of changes on AI systems' resilience and updating strategies accordingly.\nCFG-01: Configuration Management Program - Managing configurations to support and enhance AI systems' resilience.\nMON-01: Continuous Monitoring - Utilizing continuous monitoring tools to evaluate AI systems' resilience in real-time.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating resilience strategy evaluation into project management for AI systems.\nRSK-04: Risk Assessment - Conducting comprehensive risk assessments to evaluate AI system resilience and identify potential vulnerabilities.\nRSK-06: Risk Remediation - Addressing identified risk in AI systems to enhance their resilience.\nTDA-01: Technology Development & Acquisition - Considering resilience during the development and acquisition of AI systems.\nVPM-07: Penetration Testing - Implementing penetration tests to evaluate the resilience of AI systems against potential attacks.",
        "explainability": "The AI Resilience Strategy Evaluation control describes the process of evaluating the resilience strategy for AI systems, including strategies to handle unexpected events, failures, and adversarial attacks. It provides a rationale for the importance of resilience strategy evaluation in ensuring the robustness, reliability, and security of AI systems.",
        "evidence": "Rationale Explanation: Resilience Strategy Evaluation Plan: A document outlining the plan and procedures for evaluating the resilience strategy for AI systems, including testing scenarios, evaluation criteria, and reporting mechanisms. \n\nRationale for Evaluation: A report explaining the reasons for evaluating the resilience strategy, such as ensuring robustness, reliability, and security of AI systems in the face of unexpected events and adversarial attacks. \n\nEvaluation Records: Records of all resilience strategy evaluations, documenting the evaluation results, findings, and any improvements made to the strategy. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI resilience strategy evaluation for the robustness, reliability, and security of AI systems. | Responsibility Explanation: Resilience Strategy Document: A comprehensive document outlining the resilience strategies for AI systems, including testing and feedback mechanisms. \n\nStress Test and Vulnerability Assessment Reports: Reports from periodic stress tests, vulnerability assessments, and penetration tests conducted on the AI systems. \n\nStrategy Update Logs: Logs documenting each update made to the resilience strategy based on the evaluations and feedback received. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nAI System Accountability Framework: A documented framework outlining the principles and standards for accountability in AI system design, development, and operation. \n\nThird-Party Compliance Documentation: Documentation confirming that third-party entities involved in the AI life cycle adhere to the defined accountability standards. \n\nAudit Reports: Reports summarizing the outcomes of regular audits conducted to assess system integrity and compliance with accountability standards. | Fairness Explanation: Resilience Strategy Fairness Evaluation Protocol: A protocol for evaluating AI resilience strategies, with an emphasis on fairness. \n\nStress Test Fairness Analysis Reports: Reports that analyze the fairness of outcomes from AI system stress tests.\n \nPenetration Test and Vulnerability Assessment Fairness Records: Records of penetration tests and vulnerability assessments that focus on identifying fairness implications. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAI Resilience Strategy Report: Detailing current strategies and future plans. \n\nStress Test Results: Evaluate the AI system’s performance under extreme conditions. \n\nVulnerability Assessment Reports: Highlighting potential security weaknesses. \n\nPenetration Test Findings: Documenting the outcomes of simulated cyberattacks.\n\nFeedback Loop Documentation: Illustrating the processes for incorporating user and system feedback into resilience strategy updates. | Impact Explanation: Resilience Strategy Document: A documented plan outlining the strategies employed to enhance the AI system's resilience. \n\nResilience Evaluation Report: A report summarizing the outcomes of the evaluation of the AI system's resilience strategies."
    },
    {
        "control_name": "AI System Integrity & Accountability",
        "category": "AI Model Management",
        "description": "Adopt and maintain AI system designs that emphasize accountability. Ensure that any third-party third party involved in the AI life cycle fits the new definitions and that regular audits are conducted to maintain system integrity.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: PL-2: System Security and Privacy Plans\nPL-4: Rules of Behavior\nPM-11: Mission and Business Process Definition\nPM-15: Security and Privacy Groups and Associations\nPM-23: Data Governance Body\nPM-24: Data Integrity Board\nSI-13: Predictable Failure Prevention\nSR-2: Supply Chain Risk Management Plan\nSR-3: Supply Chain Controls and Processes\nSR-4: Provenance\nSR-9: Tamper Resistance and Detection, SCF: CHG-03: Security Impact Analysis for Changes - Analyzing the impact of changes on AI systems to ensure continued integrity and accountability.\nMON-01: Continuous Monitoring - Continuously monitoring AI systems to detect and address issues related to integrity and accountability.\nGOV-01: Cybersecurity & Data Protection Governance Program - Implementing a governance program that includes accountability measures for AI systems.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assigning specific responsibilities to maintain AI system integrity and uphold accountability.\nRSK-04: Risk Assessment - Assessing risk to AI system integrity and accountability and implementing measures to mitigate these risk.\nTPM-01: Third-Party Management - Managing third parties involved in the AI life cycle to ensure they adhere to accountability and integrity standards.",
        "explainability": "The AI System Integrity & Accountability control describes the measures and processes in place to ensure the integrity and accountability of AI systems. It provides a rationale for the importance of AI system integrity and accountability in maintaining trust, transparency, and ethical usage of AI systems.",
        "evidence": "Rationale Explanation: Integrity and Accountability Guidelines: A document outlining the guidelines and measures for ensuring the integrity and accountability of AI systems, including data handling, model behavior, and accountability mechanisms. \n\nRationale for Integrity & and Accountability: A report explaining the reasons for prioritizing AI system integrity and accountability, such as maintaining trust, transparency, and ethical usage. \n\nAccountability Records: Records of accountability measures, including documentation of AI system behavior, data handling, and any actions taken in case of integrity breaches. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI system integrity and accountability for trust, transparency, and ethical usage. | Responsibility Explanation: Integrity and Accountability Framework: A framework that outlines the design principles for AI system integrity and accountability. \n\nThird-Party Compliance Records: Records of compliance checks and audits conducted on any third parties involved in the AI life cycle. \n\nSystem Audit Reports: Reports from regular audits conducted to ensure ongoing AI system integrity and accountability. | Data Explanation: The deliverables for Data Explanation in this control include: \n\n AI System Accountability Framework: A documented framework outlining the principles and standards for accountability in AI system design, development, and operation. \n\n Third-Party Compliance Documentation: Documentation confirming that third-party entities involved in the AI life cycle adhere to the defined accountability standards. \n\n Audit Reports: Reports summarizing the outcomes of regular audits conducted to assess system integrity and compliance with accountability standards. | Fairness Explanation: Integrity & and Accountability Design Document: A document outlining AI system design principles that prioritize accountability and fairness. \n\nThird-Party Fairness Integration Reports: Reports assessing the integration of third-party entities into the AI life cycle with respect to fairness. \n\nAI System Audit Fairness Logs: Logs documenting regular audits of AI system integrity with fairness as a critical metric. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAI Design Accountability Framework: Outlining the principles of accountable AI design. \n\nThird-Party Compliance Certificates: Confirming that all involved parties meet the set standards. \n\nAudit Reports: Detailing the findings of regular system integrity checks. \n\nIncident Response Plans: Documenting the protocols for addressing any integrity or accountability issues. | Impact Explanation: Integrity and Accountability Framework: A documented framework outlining measures for ensuring the integrity of AI system decisions and establishing accountability mechanisms. \n\nIncident Response Plan: A plan specifying steps to be taken in the event of integrity breaches or accountability issues."
    },
    {
        "control_name": "AI Model Watermarking & Accountability",
        "category": "AI Model Versioning",
        "description": "Embed watermarks in AI model code and training data to trace potential theft sources and hold malicious actors accountable.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nSA-15: Development Process, Standards, and Tools\nSA-17: Developer Security and Privacy Architecture and Design\n, SCF: AAT-12: AI & Autonomous Technologies Intellectual Property Infringement Protections - Protecting the intellectual property of AI models through watermarking to prevent theft and unauthorized use.\nCFG-02: System Hardening Through Baseline Configurations - Including watermarking as part of system hardening to trace AI model code and data.\nMON-01: Continuous Monitoring - Continuously monitoring for the integrity of watermarks in AI models to ensure their effectiveness and traceability.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assigning responsibilities for implementing and monitoring watermarking in AI models.\nIAC-10: Authenticator Management - Managing authentication measures such as watermarking to ensure the traceability of AI models.\nTDA-01: Technology Development & Acquisition - Ensuring watermarking techniques are integrated during the development and acquisition of AI technologies.",
        "explainability": "The AI Model Watermarking & Accountability control describes the practice of watermarking AI models and establishing accountability measures to track model ownership and usage. It provides a rationale for the importance of model watermarking and accountability in ensuring model attribution, transparency, and responsible usage.",
        "evidence": "Rationale Explanation: Watermarking and Accountability Procedures: A document outlining the procedures for watermarking AI models and establishing accountability measures, including the watermarking process, accountability mechanisms, and record-keeping. record keeping.\n\nRationale for Watermarking & and Accountability: A report explaining the reasons for implementing model watermarking and accountability, such as ensuring model attribution, transparency, and responsible usage. \n\nAccountability Records: Records of accountability measures, including model watermarking information, usage logs, and actions taken in case of misuse. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI model watermarking and accountability for model attribution, transparency, and responsible usage. | Responsibility Explanation: Watermarking Implementation Guidelines: Guidelines on how to implement watermarking in AI models and training data.\n \nWatermarking Records: Records detailing where and how watermarks have been implemented in AI models and data. \n\nTheft and Misuse Incident Reports: Reports on incidents where watermarked AI models or data were involved, aiding in tracking and accountability. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nWatermarking Documentation: Comprehensive documentation outlining the watermarking process, including the methods used, locations of watermarks, and their purpose. \n\nWatermarked AI Models and Data: AI models and training data that have been watermarked to enable traceability and accountability. \n\nUnauthorized Use Reports: Reports documenting instances of unauthorized use or theft detected through watermarking. | Fairness Explanation: Model Watermarking Protocol: A protocol for embedding watermarks that outlines the process and its significance for accountability and fairness. \n\nWatermarking Traceability and Fairness Report: A report that details how watermarking contributes to traceability and fairness in AI systems. \n\nAccountability in Case of Theft Documentation: Documentation that describes procedures for holding malicious actors accountable with fairness considerations. | Safety & Performance  Explanation: Deliverables for this control include: \n\nWatermarking Implementation Guide: Describing the methods used to embed watermarks in the AI model and data. \n\nCode and Data Audit Trails: Log the usage and access of the model/data, which is supported by watermarking. \n\nIncident Response Documentation: Detailing steps to be taken if unauthorized use is detected. \n\nIntellectual Property Rights (IPR) Documentation: Affirming the ownership and terms of use for the model and data. | Impact Explanation: Watermarked AI Models: AI models embedded with unique watermarks for traceability. \n\nAccountability Mechanism Documentation: Documentation outlining the mechanisms in place to hold AI models accountable."
    },
    {
        "control_name": "Comprehensive Data Life Cycle Management",
        "category": "AI Model Management",
        "description": "Establish robust processes for data generation, acquisition, security, maintenance, and dissemination, ensuring AI system integrity.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nRecital 65: Emphasizes continuous risk management system throughout a high-risk AI system.\nRecital 74: Emphasizes the need for high-risk AI systems to meet an appropriate level of accuracy, robustness and cybersecurity., NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-21: Information Sharing\nAC-23: Data Mining Protection\nAU-13: Monitoring for Information Disclosure\nCM-12: Information Location\nCM-13: Data Action Mapping\nMA-3 Maintenance Tools\nPM-1: Information Security Program Plan\nPM-3: Information and Privacy Resources\nPM-14: Testing, Training, and Monitoring\nPM-17: Protecting Controlled Unclassified Information on External Systems\nPM-23: Data Governance Body\nPM-24: Data Integrity Board\nSI-12: Information Management and Retention\nSI-18: Personally Identifiable Information Quality Operations\nSI-19: De-Identification\nSI-20: Tainting\nSI-21: Information Refresh\nSI-22: Information Diversity\nSI-23: Information Fragmentation\n\n\n\n, SCF: AST-02: Asset Inventories - Maintaining inventories of data assets used in AI systems for better lifecycle management.\nCHG-02: Configuration Change Control - Managing changes in systems handling AI data to maintain integrity throughout its life cycle.\nCFG-02: System Hardening Through Baseline Configurations - Hardening systems that handle AI data to ensure security throughout its life cycle.\nMON-01: Continuous Monitoring - Continuously monitoring the data life cycle processes in AI systems for integrity and security.\nDCH-01: Data Protection - Protecting data throughout its life cycle in AI systems.\nDCH-02: Data & Asset Classification - Classifying data used in AI systems to manage it appropriately throughout its lifecycle.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including comprehensive data lifecycle management in project management for AI systems.\nRSK-04: Risk Assessment - Assessing risk associated with data life cycle in AI systems and implementing mitigation strategies.",
        "explainability": "The Comprehensive Data Life Cycle Management control describes the end-to-end management of data throughout its life cycle, including data acquisition, storage, processing, and disposal. It provides a rationale for the importance of comprehensive data life cycle management in ensuring data quality, privacy, security, and compliance.",
        "evidence": "Rationale Explanation: Data Life Cycle Management Policy: A document outlining the policy and procedures for comprehensive data life cycle management, including data acquisition guidelines, storage practices, processing protocols, and data disposal procedures. \n\nRationale for Data Life Cycle Management: A report explaining the reasons for adopting comprehensive data life cycle management, such as ensuring data quality, privacy, security, and compliance. \n\nData Life Cycle Records: Records of data life cycle management activities, including data acquisition sources, storage details, processing logs, and data disposal documentation.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of comprehensive data life cycle management for data quality, privacy, security, and compliance. | Responsibility Explanation: Data Life Cycle Management Plan: A plan that details the processes for managing data throughout its life cycle in AI systems. \n\nData Security and Maintenance Protocols: Protocols that outline the measures for data security and maintenance. \n\nData Life Cycle Compliance Reports: Reports assessing compliance with data life cycle management processes and the integrity of data at each stage. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nData Life Cycle Management Framework: A documented framework outlining the processes and procedures for managing data throughout its life cycle, from generation to dissemination. \n\nData Security Protocols: Documentation of security protocols and measures in place to protect data integrity and prevent unauthorized access. \n\nData Maintenance Records: Records of data maintenance activities, including data cleaning, quality checks, and updates. | Fairness Explanation: Data Life Cycle Management Fairness Guidelines: Guidelines that detail the management of data throughout its life cycle, with an emphasis on fairness. \n\nData Security and Fairness Maintenance Plan: A plan that outlines how data security practices are upheld to maintain fairness. \n\nData Dissemination Fairness Strategy: A strategy for disseminating data that ensures its integrity and fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nData Life Cycle Management Policy: Outlining the standards for handling data at each stage. \n\nData Security Protocols: Detailing measures for protecting data against unauthorized access and leaks.\n \nMaintenance Logs: that Record the history of updates, patches, and changes to the data sets. datasets.\n\nData Dissemination Reports: Describing the methods and controls for sharing data internally and externally. | Impact Explanation: Data Life Cycle Management Plan: A documented plan outlining strategies for data collection, storage, processing, and disposal throughout its life cycle.\n\nData Quality and Compliance Reports: Reports detailing the quality and compliance status of data at different stages of its life cycle."
    },
    {
        "control_name": "Nonconformity Corrective Protocol",
        "category": "AI Model Feedback Security",
        "description": "Establish a structured process designed to identify, assess, and rectify deviations in AI model behavior to ensure alignment with established performance, ethical, and regulatory standards.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nCA-1: Assessment, Authorization, and Monitoring Policies and Procedures\nPE-4: Access Control for Transmission\nPM-6: Measures of Performance\nPM-14: Testing, Training, and Monitoring\nPM-22: Personally Identifiable Information Quality Management\nPM-24: Data Integrity Board\nPM-25: Minimization of Personally Identifiable Information Used in Testing, Training, and Research\nRA-5: Vulnerability Monitoring and Scanning\nSA-3: System Development Life Cycle\nSA-11: Developer Testing and Evaluation\nSA-15: Development Process, Standards, and Tools\nSA-16: Developer-Provided Training",
        "explainability": "The Nonconformity Corrective Protocol control describes the protocol for addressing nonconformities or deviations from established standards or requirements in AI systems. It provides a rationale for the importance of having a corrective protocol in place to ensure timely identification, reporting, and rectification of nonconformities, thereby maintaining system integrity, compliance, and reliability.",
        "evidence": "Rationale Explanation: Corrective Protocol Guidelines: A document outlining the protocol and procedures for addressing non-conformities nonconformities in AI systems, including identification, reporting, investigation, rectification, and preventive measures. \n\nRationale for Corrective Protocol: A report explaining the reasons for having a corrective protocol, such as ensuring system integrity, compliance, and reliability. \n\nCorrective Protocol Records: Records of non-conformities nonconformities identified, reported, investigated, and rectified documenting the details of each case and any preventive actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of a corrective protocol for addressing nonconformities non-conformities in AI systems and its role in ensuring system integrity, compliance, and reliability. | Responsibility Explanation: Corrective Protocol Documentation: A document outlining the process for identifying and rectifying nonconformities in AI models. \n\nNonconformity Records: Records of identified nonconformities and the actions taken to rectify them. \n\nCorrection Effectiveness Reports: Reports evaluating the effectiveness of the corrective actions taken in realigning the AI model with required standards. | Data Explanation: The deliverables for Data Explanation in this control include: \n\nCorrective Protocol Documentation: Comprehensive documentation outlining the structured process for identifying, assessing, and rectifying nonconformities in AI model behavior. \n\nNon-Conformity Assessment Reports: Reports summarizing the assessment of identified nonconformities, including their impact and proposed corrective actions. \n\nCorrective Action Records: Records of the corrective actions taken to rectify nonconformities and bring AI model behavior into alignment with standards. | Fairness Explanation: Corrective Protocol for Nonconformity: A protocol outlining steps to correct nonconformities in AI behavior, with fairness as a central focus.\n \nEthical Deviation and Fairness Correction Reports: Reports that document the correction of ethical deviations, emphasizing their impact on fairness. \n\nRegulatory Nonconformity Fairness Logs: Logs that record instances of regulatory nonconformity and subsequent actions to restore fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nNonconformity Identification Procedures: Outlining steps to detect deviations in AI behavior. \n\nAssessment Reports: Evaluating the impact and cause of the deviations. \n\nCorrective Action Plans: Detailing steps to mitigate and rectify identified nonconformities\n\nPostcorrection Evaluation Documents: Verify the effectiveness of corrective actions. | Impact Explanation: Corrective Protocol Document: A documented protocol outlining the steps to be taken when nonconformities are identified, including corrective actions and preventive measures. \n\nNonconformity Reports: Reports documenting instances of nonconformities, actions taken, and measures to prevent recurrence."
    },
    {
        "control_name": "Monitoring of High-Risk AI Systems",
        "category": "AI Model Health Monitoring",
        "description": "Establish procedures to continuously monitor the performance, behavior, and impact of high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: CA-7: Continuous Monitoring\nPM-31: Continuous Monitoring Strategy\n, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Governing AI technologies with a focus on continuous performance and health monitoring.\nCFG-02: System Hardening Through Baseline Configurations - Hardening systems that manage AI to ensure they can effectively monitor AI health and performance.\nMON-01: Continuous Monitoring - Implementing continuous monitoring strategies to assess the performance and behavior of high-risk AI systems.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assigning roles and responsibilities for monitoring and maintaining the health of high-risk AI systems.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including AI system health monitoring in project management practices for high-risk AI projects.\nRSK-04: Risk Assessment - Conducting risk assessments to understand the impact of AI systems on organizational operations and objectives.\nRSK-11: Risk Monitoring - Continually monitoring risk associated with high-risk AI systems, including performance and behavioral risk.",
        "explainability": "The Monitoring of High-Risk AI Systems control describes the practice of ongoing monitoring and oversight of AI systems identified as high risk due to their potential impact or sensitivity. It provides a rationale for the importance of monitoring high-risk AI systems to ensure compliance, performance, and responsible usage.",
        "evidence": "Rationale Explanation: Monitoring Guidelines: A document outlining the guidelines and procedures for monitoring high-risk AI systems, including monitoring criteria, frequency, and reporting mechanisms. \n\nRationale for Monitoring: A report explaining the reasons for monitoring high-risk AI systems, such as ensuring compliance, performance, and responsible usage. \n\nMonitoring Records: Records of monitoring activities for high-risk AI systems documenting compliance findings, performance data, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of monitoring high-risk AI systems for ensuring compliance, performance, and responsible usage. | Responsibility Explanation: High-Risk AI Monitoring Plan: A comprehensive plan outlining the procedures and metrics for continuous monitoring of high-risk AI systems. \n\nPerformance and Behavior Logs: Logs documenting the AI systems' performance metrics, behavioral patterns, and any deviations. \n\nImpact Assessment Reports: Periodic reports evaluating the AI systems' impacts on users and society, including any unintended consequences. | Data Explanation: Detailed logs of data inputs and outputs.\n\nData lineage documents detailing the source, evolution, and structure of the data.\n\nData quality reports assessing the accuracy, completeness, and timeliness of the data. | Fairness Explanation: High-Risk AI Monitoring Framework: A framework for monitoring high-risk AI systems, with specific indicators to measure fairness. \n\nPerformance and Fairness Monitoring Reports: Reports that provide insights into the performance and fairness of high-risk AI systems. \n\nImpact Monitoring and Fairness Response Protocols: Protocols that dictate responses to monitoring results, especially in terms of maintaining or restoring fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nHigh-Risk AI Monitoring Strategy: Detailing the approach and frequency of monitoring activities. \n\nPerformance and Behavior Logs: Record real-time data on the AI system’s operations.\n \nImpact Assessment Reports: Analyzing the AI system's effects on users and the environment. \n\nRisk Mitigation Plans: Outlining the steps to be taken when issues are identified. | Impact Explanation: Monitoring Plan: A documented plan outlining the monitoring strategies, key performance indicators (KPIs), and frequency of assessments for high-risk AI systems. \n\nMonitoring Reports: Regular reports summarizing the outcomes of monitoring activities, highlighting any deviations or issues."
    },
    {
        "control_name": "Monitoring of AI Conformity Declarations",
        "category": "AI Model Health Monitoring",
        "description": "Establish a system to monitor and verify the presence of conformity declarations for high-risk AI systems, ensuring they are consistently documented.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: CA-1: Assessment, Authorization, and Monitoring Policies and Procedures\nCA-7: Continuous Monitoring\nIR-5: Incident Monitoring\nMA-3: Maintenance Tools\nPE-20: Asset Monitoring and Tracking\nPM-6: Measures of Performance\nPM-14: Testing, Training, and Monitoring\nPM-31: Continuous Monitoring Strategy\nRA-5: Vulnerability Monitoring and Scanning\nSI-4: System Monitoring\nSR-4: Provenance\n, SCF: CFG-02: System Hardening Through Baseline Configurations - Ensuring systems are configured to support the monitoring and verification of AI conformity declarations.\nMON-01: Continuous Monitoring - Continuously monitoring AI systems to ensure the presence and compliance of conformity declarations.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assigning specific roles for monitoring and verifying AI conformity declarations.\nDCH-05: Cybersecurity & Data Privacy Attributes - Documenting and maintaining AI conformity declarations as part of data privacy attributes.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including conformity declaration monitoring in the management of AI projects.\nRSK-04: Risk Assessment - Assessing risk associated with non-compliance or missing AI conformity declarations.\nTDA-04: Documentation Requirements - Requiring detailed documentation that includes AI conformity declarations for high-risk AI systems.",
        "explainability": "The Monitoring of AI Conformity Declarations control describes the practice of ongoing monitoring and oversight of AI systems' conformity declarations, which specify the extent to which the systems meet specific standards, regulations, or requirements. It provides a rationale for the importance of monitoring AI conformity declarations to ensure accuracy, compliance, and alignment with stated standards.",
        "evidence": "Rationale Explanation: Monitoring Guidelines: A document outlining the guidelines and procedures for monitoring AI conformity declarations, including declaration review criteria, frequency, and reporting mechanisms. \n\nRationale for Monitoring: A report explaining the reasons for monitoring AI conformity declarations, such as ensuring accuracy, compliance, and alignment with stated standards. \n\nMonitoring Records: Records of monitoring activities for AI conformity declarations documenting accuracy assessments, compliance findings, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of monitoring AI conformity declarations for ensuring accuracy, compliance, and alignment with standards. | Responsibility Explanation: Conformity Declaration Monitoring System: A system or procedure for tracking and verifying the existence and accuracy of conformity declarations. \n\nDeclaration Compliance Records: Records that document the presence and compliance status of conformity declarations for each high-risk AI system. \n\nDeclaration Verification Reports: Reports detailing the findings from the verification of conformity declarations. | Data Explanation: Conformity declaration documents that certify compliance with legal and industry standards.\n\nVersion-controlled documentation repositories ensuring traceability of declarations over time. | Fairness Explanation: Conformity Declaration Monitoring Protocol: A protocol that ensures conformity declarations for AI systems are monitored with fairness considerations. \n\nDeclaration Verification and Fairness Compliance Checklist: A checklist that ensures declarations are verified for accuracy and fairness. \n\nConformity and Fairness Monitoring Logs: Logs that record the monitoring of conformity declarations and any fairness-related findings. | Safety & Performance  Explanation: Deliverables for this control include: \n\nConformity Declaration Registry: A centralized database of all declarations pertaining to AI systems. \n\nCompliance Verification Reports: Detail the findings from conformity declaration inspections. \n\nDocumentation Update Logs: Track changes to the declarations over time. \n\nNonconformance Records Document any deviations from compliance standards. | Impact Explanation: Conformity Declaration Records: A repository of conformity declarations made by AI systems, specifying the standards, regulations, and ethical guidelines. \n\nMonitoring Reports: Periodic reports summarizing the outcomes of the monitoring activities, highlighting any deviations or issues related to conformity declarations."
    },
    {
        "control_name": "AI Risk Transition Monitoring",
        "category": "AI Model Health Monitoring",
        "description": "Establish monitoring mechanisms to track and audit AI systems that transition between risk classifications, especially from non-high risk to high risk.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 9: Risk Management System, NIST 800-53: CA-7: Continuous Monitoring\nCM-3: Configuration Change Control\nCM-4: Impact Analysis\nCM-6: Configuration Settings\nCM-8: System Component Inventory\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: CHG-03: Security Impact Analysis for Changes - Analyzing the security impact of AI systems transitioning between different risk levels.\nCFG-02: System Hardening Through Baseline Configurations - Configuring systems to effectively track and monitor AI risk transitions.\nMON-01: Continuous Monitoring - Continuous monitoring of AI systems to track their transition across different risk classifications.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assigning roles responsible for monitoring AI system risk transitions.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including risk transition monitoring in the management of AI projects.\nRSK-04: Risk Assessment - Conducting risk assessments to evaluate the implications of an AI system's transition from non-high-risk to high-risk.\nRSK-11: Risk Monitoring - Continually monitoring the risk associated with AI systems, particularly during classification changes.\nTDA-04: Documentation Requirements - Ensuring thorough documentation during risk classification transitions of AI systems.",
        "explainability": "The AI Risk Transition Monitoring control describes the process of monitoring and assessing changes in AI system risk over time, especially during transitions such as updates, upgrades, or shifts in usage. It provides a rationale for the importance of AI risk transition monitoring in ensuring risk awareness, mitigation, and proactive management.",
        "evidence": "Rationale Explanation: Risk Transition Monitoring Plan: A document outlining the plan and procedures for monitoring AI system risk transitions, including transition triggers, monitoring criteria, and reporting mechanisms. \n\nRationale for Monitoring: A report explaining the reasons for monitoring AI risk transitions, such as ensuring risk awareness, mitigation, and proactive risk management. \n\nMonitoring Records: Records of risk transition monitoring activities documenting transition triggers, risk assessments, mitigation actions, and outcomes. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI risk transition monitoring for risk awareness, mitigation, and proactive risk management. | Responsibility Explanation: Risk Transition Monitoring Protocol: A protocol outlining the process for monitoring AI systems transitioning between risk classifications. \n\nTransition Audit Logs: Logs documenting the audits and assessments conducted during the risk transition of AI systems. \n\nRisk Reclassification Reports: Reports detailing the reasons for the reclassification and any implications of the transition. | Data Explanation: Risk classification change logs.\n\nDocumentation of the criteria and datasets used for reassessing risk levels,\n\nReports on the implications of the transition on data handling and usage. | Fairness Explanation: Transition Monitoring Framework: A comprehensive framework to monitor risk classification transitions with fairness as a core consideration. \n\nRisk Transition Audit Reports: Detailed audit reports that track changes in risk classification and assess the implications for fairness. \n\nTransition Fairness Logs: Logs documenting each transition between classifications and the steps taken to ensure fairness is maintained. | Safety & Performance  Explanation: Deliverables for this control include: \n\nTransition Monitoring Policies: Define procedures for tracking risk classification changes. \n\nRisk Classification Audit Reports: Detailing the outcomes of evaluations when an AI system changes risk levels. \n\nUpdated Risk Mitigation Strategies: Reflecting any new risk identified during the transition. \n\nTransition Logs: Record the history and justification for any reclassification of AI system risk levels. | Impact Explanation: Risk Transition Monitoring Plan: A documented plan outlining strategies, key indicators, and timelines for monitoring the risk transition of AI systems. \n\nTransition Risk Reports: Regular reports summarizing the outcomes of risk transition monitoring activities, highlighting any identified risk or issues."
    },
    {
        "control_name": "Deployer Usage Monitoring",
        "category": "Postdeployment Model Monitoring",
        "description": "Implement continuous usage pattern monitoring of deployers when they interact with high-risk AI systems. Ensure they follow provided instructions, and maintain detailed logs of system operations and any anomalies.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AC-6: Least Privilege\nAU-6: Audit Record Review, Analysis, and Reporting\nAU-12: Audit Record Generation\nCM-2: Baseline Configuration\nCM-3: Configuration Change Control\nCM-8: System Component Inventory\nIR-4: Incident Handling\nIR-5: Incident Monitoring\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining situational awareness of deployer interactions with AI systems to identify usage anomalies.\nCFG-02: System Hardening Through Baseline Configurations - Ensuring systems are configured to support effective monitoring of AI deployer usage.\nMON-01: Continuous Monitoring - Continuously monitoring deployer interactions with high-risk AI systems to detect usage patterns and deviations.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Designating personnel responsible for monitoring AI system usage by deployers.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating deployer usage monitoring into AI system project management practices.\nRSK-11: Risk Monitoring - Monitoring risk associated with inappropriate or unauthorized usage of AI systems by deployers.",
        "explainability": "The Deployer Usage Monitoring control describes the practice of monitoring and assessing the usage of AI models by deployers or users. It provides a rationale for the importance of deployer usage monitoring in ensuring responsible usage, compliance, and transparency of AI systems.",
        "evidence": "Rationale Explanation: Usage Monitoring Guidelines: A document outlining the guidelines and procedures for monitoring deployer usage of AI models, including usage criteria, monitoring methods, and reporting mechanisms.\n \nRationale for Monitoring: A report explaining the reasons for monitoring deployer usage, such as ensuring responsible usage, compliance, and transparency of AI systems. \n\nUsage Monitoring Records: Records of deployer usage monitoring activities documenting usage patterns, compliance assessments, and any corrective actions taken.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of deployer usage monitoring for responsible usage, compliance, and transparency of AI systems. | Responsibility Explanation: Deployer Usage Monitoring Protocol: A protocol that defines how deployers' interactions with high-risk AI systems will be monitored. \n\nUsage Pattern Logs: Logs capturing detailed information on how deployers use the AI systems, including any deviations from provided instructions. \n\nAnomaly Detection Reports: Reports documenting any unusual or noncompliant usage patterns identified through monitoring. | Data Explanation: Comprehensive usage logs that capture deployer interactions.\n\nAnomaly detection reports highlighting deviations from expected behavior.\n\nOperational guidelines and instructions provided to deployers. | Fairness Explanation: Deployer Interaction Fairness Protocol: A protocol detailing fair usage patterns for deployers of high-risk AI systems. \n\nUsage Pattern Fairness Analysis Reports: Reports analyzing deployer interactions with AI systems for adherence to fairness guidelines. \n\nAnomaly and Fairness Monitoring Logs: Logs that record system operations and any anomalies, with an emphasis on the fairness of responses to such anomalies. | Safety & Performance  Explanation: Deliverables for this control include: \n\nUsage Policy and Guidelines: For deployers interacting with AI systems. \n\nOperational Logs: Capture detailed information on system usage, user activities, and any exceptions.\n \nAnomaly Detection Reports: Documenting unexpected usage patterns or system responses. \n\nInstruction Adherence Analysis: Detailing the compliance of deployers with the established usage guidelines. | Impact Explanation: Deployer Usage Monitoring Plan: A documented plan outlining the parameters, key indicators, and frequency of monitoring activities related to deployer usage. \n\nUsage Monitoring Reports: Periodic reports summarizing the outcomes of deployer usage monitoring, highlighting any deviations or concerns."
    },
    {
        "control_name": "Postmarket Monitoring",
        "category": "Postdeployment Model Monitoring",
        "description": "Implement a continuous monitoring mechanism for high-risk AI systems postmarket release. This should include periodic system performance reviews, anomaly detection, and corrective action plans.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: CA-7: Continuous Monitoring\nCM-4: Impact Analysis\nCP-9: System Backup\nIR-4: Incident Handling\nIR-5: Incident Monitoring\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSR-2: Supply Chain Risk Management Plan\nSI-4: System Monitoring\nSI-5: Security Alerts, Advisories, and Directives, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining situational awareness of high-risk AI systems' performance and operational environment.\nCHG-03: Security Impact Analysis for Changes - Conducting security impact analyses for any changes or updates made to AI systems post-market.\nCFG-02: System Hardening Through Baseline Configurations - Configuring systems to facilitate effective post-market monitoring of AI systems.\nMON-01: Continuous Monitoring - Continuously monitoring high-risk AI systems post-market to assess performance and detect anomalies.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assigning specific responsibilities for monitoring and managing high-risk AI systems post-market.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Including post-market monitoring in the management and oversight of AI projects.\nRSK-11: Risk Monitoring - Regularly monitoring risk associated with high-risk AI systems post-market, identifying potential issues.",
        "explainability": "The Postmarket Monitoring control describes the practice of continuous monitoring and assessment of AI systems after they have been deployed in the market. It provides a rationale for the importance of post-market monitoring in ensuring ongoing performance and compliance and addressing issues or risk that may arise in real-world usage.",
        "evidence": "Rationale Explanation: Post-Market Postmarket Monitoring Plan: A document outlining the plan and procedures for monitoring AI systems after deployment, including monitoring criteria, reporting mechanisms, and response protocols. \n\nRationale for Monitoring: A report explaining the reasons for post-market postmarket monitoring, such as ensuring ongoing performance, compliance, and timely issue resolution. \n\nMonitoring Records: Records of post-market postmarket monitoring activities documenting performance metrics, compliance assessments, issue reports, and actions taken to address issues. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of post-market postmarket monitoring for ongoing performance, compliance, and issue resolution of AI systems. | Responsibility Explanation: Postmarket Monitoring Plan: A plan outlining the approach for continuous monitoring of AI systems after market release. \n\nSystem Performance Reviews: Periodic reviews assessing the AI systems' performance against expected standards and objectives. \n\nAnomaly and Corrective Action Logs: Logs documenting any anomalies detected post market and the corrective actions taken in response. | Data Explanation: Performance review reports documenting system effectiveness and efficiency postdeployment.\n\nAnomaly logs capturing deviations from normal operation.\n\nAction plans outlining steps for correcting identified issues. | Fairness Explanation: Postmarket Fairness Monitoring Protocol: A protocol that outlines continuous monitoring practices for fairness postmarket release. \n\nPeriodic System Fairness Review Guidelines: Guidelines for conducting periodic reviews of system performance with a focus on fairness. \n\nAnomaly Response and Fairness Correction Plans: Plans that detail corrective actions for anomalies, ensuring they are addressed with fairness in mind. | Safety & Performance  Explanation: Deliverables for this control include:  \n\nPostmarket Monitoring Plan: Detailing the monitoring process and intervals. \n\nPerformance Review Reports: Periodically evaluate the AI system against expected benchmarks. \n\nAnomaly Detection Logs: Record any operational deviations or unexpected behaviors. \n\nCorrective Action Protocols: Outlining procedures for addressing identified issues. | Impact Explanation: Postmarket Monitoring Plan: A documented plan outlining the strategies, key performance indicators (KPIs), and timelines for monitoring AI systems in the postmarket phase. \n\nMonitoring Reports: Regular reports summarizing the outcomes of postmarket monitoring activities, highlighting any identified issues or risk."
    },
    {
        "control_name": "Model Inversion Protection",
        "category": "AI Model Feedback Security",
        "description": "Secure models against inversion attacks to protect the privacy of training data.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAU-9: Protection of Audit Information\nCM-6: Configuration Settings\nMP-6: Media Sanitization\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSI-7: Software, Firmware, and Information Integrity, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintaining awareness of threats like model inversion in the AI landscape.\nCFG-02: System Hardening Through Baseline Configurations - Hardening AI systems to be resilient against inversion attacks.\nMON-01: Continuous Monitoring - Continuously monitoring AI systems for signs of model inversion attacks.\nCRY-01: Use of Cryptographic Controls - Employing cryptographic techniques to secure AI models and training data.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Designating roles responsible for ensuring model inversion protection.\nDCH-01: Data Protection - Implementing data protection measures to safeguard training data against model inversion attacks.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporating model inversion protection strategies in the management of AI projects.\nRSK-04: Risk Assessment - Assessing the risk of model inversion attacks and their impact on data privacy.",
        "explainability": "The Model Inversion Protection control describes the measures and safeguards in place to protect against model inversion attacks, where attackers attempt to infer sensitive data from the AI model's responses. It provides a rationale for the importance of model inversion protection in safeguarding user privacy and preventing data exposure.",
        "evidence": "Rationale Explanation: Model Inversion Protection Guidelines: A document outlining the guidelines and procedures for protecting against model inversion attacks, including security measures, detection methods, and response protocols. \n\nRationale for Protection: A report explaining the reasons for implementing model inversion protection, such as safeguarding user privacy and preventing data exposure. \n\nProtection Records: Records of protection measures and responses to model inversion attempts documenting security incidents, responses, and any improvements or adjustments made. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of model inversion protection for safeguarding user privacy and preventing data exposure. | Responsibility Explanation: Inversion Attack Protection Strategy: A document outlining methods and techniques to secure models against inversion attacks. \n\nPrivacy Protection Audit Reports: Reports from audits that assess the effectiveness of the implemented inversion protection strategies.\n \nTraining Data Privacy Logs: Logs that record any attempts to access or compromise the privacy of training data. | Data Explanation: Security protocols detailing measures against model inversion.\n\nDocumentation of encryption techniques used to safeguard data.\n\nIncident response plans in the event of attempted data extraction. | Fairness Explanation: Inversion Attack Protection Protocol: A protocol that details methods for securing models against inversion attacks, with privacy and fairness as primary concerns. \n\nPrivacy Protection and Fairness Assurance Plans: Plans that link privacy protection measures to the broader goal of fairness in AI operations.\n \nTraining Data Privacy and Fairness Reports: Reports that evaluate the effectiveness of inversion protection measures in maintaining the privacy and fairness of training data. | Safety & Performance  Explanation: Deliverables for this control include: \n\nModel Inversion Protection Strategy: Outlining defense mechanisms. \n\nPrivacy Impact Assessments: To understand potential model vulnerabilities to inversion attacks. \n\nSecurity Protocols: Detailing the measures taken to protect against inversions. \n\nIncident Response Plan: For potential breaches related to model inversion. | Impact Explanation: Model Inversion Protection Measures: A documented set of measures, algorithms, or techniques implemented to protect against model inversion attacks. \n\nModel Inversion Assessment Report: A report summarizing the effectiveness of the implemented protection measures against model inversion attacks."
    },
    {
        "control_name": "Model Output Monitoring",
        "category": "AI Model Health Monitoring",
        "description": "Monitor model outputs for inconsistencies, biases, or errors that might not immediately trigger error reports.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nCM-6: Configuration Settings\nIR-4: Incident Handling\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity\nSI-10: Information Input Validation, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain situational awareness regarding the performance and output quality of AI models.\nCFG-02: System Hardening Through Baseline Configurations - Harden AI systems to ensure the reliability and accuracy of model outputs.\nMON-01: Continuous Monitoring - Continuously monitor AI model outputs to detect inconsistencies, biases, or errors.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assign specific roles for the monitoring and analysis of AI model outputs.\nDCH-01: Data Protection - Ensure data protection controls are in place to manage biases and protect the integrity of AI model outputs.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Include model output monitoring in AI project management practices.\nRSK-04: Risk Assessment - Assess the risk associated with inconsistencies or biases in AI model outputs.",
        "explainability": "The Model Output Monitoring control describes the practice of continuously monitoring the output and predictions generated by AI models to ensure they are accurate, reliable, and aligned with intended goals. It provides a rationale for the importance of model output monitoring in maintaining quality, reliability, and trust in AI systems.",
        "evidence": "Rationale Explanation: Output Monitoring Guidelines: A document outlining the guidelines and procedures for monitoring model outputs, including monitoring criteria, frequency, and reporting mechanisms. \n\nRationale for Monitoring: A report explaining the reasons for monitoring model outputs, such as ensuring accuracy, reliability, and trustworthiness of AI systems. \n\nMonitoring Records: Records of output monitoring activities documenting output assessments, deviations, corrective actions, and any improvements made to the model. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of model output monitoring for maintaining quality, reliability, and trust in AI systems. | Responsibility Explanation: Model Output Monitoring Protocol: A protocol that defines the process for continuously monitoring model outputs. \n\nOutput Analysis Reports: Reports that detail the findings from monitoring model outputs, highlighting any issues.\n \nError and Bias Log: A log documenting instances of errors or biases detected in model outputs. | Data Explanation: Reports on model performance metrics.\n\nAnalysis documents highlighting output anomalies. \n\nRecords of bias detection and rectification efforts. | Fairness Explanation: Output Monitoring and Fairness Assessment Procedures: Procedures for monitoring model outputs that include assessments for fairness.\n \nBias and Error Detection Fairness Protocols: Protocols that outline the steps for detecting biases or errors in model outputs, with an emphasis on maintaining fairness. \n\nModel Output Fairness Review Checklists: Checklists that guide the review of model outputs for fairness concerns. | Safety & Performance  Explanation: Deliverables for this control include: \n\nModel Output Monitoring Policy: Describing the monitoring procedures and standards. \n\nAnomaly and Bias Detection Reports: Documenting any discovered output irregularities. \n\nOutput Quality Metrics: Defining the parameters for acceptable outputs. \n\nIssue Resolution Logs: Capturing the actions taken in response to detected output problems. | Impact Explanation: Output Monitoring Plan: A documented plan outlining the parameters, key indicators, and frequency of monitoring activities related to model outputs. \n\nMonitoring Reports: Regular reports summarizing the outcomes of model output monitoring activities, highlighting any deviations or issues."
    },
    {
        "control_name": "Real-World Environment Testing",
        "category": "AI Model Benchmarking & Performance Audits",
        "description": "Test machine learning models in various real-world environments and conditions to ensure reliability.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 60: Testing of High-Risk AI Systems in Real World Conditions Outside AI Regulatory Sandboxes, NIST 800-53: CA-2: Control Assessments\nRA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nSR-3: Supply Chain Controls and Processes\nSR-6: Supplier Assessments and Reviews\nSI-14: Non-Persistence\nSI-2: Flaw Remediation\nSI-3: Malicious Code Protection\nSI-4: System Monitoring, SCF: CFG-02: System Hardening Through Baseline Configurations - Configure ML models to handle various real-world environmental conditions.\nMON-01: Continuous Monitoring - Continuously monitor the performance of ML models in real-world environments.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assign roles specifically for overseeing real-world testing of ML models.\nIAC-02: Identification & Authentication for Organizational Users - Ensure that ML models can reliably identify and authenticate users in real-world conditions.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporate real-world environment testing in project management for AI systems.\nRSK-04: Risk Assessment - Assess risk associated with ML models under different real-world scenarios.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include real-world environment tests during the development phase of ML models.",
        "explainability": "The Real-World Environment Testing control describes the practice of subjecting AI systems to testing in real-world environments to assess their performance and behavior under real-life conditions. It provides a rationale for the importance of real-world environment testing in ensuring the readiness, reliability, and robustness of AI systems when deployed in practical scenarios.",
        "evidence": "Rationale Explanation: Testing Plan for Real-World Environments: A document outlining the plan and procedures for testing AI systems in real-world environments, including testing scenarios, data collection methods, and performance evaluation criteria. \n\nRationale for Testing: A report explaining the reasons for conducting real-world environment testing, such as ensuring readiness, reliability, and robustness in practical scenarios. \n\nTesting Records: Records of real-world environment testing activities documenting testing results, observations, and any improvements made to the AI system. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of real-world environment testing for ensuring readiness, reliability, and robustness of AI systems in real-life situations. | Responsibility Explanation: Real-World Testing Plan: A plan outlining how models will be tested in different real-world scenarios. \n\nEnvironment Testing Results: Documentation of the results from testing models in various real-world environments. \n\nTesting Feedback and Improvement Logs: Logs that capture feedback from real-world testing and any subsequent model improvements made. | Data Explanation: Test plans that outline the different environments and conditions for evaluation.\n\nTest results that document model performance in each scenario.\n\nAnalysis reports summarizing findings and recommending improvements. | Fairness Explanation: Environment Testing and Fairness Validation Plan: A plan that describes the process for testing models in real-world environments, ensuring fairness is maintained. \n\nReal-World Fairness Testing Protocols: Protocols that outline how models will be tested in various environments with fairness as a key outcome. \n\nReliability and Fairness Test Reports: Reports documenting the results of real-world tests and their implications for the fairness and reliability of the model. | Safety & Performance  Explanation: Deliverables for this control include:  \n\n Real-World Testing Plan: Specifying the environments and conditions for testing. \n\n Test Result Reports: Summarizing the performance of machine learning models in each scenario.\n \n Reliability Analysis Documents: Evaluating the consistency of the model's performance. \n\n Environmental Impact Assessments: Detailing how the model interacts with and affects different environments. | Impact Explanation: Testing Plan for Real-World Environments: A documented plan outlining the strategies, scenarios, and key performance indicators (KPIs) for testing AI systems in real-world environments. \n\nReal-World Testing Reports: Regular reports summarizing the outcomes of real-world environment testing activities, highlighting any identified issues or areas for improvement."
    },
    {
        "control_name": "Comprehensive Model Testing",
        "category": "AI Model Benchmarking & Performance Audits",
        "description": "Ensure models are tested for both common and edge-case scenarios, including potential adversarial attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: CA-3: Information Exchange\nRA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nSA-15: Development Process, Standards, and Tools\nSC-20: Secure Name / Address Resolution Service (Authoritative Source)\nSC-39: Process Isolation\nSI-3: Malicious Code Protection\nSI-4: System Monitoring\nSI-5: Security Alerts, Advisories, and Directives, SCF: CFG-02: System Hardening Through Baseline Configurations - Configure ML models to handle various real-world environmental conditions.\nMON-01: Continuous Monitoring - Continuously monitor the performance of ML models in real-world environments.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assign roles specifically for overseeing real-world testing of ML models.\nIAC-02: Identification & Authentication for Organizational Users - Ensure that ML models can reliably identify and authenticate users in real-world conditions.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Incorporate real-world environment testing in project management for AI systems.\nRSK-04: Risk Assessment - Assess risk associated with ML models under different real-world scenarios.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include real-world environment tests during the development phase of ML models.",
        "explainability": "The Comprehensive Model Testing control describes the practice of conducting thorough and exhaustive testing of AI models to assess their performance, accuracy, and behavior under a wide range of conditions. It provides a rationale for the importance of comprehensive model testing in ensuring model quality, reliability, and suitability for various use cases.",
        "evidence": "Rationale Explanation: Testing Plan for Model Comprehensive Testing: A document outlining the plan and procedures for conducting comprehensive testing of AI models, including testing scenarios, test cases, performance criteria, and data sources. \n\nRationale for Testing: A report explaining the reasons for conducting comprehensive model testing, such as ensuring model quality, reliability, and suitability for diverse use cases. \n\nTesting Records: Records of comprehensive model testing activities documenting test results, performance metrics, observed behaviors, and any model improvements or adjustments. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of comprehensive model testing for ensuring model quality, reliability, and suitability for various use cases. | Responsibility Explanation: Model Testing Framework: A framework that outlines the comprehensive testing approach, covering common and edge-case scenarios. \n\nTest Scenario Records: Records of each testing scenario, including details on the setup, execution, and outcomes.\n \nAdversarial Attack Test Reports: Reports specifically focusing on the model’s performance under potential adversarial attack scenarios. | Data Explanation: Testing protocols that cover a wide range of scenarios.\n\nIncident reports detailing the model's response to edge cases and adversarial inputs.\n\nTest coverage analysis to ensure all relevant cases have been considered. | Fairness Explanation: Model Testing Fairness Protocol: A detailed protocol guiding comprehensive testing of models for fairness. \n\nEdge-Case Fairness Analysis Report: A report that specifically analyzes model performance in edge cases with fairness considerations. \n\nAdversarial Attack Fairness Testing Results: Results from testing models against adversarial attacks, evaluating the fairness of outcomes. | Safety & Performance  Explanation: Deliverables for this control include: \n\nTesting Protocol Documentation: Encompasses both common and edge-case scenarios. \n\nAdversarial Attack Test Reports: Detailing attempts to compromise model integrity and the outcomes. \n\nScenario Testing Results: Include a comprehensive set of test cases covering various use cases. \n\nTest Coverage Analysis: Ensure all relevant scenarios are adequately tested. | Impact Explanation: Testing Plan for Model Comprehensive Testing: A documented plan outlining the testing strategies, scenarios, and key performance indicators (KPIs) for comprehensive testing of AI models. \n\nComprehensive Testing Reports: Regular reports summarizing the outcomes of comprehensive testing activities, highlighting any identified issues or areas for improvement."
    },
    {
        "control_name": "Resilience to Distributional Shifts",
        "category": "AI Model Health Monitoring",
        "description": "Ensure models can adapt or handle scenarios when they are exposed to data distributions they were not trained on.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: CP-2: Contingency Plan\nIR-4: Incident Handling\nRA-3: Risk Assessment\nSA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nSC-32: System Partitioning\nSI-3: Malicious Code Protection\nSI-4: System Monitoring\nSI-8: Spam Protection, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Stay aware of evolving data patterns and update ML models to adapt to these changes.\nCFG-02: System Hardening Through Baseline Configurations - Harden ML models to improve resilience against unforeseen data distributions.\nMON-01: Continuous Monitoring - Monitor the performance of ML models continuously to detect and respond to distributional shifts.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assign roles for managing and updating ML models to handle new data distributions.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensure project management plans include strategies for ML models to adapt to distributional shifts.\nRSK-04: Risk Assessment - Assess the risk and capabilities of ML models in handling distributional shifts.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include tests for resilience to distributional shifts during the ML model development phase.",
        "explainability": "The Resilience to Distributional Shifts control describes the strategies and measures in place to ensure that AI models remain effective and reliable when faced with distributional shifts in the data they operate on. It provides a rationale for the importance of resilience to distributional shifts to maintain model performance and trustworthiness.",
        "evidence": "Rationale Explanation: Resilience Strategy for Distributional Shifts: A document outlining the strategies and procedures for ensuring resilience to distributional shifts, including monitoring techniques, retraining protocols, and adaptation mechanisms. \n\nRationale for Resilience: A report explaining the reasons for prioritizing resilience to distributional shifts, such as maintaining model performance and trustworthiness in changing data conditions. \n\nResilience Records: Records of resilience strategies and activities documenting instances of distributional shifts, responses, and adaptations made to AI models. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of resilience to distributional shifts for maintaining model performance and trustworthiness. | Responsibility Explanation: Distributional Shift Resilience Plan: A plan outlining how models will be made resilient to distributional shifts. \n\nShift Detection and Response Logs: Logs documenting instances where the model was exposed to different data distributions and how it responded. \n\nAdaptation Strategy Reports: Reports detailing strategies used to enable the model to adapt to new data distributions. | Data Explanation: Strategies for dynamic adaptation to new data distributions.\n\nReports on model behavior under simulated distributional shifts.\n\nDocumentation on model updating and retraining processes. | Fairness Explanation: Distributional Shifts Fairness Plan: A plan to evaluate and ensure model fairness in the face of distributional shifts. \n\nResilience Testing Fairness Guidelines: Guidelines on conducting resilience tests to distributional shifts with an emphasis on fairness. \n\nAdaptation Strategy Fairness Documents: Documents that outline adaptation strategies for models when exposed to new data distributions, focusing on maintaining fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nDistributional Shift Adaptation Strategy: Outlining approaches for handling shifts.\n \nRobustness Testing Reports: Assess model performance with varied data distributions. \n\nModel Retraining Frameworks: Update models in response to new data. \n\nShift Detection Logs: Record instances of distributional shifts and the model's response. | Impact Explanation: Distributional Shift Resilience Plan: A documented plan outlining strategies, scenarios, and key indicators for assessing the resilience of AI models to distributional shifts. \n\nResilience Assessment Reports: Regular reports summarizing the outcomes of resilience assessments, highlighting any identified issues or areas for improvement."
    },
    {
        "control_name": "Model Stealing Prevention",
        "category": "AI Model Feedback Security",
        "description": "Implement measures to prevent adversaries from stealing or copying the machine learning model for malicious use.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAU-9: Protection of Audit Information\nCM-6: Configuration Settings\nMP-6: Media Sanitization\nRA-5: Vulnerability Monitoring and Scanning\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSC-30: Concealment and Misdirection, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain awareness of evolving threats like model stealing in AI systems.\nCFG-02: System Hardening Through Baseline Configurations - Harden ML systems to safeguard against unauthorized access and copying.\nMON-01: Continuous Monitoring - Continuously monitor ML systems for signs of unauthorized access or model stealing.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assign roles specifically responsible for protecting ML models from theft.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensure project management includes safeguards against ML model stealing.\nRSK-04: Risk Assessment - Assess risk associated with ML model stealing and implement appropriate mitigation strategies.\nTDA-01: Technology Development & Acquisition - Develop technologies with built-in protections against model theft.",
        "explainability": "The Model Stealing Prevention control describes the measures and safeguards in place to prevent unauthorized access or theft of AI models, which could compromise intellectual property and data privacy. It provides a rationale for the importance of model stealing prevention in safeguarding intellectual property and protecting sensitive data.",
        "evidence": "Rationale Explanation: Model Stealing Prevention Measures: A document outlining the measures and protocols in place to prevent model stealing, including access controls, encryption, and security policies. \n\nRationale for Prevention: A report explaining the reasons for implementing model stealing prevention, such as safeguarding intellectual property and data privacy. \n\nPrevention Records: Records of prevention measures, security incidents, and responses related to model stealing attempts or breaches. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of model stealing prevention for safeguarding intellectual property and protecting sensitive data. | Responsibility Explanation: Model Security Protocol: A detailed document outlining the security measures and techniques to prevent model theft. \n\nSecurity Implementation Logs: Logs recording the implementation of security measures on the machine learning model. \n\nModel Theft Incident Reports: Reports of any attempted or successful model theft incidents, including actions taken in response. | Data Explanation: Implementation of model hardening techniques.\n\nSecurity protocols and incident reports related to model theft attempts.\n\nAudit trails that record access and usage of model data. | Fairness Explanation: Model Theft Prevention Fairness Procedures: Procedures that prevent model theft while considering the fairness implications of such security measures. \n\nAntitheft Fairness Compliance Certificates: Certificates that validate the model's theft prevention measures for their compliance with fairness standards. \n\nIntellectual Property Protection and Fairness Logs: Logs that detail efforts to protect IP in a manner that also upholds fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nModel Protection Strategy: Outlining the security measures in place. \n\nEncryption Standards Documentation: For model code and data. \n\nAccess Control Logs: Detailing who has access to the model and when. \n\nSecurity Breach Incidence Reports: Documenting any attempts or acts of theft. | Impact Explanation: Model Stealing Prevention Measures: A documented set of measures, algorithms, or techniques implemented to prevent model stealing attacks. \n\nSecurity Assessment Reports: Regular reports summarizing the outcomes of security assessments, highlighting any identified vulnerabilities or areas for improvement."
    },
    {
        "control_name": "Real-Time Model Performance Monitoring",
        "category": "AI Model Health Monitoring",
        "description": "Monitor the performance of machine learning models in real-time to detect anomalies, biases, or potential attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nCM-4: Impact Analysis\nIR-4: Incident Handling\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-2: Flaw Remediation\nSI-3: Malicious Code Protection\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain situational awareness regarding ML model performance.\nCFG-02: System Hardening Through Baseline Configurations - Harden ML systems to improve real-time performance monitoring capabilities.\nMON-01: Continuous Monitoring - Implement continuous monitoring mechanisms to oversee real-time performance of ML models.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assign dedicated personnel for real-time monitoring of ML models.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensure projects involving ML models include real-time monitoring strategies.\nRSK-04: Risk Assessment - Assess risk related to ML model performance, including potential for biases and attacks.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include real-time performance monitoring in ML model testing.",
        "explainability": "The Real-Time Model Performance Monitoring control describes the practice of continuously monitoring the performance of AI models in real time as they operate. It provides a rationale for the importance of real-time model performance monitoring in ensuring that models remain accurate, reliable, and responsive to changing conditions.",
        "evidence": "Rationale Explanation: Real-Time Monitoring Plan: A document outlining the plan and procedures for monitoring the performance of AI models in real-time, including monitoring criteria, alert thresholds, and reporting mechanisms. \n\nRationale for Real-Time Monitoring: A report explaining the reasons for conducting real-time model performance monitoring, such as maintaining accuracy, reliability, and responsiveness to changing conditions. \n\nMonitoring Records: Records of real-time model performance monitoring activities documenting performance metrics, alerts triggered, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of real-time model performance monitoring for maintaining model accuracy, reliability, and responsiveness to changing conditions. | Responsibility Explanation: Performance Monitoring System: A system or set of tools designed for real-time monitoring of the machine learning model’s performance. \n\nPerformance Anomaly Logs: Logs that record any detected performance anomalies or unusual patterns. \n\nMonitoring Reports: Regular reports summarizing the findings from the performance monitoring activities. | Data Explanation: Real-time monitoring dashboards displaying key performance indicators.\n\nAlert systems for anomaly detection.\n\nRegularly updated logs of performance metrics. | Fairness Explanation: Performance Monitoring and Fairness Protocol: A protocol that ensures real-time monitoring of models for continuous fairness. \n\nReal-Time Bias Detection Reports: Reports that focus on detecting biases in real time, assessing their impact on fairness. \n\nAnomaly Response and Fairness Adjustment Plans: Plans that detail immediate responses to anomalies, prioritizing fairness in corrective actions. | Safety & Performance  Explanation: Deliverables for this control include: \n\nReal-Time Monitoring Strategy: Outlines the parameters and processes for ongoing surveillance. \n\nPerformance Dashboards: Display live data on model operations. \n\nAnomaly and Bias Alert Systems: Flag unexpected model outputs. \n\nSecurity Incident Reports: Detailing detected threats or compromises. | Impact Explanation: Real-Time Monitoring System: A system or tool for real-time tracking of key performance indicators (KPIs) and metrics related to AI model performance.\n \nReal-Time Performance Reports: Continuous reports summarizing the real-time performance of AI models, highlighting any deviations or issues."
    },
    {
        "control_name": "Model Output Manipulation",
        "category": "AI Model Feedback Security",
        "description": "Check for vulnerabilities allowing attackers to manipulate the model's output.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-6: Least Privilege\nCM-6: Configuration Settings\nIR-4: Incident Handling\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSC-7: Boundary Protection\nSI-3: Malicious Code Protection\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain awareness of potential vulnerabilities in AI systems that could result in output manipulation.\nCFG-02: System Hardening Through Baseline Configurations - Harden AI systems to protect against output manipulation.\nMON-01: Continuous Monitoring - Implement continuous monitoring to detect and respond to attempts at model output manipulation.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensure that projects involving AI models account for the risk of output manipulation.\nRSK-04: Risk Assessment - Conduct risk assessments to identify and prioritize risk related to model output manipulation.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include tests for output manipulation vulnerabilities during the development of AI models.\nTHR-07: Threat Hunting - Proactively search for threats that could exploit vulnerabilities leading to model output manipulation.\nVPM-02: Vulnerability Remediation Process - Establish processes for identifying and remediating vulnerabilities that could lead to model output manipulation.",
        "explainability": "The Model Output Manipulation control describes the measures and safeguards in place to prevent unauthorized or malicious manipulation of AI model outputs, which could compromise system integrity and trustworthiness. It provides a rationale for the importance of model output manipulation prevention in maintaining the reliability and security of AI systems.",
        "evidence": "Rationale Explanation: Output Manipulation Prevention Measures: A document outlining the measures and protocols in place to prevent model output manipulation, including authentication, access controls, and security policies. \n\nRationale for Prevention: A report explaining the reasons for implementing model output manipulation prevention, such as maintaining system integrity and trustworthiness. \n\nPrevention Records: Records of prevention measures, security incidents, and responses related to model output manipulation attempts or breaches. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of model output manipulation prevention for maintaining system integrity and trustworthiness. | Responsibility Explanation: Output Manipulation Vulnerability Assessment: An assessment to identify potential vulnerabilities in the model that could be exploited to manipulate outputs. \n\nVulnerability Mitigation Plans: Plans detailing the steps to mitigate identified vulnerabilities and protect the model’s outputs. \n\n Manipulation Attempt Logs: Logs recording any attempts to manipulate the model’s outputs, whether successful or thwarted. | Data Explanation: Security assessments of the model's output integrity.\n\nDocumentation of measures in place to prevent output manipulation.\n\nIncident reports on any past manipulation attempts. | Fairness Explanation: Output Manipulation Protection Protocol: A protocol outlining measures to protect model outputs from manipulation using a fairness perspective. \n\nOutput Integrity and Fairness Assurance Plan: A plan to ensure model output integrity, linking such integrity directly to fairness. \n\nManipulation Detection and Fairness Response Strategy: A strategy for detecting output manipulation and responding in a way that upholds fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nOutput Manipulation Risk Assessment: Detailing potential manipulation vectors and associated risk.\n\nSecurity Protocols Document: Specifying measures to protect against output manipulation. \n\nPenetration Test Reports: With findings on model output security. \n\nIncident Response Strategy: Addressing any output manipulation attempts. | Impact Explanation: Output Manipulation Prevention Measures: A documented set of measures, algorithms, or techniques implemented to prevent model output manipulation. \n\nIntegrity Assessment Reports: Regular reports summarizing the outcomes of integrity assessments, highlighting any identified vulnerabilities or areas for improvement."
    },
    {
        "control_name": "Noise Sensitivity Testing",
        "category": "AI Model Benchmarking & Performance Audits",
        "description": "Ensure model robustness against random and adversarial input noise is periodically assessed.",
        "risk_level": "General",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behaviour that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: RA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSA-17: Software, Firmware, and Information Integrity\nSC-7: Boundary Protection\nSC-31: Covert Channel Analysis\nSI-3: Malicious Code Protection\nSI-7: Software, Firmware, and Information Integrity\nSI-16: Transmission of Security and Privacy Attributes, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain awareness of AI system performance in the presence of input noise.\nCFG-02: System Hardening Through Baseline Configurations - Harden AI systems to improve resilience against random and adversarial noise.\nMON-01: Continuous Monitoring - Monitor AI models continuously for signs of susceptibility to input noise.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensure project management processes for AI include noise sensitivity testing.\nRSK-04: Risk Assessment - Conduct risk assessments to evaluate the AI model's resilience to random and adversarial noise.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include noise sensitivity tests in AI model development phases to assess robustness.\nTHR-07: Threat Hunting - Proactively identify potential threats that exploit the model's noise sensitivity.\nVPM-06: Vulnerability Scanning - Use vulnerability scanning tools to detect weaknesses in AI models related to noise sensitivity.",
        "explainability": "The Noise Sensitivity Testing control describes the practice of testing AI models to assess their sensitivity to noise or disturbances in input data. It provides a rationale for the importance of noise sensitivity testing in ensuring that AI models can maintain performance and reliability even in the presence of real-world data variations and uncertainties.",
        "evidence": "Rationale Explanation: Noise Sensitivity Testing Plan: A document outlining the plan and procedures for testing AI models for noise sensitivity, including testing scenarios, noise levels, and performance criteria. \n\nRationale for Testing: A report explaining the reasons for conducting noise sensitivity testing, such as ensuring robustness and reliability in the presence of data variations and uncertainties. \n\nTesting Records: Records of noise sensitivity testing activities documenting test results, sensitivity assessments, and any model adjustments made. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of noise sensitivity testing for maintaining model performance and reliability in real-world conditions. | Responsibility Explanation: Noise Sensitivity Testing Plan: A plan outlining how the model will be tested against different types of noise. \n\nRobustness Test Results: Documentation of the results from testing the model's robustness to noise.\n \nNoise Adaptation Strategies: Strategies developed based on test results to enhance the model's resilience to input noise. | Data Explanation: Test protocols for noise injection and model response analysis.\n\nRobustness reports detailing model performance under various noise conditions.\n\nImprovement plans based on test outcomes. | Fairness Explanation: Noise Sensitivity Fairness Testing Protocol: A protocol for testing models against noise, ensuring that sensitivity or insensitivity does not lead to unfair outcomes. \n\nRobustness Assessment and Fairness Reports: Reports assessing model robustness to noise, focusing on the fairness of outcomes. \n\nAdversarial Noise Fairness Impact Analysis: An analysis of how the model responds to adversarial noise, with a focus on maintaining fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nNoise Sensitivity Test Plan: Describing different noise conditions to test. \n\nRobustness Test Reports: Outlining the model's performance in the face of noise. \n\nAdversarial Noise Simulation Results: Detailing the model's response to intentionally perturbed inputs. \n\nModel Tuning Documentation: For adjustments made to improve noise resistance. | Impact Explanation: Noise Sensitivity Testing Plan: A documented plan outlining testing strategies, scenarios, and key indicators for assessing the sensitivity of AI models to noise. \n\nTesting Reports: Regular reports summarizing the outcomes of noise sensitivity testing, highlighting any identified issues or areas for improvement."
    },
    {
        "control_name": "Confidence Thresholding",
        "category": "AI Model Feedback Security",
        "description": "Implement thresholds so predictions with low confidence require manual review or flagging.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-6: Least Privilege\nAU-2: Event Logging\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-6: Configuration Settings\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-10: Information Input Validation, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain situational awareness regarding the AI model's performance in relation to confidence thresholds.\nCFG-02: System Hardening Through Baseline Configurations - Configure AI systems to incorporate confidence thresholding mechanisms.\nMON-01: Continuous Monitoring - Monitor AI model predictions continuously for adherence to set confidence thresholds.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Include confidence thresholding in AI project management and development strategies.\nRSK-04: Risk Assessment - Evaluate the risk associated with low-confidence predictions and the effectiveness of thresholding measures.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Test AI models for the effectiveness of confidence thresholding during development.\nTHR-07: Threat Hunting - Proactively hunt for threats that could exploit or bypass the confidence thresholding mechanisms.\nVPM-06: Vulnerability Scanning - Use vulnerability scanning to identify weaknesses in implementing confidence thresholds.",
        "explainability": "The Confidence Thresholding control describes the practice of setting confidence thresholds for AI model outputs, beyond which decisions are made, in order to control the model's reliability and ensure that decisions are made with a certain level of confidence. It provides a rationale for the importance of confidence thresholding in maintaining decision quality and managing uncertainty.",
        "evidence": "Rationale Explanation: Confidence Thresholding Guidelines: A document outlining the guidelines and procedures for setting confidence thresholds for AI model outputs, including threshold values, decision criteria, and adjustment mechanisms. \n\nRationale for Thresholding: A report explaining the reasons for implementing confidence thresholding, such as ensuring decision quality and managing uncertainty. \n\nThresholding Records: Records of confidence thresholding activities documenting threshold values, decision outcomes, and any adjustments made to the thresholds. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of confidence thresholding for maintaining decision quality and managing uncertainty. | Responsibility Explanation: Confidence Thresholding Protocol: A document outlining the procedure for setting and managing confidence thresholds in the model. \n\nThreshold Alert Logs: Logs that capture instances where model predictions fall below the set confidence thresholds. \n\nManual Review Records: Records of manual reviews conducted on low-confidence predictions, including outcomes and any subsequent actions. | Data Explanation: Policies defining confidence thresholds for automatic versus manual processing.\n\nLogs of predictions with associated confidence scores.\n\nReports on outcomes of manual reviews of low-confidence predictions. | Fairness Explanation: Confidence Thresholding Fairness Guidelines: Guidelines that establish how confidence thresholds are set and managed to support fairness. \n\nLow Confidence Handling Fairness Protocols: Protocols that dictate the handling of low confidence predictions, ensuring they align with fairness standards. \n\nThreshold Setting and Fairness Review Procedures: Procedures for setting and reviewing confidence thresholds, emphasizing their role in maintaining fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nConfidence Threshold Policy: Detailing the threshold levels and the rationale behind their selection. \n\nImplementation Guide: For integrating confidence thresholds into model prediction workflows. \n\nReview Process Documentation: For handling low-confidence predictions. \n\nAudit Trails: For all predictions that were flagged and the subsequent actions taken. | Impact Explanation: Confidence Thresholding Policy: A documented policy outlining the criteria and thresholds for accepting or rejecting AI model predictions based on confidence levels. \n\nMonitoring Reports: Regular reports summarizing the outcomes of monitoring activities related to confidence thresholds, highlighting any deviations or issues."
    },
    {
        "control_name": "Feature Importance Analysis",
        "category": "AI Model Benchmarking & Performance Audits",
        "description": "Understand and ensure the model's most important features are not manipulated.",
        "risk_level": "General",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behaviour that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: AC-6: Least Privilege\nCM-6: Configuration Settings\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSC-7: Boundary Protection\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain awareness of how the AI model’s important features perform under different conditions.\nCFG-02: System Hardening Through Baseline Configurations - Harden AI systems to protect crucial model features from manipulation.\nMON-01: Continuous Monitoring - Continuously monitor AI models to detect and address any manipulation of key features.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Ensure project management processes include safeguarding critical features of AI models.\nRSK-04: Risk Assessment - Evaluate risk associated with the potential manipulation of important model features.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include feature importance analysis in AI model development and testing.\nTHR-07: Threat Hunting - Proactively hunt for threats that could target or exploit the model's critical features.\nVPM-06: Vulnerability Scanning - Scan for vulnerabilities that could allow manipulation of the model's key features.",
        "explainability": "The Feature Importance Analysis control describes the practice of analyzing and evaluating the importance of features or variables used in AI models to make decisions. It provides a rationale for the importance of feature importance analysis in understanding model behavior, transparency, and potential bias.",
        "evidence": "Rationale Explanation: Feature Importance Analysis Plan: A document outlining the plan and procedures for analyzing feature importance in AI models, including analysis methods, criteria, and reporting mechanisms.\n \nRationale for Analysis: A report explaining the reasons for conducting feature importance analysis, such as understanding model behavior, ensuring transparency, and identifying potential bias. \n\nAnalysis Records: Records of feature importance analysis activities documenting feature importance rankings, insights into model behavior, and any actions taken based on the analysis. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of feature importance analysis for understanding model behavior, ensuring transparency, and identifying potential bias. | Responsibility Explanation: Feature Importance Report: A detailed report that identifies and ranks the importance of various features in the model's decision-making process. \n\nFeature Manipulation Risk Assessment: Assessment of the potential risk of manipulation of important features. \n\nFeature Security Plans: Plans outlining measures to protect key features from manipulation. | Data Explanation: Documentation on feature importance derived from the model.\n\nAnalysis of potential impact if key features are manipulated.\n\nProtection strategies to safeguard critical features. | Fairness Explanation: Feature Importance Fairness Protocol: A document outlining procedures to analyze feature importance with fairness considerations. \n\nImportance Analysis Fairness Report: A detailed report on the model's key features and their influence on fairness. \n\nFeature Manipulation Protection Plan: A plan for protecting important features from manipulation to maintain fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nFeature Importance Report: Ranks features by their influence on model output. \n\nFeature Protection Plan: Outlining methods to prevent manipulation of key features. \n\nFeature Manipulation Test Results: Documenting the system's response to attempts at tampering with important features. \n\nModel Update Logs: Showing any changes made to the model, especially concerning feature weighting. | Impact Explanation: Feature Importance Analysis Report: A documented report summarizing the outcomes of the feature importance analysis, highlighting the significance of each input feature. \n\nFeature Importance Documentation: Documentation outlining the methodology and criteria used for assessing feature importance."
    },
    {
        "control_name": "Model Confidentiality",
        "category": "AI Confidentiality & Information Leakage Prevention",
        "description": "Ensure model's architecture, weights, and hyperparameters are confidential to prevent inversion or extraction attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 78: Confidentiality\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks.\nRecital 152: Emphasizes the need to ensure the confidentiality of information and data., NIST 800-53: AC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAU-9: Protection of Audit Information\nCM-6: Configuration Settings\nMP-4: Media Storage and Transport\nRA-5: Vulnerability Monitoring and Scanning\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSC-30: Concealment and Misdirection, SCF: AAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain awareness of the security status of the AI model’s confidential components.\nCFG-02: System Hardening Through Baseline Configurations - Configure AI systems to safeguard model architecture, weights, and hyperparameters.\nMON-01: Continuous Monitoring - Continuously monitor AI models to detect breaches in confidentiality of model components.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Include strategies to protect AI model confidentiality in project management.\nRSK-04: Risk Assessment - Assess the risk associated with potential confidentiality breaches of the AI model’s architecture and data.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Test AI models during development to ensure the confidentiality of their critical components.\nTHR-07: Threat Hunting - Proactively search for threats that could target the confidential aspects of AI models.\nVPM-06: Vulnerability Scanning - Scan for vulnerabilities that could lead to the exposure of the model's confidential elements.",
        "explainability": "The Model Confidentiality control describes the measures and safeguards in place to protect the confidentiality of AI models, including their architecture, parameters, and training data. It provides a rationale for the importance of model confidentiality in safeguarding intellectual property and sensitive information.",
        "evidence": "Rationale Explanation: Model Confidentiality Measures: A document outlining the measures and protocols in place to protect the confidentiality of AI models, including encryption, access controls, and security policies. \n\nRationale for Confidentiality: A report explaining the reasons for implementing model confidentiality measures, such as safeguarding intellectual property and sensitive information. \n\nConfidentiality Records: Records of confidentiality measures, security incidents, and responses related to unauthorized access or breaches. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of model confidentiality for safeguarding intellectual property and sensitive information. | Responsibility Explanation: Model Confidentiality Protocol: A protocol outlining measures to keep the model's critical components confidential. \n\nSecurity Compliance Records: Records documenting the compliance with confidentiality measures for model components. \n\nConfidentiality Breach Reports: Reports detailing any breaches in model confidentiality and the response actions taken. | Data Explanation: Confidentiality policies and procedures for model components.\n\nAccess control lists and protocols for model data.\n\nEncryption standards applied to model architecture and weights. | Fairness Explanation: Model Confidentiality Fairness Guidelines: Guidelines ensuring the confidentiality of model details in the interest of fairness. \n\nConfidentiality and Fairness Compliance Certificates: Certificates affirming the model's compliance with confidentiality and fairness standards. \n\nModel Protection and Fairness Logs: Logs documenting efforts to protect model confidentiality and their fairness implications. | Safety & Performance  Explanation: Deliverables for this control include: \n\nModel Confidentiality Policy: Outlining the measures to protect sensitive information.\n \nEncryption and Access Control Protocols: Detailing security practices for the model components. \n\nSecurity Audit Reports: Document the findings from confidentiality assessments. \n\nBreach Response Plan: Specifies actions in case of unauthorized access detection. | Impact Explanation: Confidentiality Policy: A documented policy outlining the measures and protocols in place to maintain the confidentiality of AI models. \n\nAccess Logs: Logs recording access to AI models, detailing who accessed them, when, and for what purpose."
    },
    {
        "control_name": "Address Changes in Accuracy & Precision",
        "category": "AI Model Feedback Security",
        "description": "Implement procedures to determine how accountable individuals will address changes due to adversarial attempts or changes in the environment.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: CA-7: Continuous Monitoring\nCM-4: Impact Analysis\nIR-4: Incident Handling\nPL-2: System Security and Privacy Plans\nPM-16: Threat Awareness Program\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: CFG-02: System Hardening Through Baseline Configurations - Harden AI systems to be resilient against changes that could affect model precision and accuracy.\nMON-01: Continuous Monitoring - Implement continuous monitoring of AI models to detect and address accuracy changes in real time.\nIRO-02: Incident Handling - Develop incident response protocols for handling accuracy and precision issues in AI models.\nRSK-04: Risk Assessment - Perform risk assessments to identify and mitigate factors that could adversely impact model accuracy and precision.\nOPS-01: Operations Security - Include model accuracy and precision maintenance in the operational security procedures.\nTHR-04: Insider Threat Program - Establish a program to monitor and respond to insider threats that might affect AI model accuracy and precision.\nTHR-07: Threat Hunting - Proactively search for signs of adversarial attempts affecting model accuracy.\nTHR-10: Threat Analysis - Analyze potential threats that could cause detrimental changes in model accuracy or precision.\nVPM-07: Penetration Testing - Conduct penetration testing to simulate adversarial attempts and assess their impact on AI model accuracy.",
        "explainability": "The Address Changes in Accuracy & Precision control describes the process of continuously monitoring, analyzing, and addressing changes in the accuracy and precision of AI models. It provides a rationale for the importance of addressing these changes to maintain model performance, reliability, and quality.",
        "evidence": "Rationale Explanation: Monitoring Plan for Accuracy and Precision: A document outlining the plan and procedures for monitoring accuracy and precision changes in AI models, including measurement methods, alert thresholds, and reporting mechanisms. \n\nRationale for Monitoring: A report explaining the reasons for monitoring accuracy and precision, such as maintaining model performance, reliability, and quality. \n\nMonitoring Records: Records of monitoring activities documenting accuracy and precision measurements, deviations, and corrective actions taken.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of addressing changes in accuracy and precision for maintaining model performance, reliability, and quality. | Responsibility Explanation: Accuracy Adjustment Protocol: A protocol outlining steps to address changes in model accuracy and precision. \n\nModel Performance Logs: Logs documenting the model's performance over time, highlighting any changes in accuracy and precision.\n \nAdjustment Action Reports: Reports detailing actions taken to address changes in the model's accuracy and precision. | Data Explanation: Action plans and protocols for responding to changes in model performance.\n\nDocumentation of historical accuracy and precision metrics for benchmarking.\n\nReports on identified adversarial attempts or environmental changes affecting performance. | Fairness Explanation: Accuracy Change Fairness Response Plan: A plan detailing how to address changes in model accuracy and precision with fairness as a priority.\n \nEnvironmental Change Fairness Adjustment Guidelines: Guidelines for adjusting model performance in response to environmental changes to maintain fairness. \n\nAdversarial Impact Fairness Analysis: An analysis of how adversarial attempts affect accuracy and precision, focusing on maintaining fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAccuracy and Precision Monitoring Plan: Detailing strategies for tracking performance metrics. \n\nAccountability Assignment Document: Specifying who is responsible for addressing performance changes. \n\nChange Management Logs: Recording all identified changes in model performance and the steps taken in response. \n\nAdversarial and Environmental Impact Reports: Assessing the reasons behind changes in accuracy and precision. | Impact Explanation: Accuracy and Precision Monitoring Plan: A documented plan outlining strategies, key indicators, and timelines for monitoring changes in accuracy and precision. \n\nAccuracy and Precision Reports: Regular reports summarizing the outcomes of monitoring activities, highlighting any deviations or issues related to changes in accuracy and precision."
    },
    {
        "control_name": "Reward Mechanism Review",
        "category": "Performance Management",
        "description": "Review the reward mechanisms in place to prevent unintended behaviors or reward hacking.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 56: Codes of Practice, NIST 800-53: AC-6: Least Privilege\nAU-6: Audit Record Review, Analysis, and Reporting\nCA-7: Continuous Monitoring\nCM-2: Baseline Configuration\nCM-3: Configuration Change Control\nPM-9: Risk Management Strategy\nRA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring, SCF: CFG-02: System Hardening Through Baseline Configurations - Harden AI models against vulnerabilities in reward mechanisms.\nMON-01: Continuous Monitoring - Implement continuous monitoring to detect unintended behaviors resulting from reward mechanism issues.\nIRO-02: Incident Handling - Prepare for incident response relating to AI model behaviors caused by flawed reward mechanisms.\nPRM-04: Cybersecurity & Data Privacy in Project Management - Integrate performance and reward mechanism review in project management practices for AI models.\nRSK-07: Risk Assessment Update - Regularly update risk assessments to include evaluation of AI model reward mechanisms.\nSAT-03: Role-Based Cybersecurity & Data Privacy Training - Train relevant personnel on identifying and managing issues in AI model reward mechanisms.\nOPS-01: Operations Security - Include the review of reward mechanisms as part of the operations security for AI systems.\nTHR-07: Threat Hunting - Actively search for signs of reward mechanism exploitation in AI models.\nTHR-10: Threat Analysis - Analyze potential threats related to reward mechanism exploitation or hacking in AI models.",
        "explainability": "The Reward Mechanism Review control describes the practice of reviewing and assessing the reward mechanisms used in reinforcement learning and AI systems to ensure that they align with ethical, fair, and desired outcomes. It provides a rationale for the importance of reward mechanism review in to promote responsible and ethical AI systems.",
        "evidence": "Rationale Explanation: Reward Mechanism Review Guidelines: A document outlining the guidelines and procedures for reviewing reward mechanisms in AI systems, including assessment criteria, fairness considerations, and ethical standards. \n\nRationale for Review: A report explaining the reasons for conducting reward mechanism reviews, such as promoting ethical AI, fairness, and responsible behavior. \n\nReview Records: Records of reward mechanism review activities documenting assessment outcomes, recommendations, and any adjustments made to the reward mechanisms. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of reward mechanism reviews for promoting ethical AI, fairness, and responsible behavior. | Responsibility Explanation: Reward System Review Protocol: A protocol for regularly reviewing the reward mechanisms in AI systems.\n \nReward System Audit Reports: Reports from audits examining the reward mechanisms for potential issues. \n\nMechanism Adjustment Records: Records of any adjustments made to the reward systems to address identified issues. | Data Explanation: Assessment reports on the efficacy and safety of reward mechanisms.\n\nRevision histories of reward structures.\n\nIncident logs related to unexpected model behaviors due to reward dynamics. | Fairness Explanation: Reward Mechanism Fairness Review Protocol: A protocol for periodically reviewing the fairness of reward mechanisms. \n\nIncentive System Fairness Analysis Reports: Reports analyzing the fairness of the AI's incentive systems. \n\nReward Hacking Prevention and Fairness Strategy: A strategy to prevent reward hacking, ensuring that it aligns with fairness objectives. | Safety & Performance  Explanation: Deliverables for this control include: \n\nReward System Documentation: Outlining the structure and objectives of the reward mechanisms. \n\nReview Reports on Reward Mechanisms: Summarizing the findings from periodic evaluations. \n\nBehavior Audit Logs: Showing the model's behavior in response to the reward mechanisms. \n\nReward System Adjustment Records: Detailing any modifications made to prevent hacking or unintended behavior. | Impact Explanation: Reward Mechanism Documentation: A documented description of the reward mechanisms employed, including criteria, considerations, and potential impacts on system behavior. \n\nReview Reports: Regular reports summarizing the outcomes of reward mechanism reviews, highlighting any identified issues or areas for improvement."
    },
    {
        "control_name": "Sandbox Data Nonimpact",
        "category": "AI Sandbox Establishment",
        "description": "Ensure processing of personal data in the sandbox does not affect data subjects or their rights under local law (e.g., EU law, Australian Privacy Principles).",
        "risk_level": "General",
        "framework": "EU AI Law: Article 57: AI Regulatory Sandboxes\nArticle 58: Detailed arrangements for and functioning of AI regulatory sandboxes\nArticle 59: Further Processing of Personal Data for Developing Certain AI Systems in the Public Interest in the AI Regulatory Sandbox, NIST 800-53: AC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAU-11: Audit Record Retention\nMP-4: Media Storage\nMP-5: Media Transport \nPL-4: Rules of Behavior\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest, SCF: CPL-03: Cybersecurity & Data Privacy Assessments - Conduct regular assessments of AI sandbox activities for data privacy compliance.\nCFG-08: Sensitive / Regulated Data Access Enforcement - Enforce strict access controls for sensitive and personal data processed in AI sandboxes.\nMON-01: Continuous Monitoring - Continuously monitor AI sandbox operations to ensure data subject rights are not impacted.\nDCH-01: Data Protection - Implement data protection measures in AI sandbox environments to ensure no impact on data subjects.\nDCH-06: Media Storage - Securely store data used in AI sandboxes to prevent unauthorized access or impacts.\nPRI-01: Data Privacy Program - Develop a data privacy program that includes provisions for sandbox data processing and its impact on individuals.\nPRI-05: Personal Data Accuracy & Integrity - Maintain the accuracy and integrity of personal data within AI sandbox environments.\nPRI-07: Information Sharing With Third Parties - Control and monitor data sharing from AI sandboxes with third parties to ensure compliance with data subject rights.\nPRI-10: Data Analytics Bias - Monitor for biases in sandbox data processing that could affect data subjects.\nIAC-20: Access to Sensitive Data - Restrict access to sensitive data within AI sandboxes to minimize potential impacts.",
        "explainability": "The Sandbox Data Nonimpact control describes the practice of using sandbox or simulated data for testing and development purposes to ensure that it does not impact real-world data or individuals. It provides a rationale for the importance of sandbox data nonimpact in protecting real data and individuals' privacy and security.",
        "evidence": "Rationale Explanation: Sandbox Data Usage Guidelines: A document outlining the guidelines and procedures for using sandbox data in AI development and testing, including data protection measures and privacy safeguards. \n\nRationale for Nonimpact Non-Impact: A report explaining the reasons for ensuring sandbox data non-impact, nonimpact, such as protecting real data and individuals' privacy and security. \n\nUsage Records: Records of sandbox data usage activities documenting data protection measures, privacy safeguards, and any actions taken to prevent data impact. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of sandbox data nonimpact non-impact in protecting real data and individuals' privacy and security. | Responsibility Explanation: Data Processing Protocol in Sandbox: A protocol outlining how personal data is processed in the sandbox, ensuring no negative impact on data subjects. \n\nData Subject Rights Compliance Records: Records demonstrating compliance with data subject rights during sandbox data processing. \n\nImpact Assessment Reports: Reports assessing the impact of sandbox data processing on data subjects and their rights. | Data Explanation: Protocols ensuring sandbox environments are isolated and their data is not used for decision making regarding individuals.\n\nData handling policies that comply with EU law regarding personal data.\n\nAudit reports verifying that sandbox activities do not impact data subjects. | Fairness Explanation: Sandbox Data Fairness Policy: A policy that dictates the nonimpact of sandbox data processing on personal fairness. \n\nData Subject Rights Protection Plan: A plan to protect data subjects' rights during sandbox processing. \n\nSandbox Processing Fairness Compliance Reports: Reports that confirm sandbox data processing complies with fairness standards under EU law. | Safety & Performance  Explanation: Deliverables for this control include: \n\nSandbox Environment Policy: Outlines the guidelines for data processing within the sandbox. \n\nData Subject Impact Assessments: Evaluate how sandbox testing may affect personal data rights. \n\nCompliance Reports: For data protection laws, specifically  sandbox operations. \n\nAnonymization and Pseudonymization Protocols: Applied to personal data within the sandbox. | Impact Explanation: Sandbox Data Usage Policy: A documented policy outlining the criteria, protocols, and safeguards in place to prevent the impact of sandbox data on real-world data and model biases. \n\nData Impact Assessment Reports: Regular reports summarizing the outcomes of assessments, highlighting any identified issues or areas for improvement related to the impact of sandbox data."
    },
    {
        "control_name": "Monitoring Model Predictions for Anomalies",
        "category": "Postdeployment Model Monitoring",
        "description": "Continuously monitor and analyze the model's predictions for anomalies, comparing them to ground truth data or tracking performance over time to prevent model inversion attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nCA-7: Continuous Monitoring\nCM-4: Impact Analysis\nIR-4: Incident Handling\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: MON-01: Continuous Monitoring - Implement continuous monitoring of AI model outputs to detect and address anomalies promptly.\nMON-01.12: Automated Alerts - Set up automated alerts for any anomalies detected in AI model predictions.\nMON-11: Monitoring for Information Disclosure - Monitor AI model predictions to prevent unauthorized information disclosure and inversion attacks.\nMON-16: Anomalous Behavior - Implement mechanisms to detect and respond to anomalous behavior in AI model predictions.\nIAC-24: Session Lock - Monitor and lock any sessions that produce anomalous AI model predictions, requiring manual review or intervention.\nNET-18: DNS & Content Filtering - Use filtering mechanisms to examine the content and nature of AI model outputs for anomalies.\nSEA-07: Predictable Failure Analysis - Analyze AI model outputs for failure points that could lead to anomalies or inversion attacks.\nOPS-01: Operations Security - Ensure operational security practices include the monitoring of AI models for anomalous predictions.\nTHR-07: Threat Hunting - Proactively search for threats or vulnerabilities in AI models that could cause anomalous predictions.\nVPM-06: Vulnerability Scanning - Regularly scan the AI model and its outputs for vulnerabilities that could lead to inversion attacks.",
        "explainability": "The Monitoring Model Predictions for Anomalies control describes the practice of continuously monitoring AI model predictions to detect and respond to anomalies or unexpected behavior. It provides a rationale for the importance of monitoring model predictions for anomalies to ensure model reliability and trustworthiness.",
        "evidence": "Rationale Explanation: Monitoring Plan for Anomalies: A document outlining the plan and procedures for monitoring AI model predictions for anomalies, including anomaly detection techniques, alert thresholds, and response mechanisms. \n\nRationale for Monitoring: A report explaining the reasons for monitoring model predictions for anomalies, such as ensuring model reliability, trustworthiness, and early anomaly detection.\n \nMonitoring Records: Records of monitoring activities documenting anomaly detections, responses, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of monitoring model predictions for anomalies in ensuring model reliability, trustworthiness, and early anomaly detection. | Responsibility Explanation: Anomaly Detection Protocol: A protocol outlining the methods for monitoring and identifying anomalies in model predictions. \n\nPrediction Monitoring Logs: Logs recording instances of model predictions and any detected anomalies. \n\nAnomaly Analysis Reports: Reports analyzing identified anomalies, their potential causes, and impacts. | Data Explanation: Monitoring systems that flag anomalies in model predictions.\n\nComparative analysis reports between predictions and ground truth data. \n\nTime-series data on model performance to identify trends or irregularities. | Fairness Explanation: Model Prediction Monitoring Protocol: A protocol that focuses on monitoring model predictions for fairness-related anomalies. \n\nAnomaly Detection Fairness Strategy: A strategy for detecting anomalies in model predictions with an emphasis on upholding fairness. \n\nModel Output Fairness Logs: Logs that record any anomalies in model outputs and the subsequent fairness checks performed. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAnomaly Detection Protocol: Defines what constitutes an anomaly and the response procedures.\n\nPrediction Monitoring Reports: Document the comparison between predicted and actual results. \n\nPerformance Trend Analysis: Tracks the model's accuracy and precision over time. \n\nModel Inversion Incident Reports: Detailing any detected or suspected inversion attacks. | Impact Explanation: Anomaly Detection Plan: A documented plan outlining the strategies, key indicators, and timelines for monitoring model predictions for anomalies. \n\nAnomaly Reports: Regular reports summarizing the outcomes of anomaly monitoring activities, highlighting any detected anomalies or issues."
    },
    {
        "control_name": "Monitoring & Updating Training Datasets",
        "category": "Postdeployment Model Monitoring",
        "description": "Regularly monitor and update training datasets to prevent malicious knowledge transfer and ensure the training datasets remain relevant and representative.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: CM-2: Baseline Configuration\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: AST-02.9: Configuration Management Database (CMDB) - Utilize CMDB to track and manage changes in training datasets.\nAST-31.1: Categorize Artificial Intelligence (AI)-Related Technologies - Ensure AI-related technologies are categorized for effective monitoring and updating of training datasets.\nCFG-02: System Hardening Through Baseline Configurations - Apply system hardening practices to secure training datasets against unauthorized changes.\nCFG-02.1: Reviews & Updates - Regularly review and update training datasets to maintain their integrity and relevance.\nMON-01: Continuous Monitoring - Implement continuous monitoring mechanisms for training datasets to detect and address potential malicious activities.\nMON-01.2: Automated Tools for Real-Time Analysis - Use automated tools for real-time analysis of changes in training datasets.\nDCH-06.3: Periodic Scans for Sensitive Data - Conduct periodic scans of training datasets to identify and mitigate risk of malicious knowledge transfer.\nTDA-10: Use of Live Data - Ensure the integrity and security of live data used in training datasets.\nTDA-10.1: Test Data Integrity - Maintain the integrity of test data included in training datasets to prevent malicious knowledge transfer.\nVPM-06: Vulnerability Scanning - Regularly scan training datasets for vulnerabilities that could lead to malicious knowledge transfer.",
        "explainability": "The Monitoring & Updating Training Datasets control describes the practice of continuously monitoring and updating the training datasets used for AI model development to ensure they remain relevant and representative. It provides a rationale for the importance of monitoring and updating training datasets to maintain model accuracy, reliability, and relevance.",
        "evidence": "Rationale Explanation: Monitoring and Updating Plan: A document outlining the plan and procedures for monitoring and updating training datasets, including data sources, update frequency, and quality checks.\n \nRationale for Monitoring: A report explaining the reasons for monitoring and updating training datasets, such as maintaining model accuracy, reliability, and relevance. \n\nMonitoring and Update Records: Records of monitoring and updating activities documenting dataset changes, quality assessments, and any dataset adjustments made. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of monitoring and updating training datasets in maintaining model accuracy, reliability, and relevance. | Responsibility Explanation: Dataset Monitoring and Update Plan: A plan detailing how training datasets will be monitored and periodically updated. \n\nDataset Integrity Logs: Logs documenting the monitoring activities and any updates or changes made to the training datasets. \n\nDataset Update Impact Reports: Reports evaluating the impact of dataset updates on model performance and security. | Data Explanation: Logs of dataset updates and modifications. \n\nChange management documentation for training data. \n\nAudit trails verifying the authenticity and integrity of data sources. | Fairness Explanation: Dataset Monitoring and Update Protocol: A comprehensive guide detailing the regular monitoring and updating of datasets to maintain fairness. \n\nKnowledge Transfer Fairness Assessment: An assessment report evaluating the prevention of malicious knowledge transfer and its implications for fairness.\n \nDataset Update Fairness Logs: Logs that track dataset updates with annotations on their relevance to fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nDataset Monitoring Schedule: Outlining the frequency and methods of dataset reviews. \n\nData Integrity Reports: Summarizing the findings from each monitoring cycle. \n\nDataset Update Logs: Recording all changes made to the training datasets. \n\nKnowledge Transfer Analysis: Assess the impact of new data on the model's learning. | Impact Explanation: Dataset Monitoring Plan: A documented plan outlining the strategies, key indicators, and timelines for monitoring training datasets. \n\nDataset Update Reports: Regular reports summarizing the outcomes of dataset monitoring activities, highlighting any identified issues or areas for dataset updates."
    },
    {
        "control_name": "Testing & Monitoring for Anomalies",
        "category": "Postdeployment Model Monitoring",
        "description": "Regularly test and monitor model behavior for anomalies and unexpected behavior to prevent membership in types of inference attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nCA-7: Continuous Monitoring\nCM-4: Impact Analysis\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-3: Malicious Code Protection\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: MON-01: Continuous Monitoring - Implement continuous monitoring of AI systems to detect anomalies in model behavior.\nMON-01.11: Automated Response to Suspicious Events - Set up automated responses for detected anomalies in AI model behavior.\nMON-01.2: Automated Tools for Real-Time Analysis - Utilize automated tools for real-time analysis and anomaly detection in AI models.\nMON-01.7: File Integrity Monitoring (FIM) - Use FIM to detect unauthorized changes in AI models that could indicate anomalies.\nMON-06: Monitoring Reporting - Regularly report on monitoring activities to identify and analyze anomalies in AI models.\nMON-06.2: Trend Analysis Reporting - Perform trend analysis to understand anomaly patterns in AI models.\nVPM-06: Vulnerability Scanning - Conduct vulnerability scanning to detect weaknesses in AI models that could lead to anomalies.\nVPM-06.5: Review Historical Audit Logs - Review historical audit logs for signs of past anomalies in AI model behavior.\nVPM-07: Penetration Testing - Perform penetration testing to proactively identify potential anomalies in AI models.\nVPM-10: Red Team Exercises - Engage in red team exercises to simulate attacks and detect potential anomalies in AI models.",
        "explainability": "The Testing & Monitoring for Anomalies control involves the practice of systematically testing AI systems and continuously monitoring their behavior to detect and respond to anomalies or unexpected behavior. The rationale for this control is to ensure that AI systems remain reliable, trustworthy, and perform as expected while minimizing risk associated with anomalous behavior.",
        "evidence": "Rationale Explanation: Anomaly Testing and Monitoring Plan: A document outlining the plan for testing and monitoring AI systems for anomalies, including testing scenarios, monitoring criteria, and response procedures. \n\nRationale for Anomaly Testing: A report explaining the reasons for conducting testing and monitoring for anomalies, such as ensuring reliability, trustworthiness, and risk mitigation. \n\nTesting and Monitoring Records: Records of testing and monitoring activities, documenting test results, anomaly detections, responses, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of testing and monitoring for anomalies in ensuring AI system reliability, trustworthiness, and risk mitigation. | Responsibility Explanation: Anomaly Testing and Monitoring Framework: A framework that outlines the approach for regular testing and monitoring of the AI model for anomalies. \n\nBehavioral Anomaly Logs: Logs that capture instances of abnormal model behavior suggestive of potential attacks. \n\nModel Testing Reports: Reports from the regular testing of the model, focusing on detecting behavioral anomalies. | Data Explanation: Test results documenting the presence of anomalies in model behavior. \n\nMonitoring logs that detail the detection of any abnormal activities.\n\nInvestigation reports following anomaly detection, assessing the potential for membership inference risk. | Fairness Explanation: Anomaly Testing and Monitoring Protocol: A protocol for regularly testing and monitoring AI models to identify behaviors that could compromise fairness. \n\nMembership Inference Attack Prevention Plan: A plan outlining specific measures to detect and prevent attacks that could threaten the fairness of model outputs. \n\nBehavior Anomaly Fairness Reports: Reports documenting detected behavioral anomalies and the actions taken to ensure they do not undermine fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAnomaly Testing Protocol: Specifies how to conduct tests for unusual model behavior. \n\nBehavioral Monitoring Reports: Document any identified anomalies in the model’s outputs. \n\nMembership Inference Attack Detection Logs: Record any potential inference attempts. \n\nModel Behavior Correction Plans: Detail the steps to mitigate and address detected anomalies. | Impact Explanation: Anomaly Testing Plan: A documented plan outlining the strategies, scenarios, and key indicators for testing AI systems for anomalies. \n\nAnomaly Monitoring Reports: Regular reports summarizing the outcomes of anomaly monitoring activities, highlighting any identified anomalies or issues."
    },
    {
        "control_name": "AI IP Strategy Development",
        "category": "AI Intellectual Property Rights Management",
        "description": "Develop a comprehensive strategy for intellectual property to manage and protect the intellectual property rights of AI models, algorithms, and datasets.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 53: Obligations for Providers of General Purpose AI Models, NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-4: Information Flow Enforcement\nAC-22: Publicly Accessible Content\nPL-8: Security and Privacy Architecture\nPM-11: Mission/Business Process Definition\nPS-6: Access Agreements\nRA-3: Risk Assessment\nRA-9: Criticality Analysis\nSC-28: Protection of Information at Rest, SCF: AST-01: Asset Governance - Develop governance strategies for AI assets including models, algorithms, and datasets.\nAST-01.1: Asset-Service Dependencies - Identify and manage dependencies between AI assets and services.\nAST-02: Asset Inventories - Maintain inventories of AI intellectual properties such as models and algorithms.\nAST-02.4: Approved Baseline Deviations - Monitor and approve deviations from standard IP handling practices for AI assets.\nAST-03: Asset Ownership Assignment - Assign ownership responsibilities for AI intellectual property assets.\nAST-03.2: Provenance - Track the origin and history of AI intellectual property assets.\nAST-11: Removal of Assets - Manage the removal or transfer of AI intellectual property assets securely.\nAST-12: Use of Personal Devices - Control the use of personal devices for handling AI intellectual properties.\nAST-23: Multi-Function Devices (MFD) - Securely manage devices used for AI IP development or storage.\nAST-31.1: Categorize Artificial Intelligence (AI)-Related Technologies - Categorize AI assets for proper IP management.",
        "explainability": "The AI IP Strategy Development control involves the development of an intellectual property (IP) strategy specifically tailored to AI systems and technologies. The rationale for this control is to protect and manage the intellectual property generated by AI innovations, safeguard innovation efforts, and ensure responsible IP management.",
        "evidence": "Rationale Explanation: AI IP Strategy Document: A comprehensive document outlining the AI IP strategy, including objectives, patent filing processes, data rights management, and IP protection measures. \n\nRationale for AI IP Strategy: A report explaining the reasons for developing an AI-specific IP strategy, such as safeguarding innovations, promoting responsible IP management, and maintaining a competitive edge. \n\nIP Strategy Implementation Records: Records of the implementation of the AI IP strategy documenting patent filings, data rights management, and other IP protection activities. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI IP strategy development for safeguarding innovations, promoting responsible IP management, and maintaining competitiveness. | Responsibility Explanation: AI IP Management Plan: A strategic plan outlining the approach to managing and protecting AI IP.\n\nIP Documentation and Records: Detailed documentation and records of all AI-related IP, including models, algorithms, and datasets. \n\nIP Protection Strategy Reports: Reports detailing the strategy and measures in place for the protection of AI IP. | Data Explanation: IP management policies.\n\nPatent and copyright filings for AI technology.\n\nLicensing agreements and documentation for proprietary datasets. | Fairness Explanation: AI IP Fairness Strategy Document: A document outlining the strategy for AI IP management with fairness considerations. \n\nIP Rights Fairness Guidelines: Guidelines that ensure IP rights management upholds fairness. \n\nStrategy Development Fairness Impact Analysis: An analysis of how the IP strategy impacts fairness within the AI ecosystem. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAI IP Strategy Document: Outlines the approach for IP management and protection. \n\nIP Registration Files: For models, algorithms, and datasets where applicable. \n\nLicensing Agreements and Contracts: Govern the use of AI-related IP. \n\nIP Infringement Monitoring Reports: Detailing any unauthorized use or breaches. | Impact Explanation: AI IP Strategy Document: A documented strategy outlining how IP related to AI systems will be identified, protected, and leveraged. \n\nLegal Compliance Reports: Regular reports summarizing the outcomes of legal compliance assessments related to AI IP."
    },
    {
        "control_name": "AI IP Infringement Monitoring",
        "category": "AI Intellectual Property Rights Management",
        "description": "Implement monitoring systems to detect and respond to unauthorized use or infringement of the organization's AI-related intellectual property.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 25: Responsibilities Along the AI Value Chain, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nCA-7: Continuous Monitoring\nCM-8: System Component Inventory\nPE-3: Physical Access Control\nPS-6: Access Agreements\nRA-5: Vulnerability Monitoring and Scanning\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSI-4: System Monitoring, SCF: AST-01: Asset Governance - Governance policies encompassing monitoring and protection of AI intellectual property.\nAST-02.2: Automated Unauthorized Component Detection - Automated systems for detecting unauthorized use or access to AI-related IP.\nAST-02.8: Data Action Mapping - Mapping and monitoring of actions taken on AI-related intellectual property assets.\nAST-11: Removal of Assets - Procedures for detecting and responding to unauthorized removal or use of AI IP.\nAST-22: Microphones & Web Cameras - Monitoring tools for detecting unauthorized access or use of AI-related IP.\nAST-27: Jump Server - Use of secure intermediary systems to monitor and control access to AI IP assets.\nAST-29: Radio Frequency Identification (RFID) Security - RFID technologies to track and monitor AI IP assets.\nAST-29.1: Contactless Access Control Systems - Use of contactless systems for monitoring access to AI-related IP.\nAST-30: Decommissioning - Monitoring and managing the decommissioning process of AI IP assets to prevent IP infringement.",
        "explainability": "The AI IP Infringement Monitoring control involves the practice of continuously monitoring AI-related intellectual property (IP) to detect and respond to potential infringements. The rationale for this control is to protect AI innovations, enforce IP rights, and mitigate risk associated with IP infringement.",
        "evidence": "Rationale Explanation: IP Infringement Monitoring Plan: A document outlining the plan and procedures for monitoring AI-related IP for potential infringements, including monitoring methods, alert criteria, and response mechanisms. \n\nRationale for Monitoring: A report explaining the reasons for monitoring AI-related IP, such as protecting AI innovations, enforcing IP rights, and risk mitigation. \n\nMonitoring Records: Records of monitoring activities documenting potential infringements, responses, and any legal actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI IP infringement monitoring for protecting AI innovations, enforcing IP rights, and mitigating risk. risk. | Responsibility Explanation: IP Infringement Monitoring System: A system or procedure for detecting unauthorized use or infringement of AI IP. \n\nInfringement Alert Logs: Logs recording instances of potential IP infringement detected by the monitoring system. \n\nInfringement Response Plans: Plans detailing the steps to be taken in response to detected IP infringements. | Data Explanation: IP monitoring tools and alert systems. \n\nProcesses for investigating and documenting IP infringement cases. \n\nLegal action plans to respond to unauthorized use of AI IP. | Fairness Explanation: IP Infringement Monitoring Protocol: A protocol detailing the monitoring for potential IP infringements and their implications for fairness. \n\nUnauthorized Use Response Fairness Procedures: Procedures that dictate how to respond to unauthorized uses, emphasizing fairness. \n\nInfringement Detection Fairness Reports: Reports that describe detected infringements and the fairness of the response strategies. | Safety & Performance  Explanation: Deliverables for this control include: \n\nIP Infringement Monitoring System: Setup and operational guidelines. \n\nInfringement Alert Protocols: Define the response to potential IP breaches. \n\nIP Infringement Reports: Documenting any unauthorized use and actions taken. \n\nLegal Action Plans: In case of confirmed IP violations. | Impact Explanation: IP Infringement Monitoring Plan: A documented plan outlining strategies, key indicators, and timelines for monitoring AI systems for potential IP infringements. \n\nIP Infringement Reports: Regular reports summarizing the outcomes of IP infringement monitoring activities, highlighting any identified infringements or issues."
    },
    {
        "control_name": "AI Model Licensing Guidelines",
        "category": "AI Model Licensing & Open-Source Considerations",
        "description": "Establish guidelines for licensing AI models, including usage rights, restrictions, and compliance with open-source licenses.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: PL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nPS-6: Access Agreements\nRA-9: Criticality Analysis\nSA-22: Unsupported System Components\nPL-8: Security and Privacy Architectures\nRA-1: Risk Assessment Policy and Procedures\nRA-3: Risk Assessment\nSA-10: Developer Configuration Management, SCF: AST-01: Asset Governance - Governance of AI model licensing and ensuring compliance with open source and usage rights.\nAST-02.7: Software Licensing Restrictions - Management and enforcement of software licensing restrictions for AI models.\nAST-04.3: Compliance-Specific Asset Identification - Identifying AI models that require specific licensing or open-source compliance.\nAST-23: Multi-Function Devices (MFD) - Monitoring and controlling multi-function devices to ensure AI model licensing compliance.\nAST-27: Jump Server - Use of intermediary systems to control and monitor licensing compliance for AI models.\nAST-28.1: Database Management System (DBMS) - Database systems to track and manage AI model licenses and compliance.\nAST-31.1: Categorize Artificial Intelligence (AI)-Related Technologies - Categorization of AI models in relation to licensing and open-source requirements.",
        "explainability": "Establish guidelines for licensing AI models and technologies to third parties. The rationale for this control is to define the terms and conditions for AI model usage, protect intellectual property rights, and ensure responsible and ethical model distribution.",
        "evidence": "Rationale Explanation: AI Model Licensing Guidelines Document: A comprehensive document outlining the guidelines for licensing AI models, including licensing terms, usage restrictions, and ethical considerations. \n\nRationale for Licensing Guidelines: A report explaining the reasons for developing AI model licensing guidelines, such as protecting IP rights, defining responsible usage, and ensuring ethical distribution. \n\nLicensing Guidelines Implementation Records: Records of the implementation of the AI model licensing guidelines documenting licensing agreements, usage restrictions, and adherence to ethical principles. \n\nRationale Document: A comprehensive non-technical report justifying the importance of AI Model Licensing Guidelines for protecting IP rights, responsible usage, and ethical distribution. | Responsibility Explanation: AI Model Licensing Framework: A document outlining the guidelines and principles for licensing AI models, including usage rights and restrictions. \n\nLicensing Agreement Templates: Standardized templates for AI model licensing agreements. \n\nOpen-Source Compliance Records: Records ensuring compliance with open-source licenses where applicable. | Data Explanation: Comprehensive licensing policy documents for AI models. \n\nTemplates and examples of licensing agreements. \n\nChecklists for compliance with various open-source licenses. | Fairness Explanation: Licensing Fairness Framework: A framework that provides fair guidelines for AI model licensing. \n\nUsage Rights and Restrictions Fairness Manual: A manual that outlines usage rights and restrictions with a focus on fairness. \n\nLicensing Compliance and Fairness Checklists: Checklists to ensure licensing agreements comply with fairness standards. | Safety & Performance  Explanation: Deliverables for this control include: \n\nLicensing Policy Document: Outlining terms and conditions for AI model usage. \n\nUsage Rights and Restrictions Framework: Detailing what is and is not allowed under the license. \n\nCompliance Checklist for Open-Source Licenses: Ensuring the organization's models do not violate third-party terms. \n\nLicense Agreement Templates: For use with external parties. | Impact Explanation: Licensing Guidelines Document: A documented set of guidelines outlining the terms, conditions, and ethical considerations for licensing AI models.  \n\nCompliance Assessments: Regular reports summarizing the outcomes of compliance assessments related to AI model licensing."
    },
    {
        "control_name": "Open-Source AI Contribution Policies",
        "category": "AI Model Licensing & Open-Source Considerations",
        "description": "Formulate policies for contributing to open-source AI projects, ensuring alignment with organizational objectives and legal compliance.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: PL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nRA-1: Risk Assessment Policy and Procedures\nRA-3: Risk Assessment\nSA-10: Developer Configuration Management\nSA-22: Unsupported System Components\nRA-9: Criticality Analysis\nPL-8: Security and Privacy Architectures\nPS-6: Access Agreements, SCF: AST-01: Asset Governance - Governance policies for managing contributions to open source AI projects, aligning with organizational objectives.\nAST-02.7: Software Licensing Restrictions - Management of licensing restrictions related to open source AI contributions.\nAST-04.3: Compliance-Specific Asset Identification - Identifying AI assets specifically involved in open source contributions for compliance purposes.\nAST-14: Usage Parameters - Defining the parameters for using AI models in open source projects.\nAST-17: Prohibited Equipment & Services - Policies to avoid the use of prohibited equipment or services in open-source AI projects.\nAST-27: Jump Server - Use of intermediary systems to control and monitor open source contributions of AI models.\nAST-31.1: Categorize Artificial Intelligence (AI)-Related Technologies - Categorization of AI models and technologies involved in open source contributions.",
        "explainability": "Formulate policies for contributing to open-source AI projects. The rationale for this control is to promote responsible and ethical AI contributions, define contribution processes, and ensure adherence to open source principles.",
        "evidence": "Rationale Explanation: Open-Source AI Contribution Policies Document: A comprehensive document outlining the policies and guidelines for contributing to open-source AI projects, including contribution processes, ethical considerations, and adherence to open-source principles. \n\nRationale for Contribution Policies: A report explaining the reasons for developing open-source AI contribution policies, such as promoting responsible contributions, defining processes, and upholding open-source values. \n\nContribution Policies Implementation Records: Records of the implementation of the open-source AI contribution policies documenting contributions, compliance with ethical guidelines, and adherence to open-source principles. \n\nRationale Document: A comprehensive nontechnical non-technical report justifying the importance of Open Source AI Contribution Policies for promoting responsible contributions, defining processes, and upholding open-source principles. | Responsibility Explanation: Open-Source Contribution Policy: A policy document that outlines the guidelines and conditions for contributing to open-source AI projects.\n \nContribution Records: Detailed records of the organization’s contributions to open-source AI projects. \n\nPolicy Compliance Reports: Reports assessing adherence to the open-source contribution policies. | Data Explanation: Policy documents outlining the rules and procedures for open-source contributions. \n\nTraining materials for developers on compliance. \n\nDocumentation of contributions made to AI projects. | Fairness Explanation: Open-Source Contribution Fairness Policy: A policy that governs contributions to open-source AI projects with fairness as a primary concern. \n\nContribution Alignment and Fairness Reports: Reports that assess the alignment of open-source contributions with organizational objectives and fairness. \n\nLegal Compliance and Fairness Guidelines for Contributions: Guidelines that ensure open-source contributions comply with legal standards and uphold fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nOpen-Source Contribution Policy Document: Defining the how, when, and why of contributions. \n\nLegal Compliance Framework: Ensure contributions do not infringe on IP laws or violate licenses. \n\nContribution Strategy: Aligns open-source work with the organization's objectives.\n \nAudit Reports of Contributions: Verify adherence to policies and legal standards. | Impact Explanation: Contribution Policies Document: A documented set of policies outlining the guidelines, ethical considerations, and compliance requirements for contributing to open-source AI projects. \n\nCompliance Reports: Regular reports summarizing the outcomes of compliance assessments related to open-source AI contributions."
    },
    {
        "control_name": "Comprehensive Conformity Assessment",
        "category": "Audit, Verification, & System Review",
        "description": "Regularly conduct in-depth conformity assessments for high-risk AI systems to ensure they meet established standards and best practices. This should include technical evaluations, risk assessments, and performance reviews.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: CA-2: Control Assessments\nCA-3: Information Exchange\nCA-7: Continuous Monitoring\nCM-4: Impact Analysis\nRA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSA-15: Development Process, Standards, and Tools\nSI-2: Flaw Remediation, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring high-risk AI systems comply with statutory, regulatory, and contractual requirements.\nCPL-02.1: Internal Audit Function - Conducting internal audits to evaluate the conformity of high-risk AI systems.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Reviewing the functionality of cybersecurity and data privacy controls in high-risk AI systems.\nCPL-04: Audit Activities - Regular audit activities to assess the conformity of high-risk AI systems.\nCFG-02.1: Reviews & Updates - Periodic reviews and updates to ensure high-risk AI systems are in conformity with set standards.\nMON-02.2: Central Review & Analysis - Centralized review and analysis as part of the conformity assessment process.\nIAC-28.3: Identity Evidence Validation & Verification - Validation and verification processes relevant for AI systems’ identity management and access controls.\nIAO-02.4: Security Assessment Report (SAR) - Generating security assessment reports to document conformity assessments of high-risk AI systems.\nIAO-07: Security Authorization - Authorization processes based on conformity assessments for high-risk AI systems.\nRSK-04: Risk Assessment - Conducting risk assessments as part of the conformity evaluation of high-risk AI systems.",
        "explainability": "The Comprehensive Conformity Assessment control involves conducting a thorough evaluation of AI systems to ensure they conform to relevant regulations, standards, and ethical guidelines. The rationale for this control is to assess and certify the AI system's compliance, mitigate legal and ethical risk, and maintain responsible AI deployment.",
        "evidence": "Rationale Explanation: Conformity Assessment Plan: A document outlining the plan and procedures for assessing AI system conformity, including evaluation criteria, compliance checks, and certification processes. \n\nRationale for Conformity Assessment: A report explaining the reasons for conducting a comprehensive conformity assessment, such as legal compliance, risk mitigation, and responsible AI deployment. \n\nAssessment Records: Records of the conformity assessment activities documenting compliance findings, certification outcomes, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Comprehensive Conformity Assessment for legal compliance, risk mitigation, and responsible AI deployment. | Responsibility Explanation: Conformity Assessment Protocol: A comprehensive protocol outlining the process for conducting conformity assessments of high-risk AI systems.\n \nTechnical Evaluation Reports: Reports from technical evaluations of the AI systems. \n\nRisk Assessment and Performance Review Documents: Documents detailing the outcomes of risk assessments and performance reviews of the AI systems. | Data Explanation: Conformity assessment reports that detail compliance with standards. \n\nRisk assessment documentation identifying potential and actual risk. \n\nPerformance review records comparing system outcomes with expected standards. | Fairness Explanation: Conformity Assessment Fairness Checklist: A detailed checklist that aligns conformity assessments with fairness standards. \n\nRisk and Performance Fairness Review Protocol: A protocol for conducting technical evaluations and performance reviews with a focus on fairness. \n\nAI System Conformity and Fairness Reports: Reports documenting the outcomes of conformity assessments and their implications for fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nConformity Assessment Plan: Detailing the scope, methodology, and frequency of evaluations. \n\nTechnical Evaluation Reports: Document the AI system's adherence to technical standards. \n\nRisk Assessment Documents: Outlining potential risk and the measures in place to mitigate them. \n\nPerformance Review Records: Track the AI system's operational effectiveness and efficiency. | Impact Explanation: Conformity Assessment Plan: A documented plan outlining the strategies, criteria, and timelines for assessing AI systems for conformity. \n\nConformity Assessment Reports: Regular reports summarizing the outcomes of conformity assessments, highlighting any identified nonconformities or areas for improvement."
    },
    {
        "control_name": "Multilevel AI Compliance Verification",
        "category": "Audit, Verification, & System Review",
        "description": "Establish a multitiered verification process involving both supervisory and judicial authorities to validate a high-risk AI system's compliance with regulations, standards and ethical guidelines.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AC-6: Least Privilege\nCA-2: Control Assessments\nCA-6: Authorization\nCA-7: Continuous Monitoring\nPM-9: Risk Management Strategy\nRA-3: Risk Assessment\nSA-10: Developer Configuration Management\nSA-15: Development Process, Standards, and Tools\nSA-17: Software, Firmware, and Information Integrity, SCF: CPL-01.1: Non-Compliance Oversight - Oversight for non-compliance issues in high-risk AI systems, potentially involving supervisory authorities.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of cybersecurity and data privacy controls, crucial for validating AI system compliance.\nCPL-03.1: Independent Assessors - Utilizing independent assessors as part of the multi-level compliance verification process.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Functional reviews that could involve supervisory or judicial authorities.\nCPL-05: Legal Assessment of Investigative Inquires - Legal assessments relevant in a judicial review context for high-risk AI systems.\nCPL-05.2: Investigation Access Restrictions - Managing access restrictions during investigations by supervisory and judicial authorities.\nMON-02: Centralized Collection of Security Event Logs - Centralized log collection to support compliance verification by different authorities.\nIAC-28.4: In-Person Validation & Verification - In-person validation and verification that may involve multiple levels of authorities.\nIAO-07: Security Authorization - Involving various authorities in the security authorization process for AI systems.\nRSK-04: Risk Assessment - Conducting risk assessments as part of the multi-level compliance verification process.",
        "explainability": "The Multilevel AI Compliance Verification control involves verifying and ensuring compliance with multiple levels of regulations, standards, and ethical guidelines in the deployment of AI systems. The rationale for this control is to assess and confirm that AI systems adhere to various levels of compliance, mitigating legal and ethical risk, and ensuring responsible and ethical AI deployment.",
        "evidence": "Rationale Explanation: Multi-level Multilevel Compliance Verification Plan: A document outlining the plan and procedures for verifying AI system compliance across multiple levels, including regulatory, ethical, and industry-specific standards.\n \nRationale for Compliance Verification: A report explaining the reasons for conducting multi-level multilevel AI compliance verification, such as addressing legal and ethical concerns, risk mitigation, and ensuring responsible AI deployment. \n\nVerification Records: Records of the compliance verification activities documenting findings, compliance levels, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Multi-level Multilevel AI Compliance Verification for addressing legal and ethical concerns, risk mitigation, and responsible AI deployment. | Responsibility Explanation: Compliance Verification Framework: A framework detailing the multitiered process for verifying AI system compliance. \n\nSupervisory Authority Audit Reports: Reports from audits conducted by supervisory authorities on AI system compliance. \n\nJudicial Review Documents: Documentation from reviews conducted by judicial authorities to verify compliance. | Data Explanation: Documentation of compliance checks at various levels.\n\nReports from supervisory authorities on compliance status. \n\nJudicial review findings on the legal aspects of AI system deployment. | Fairness Explanation: Compliance Verification Fairness Framework: A framework that establishes a multitiered verification process with fairness as a central component. \n\nSupervisory Authority Fairness Checklists: Checklists used by supervisory authorities to validate compliance with fairness considerations. \n\nJudicial Review Fairness Documentation: Documentation that outlines the role of judicial authorities in reviewing AI system compliance with a focus on fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nMultilevel Verification Framework: Documenting the roles and responsibilities at each level of oversight. \n\nSupervisory Authority Review Reports: Summarizing findings and recommendations. \n\nJudicial Review Documentation: Detailing the legal compliance and any court findings (if applicable).\n\nCompliance Certificates or Statements: Formal attestations of the AI system's adherence to regulations. | Impact Explanation: Multilevel Compliance Verification Plan: A documented plan outlining strategies, criteria, and timelines for verifying compliance across multiple levels.\n \nCompliance Verification Reports: Regular reports summarizing the outcomes of compliance verification activities, highlighting adherence or areas needing improvement."
    },
    {
        "control_name": "Routine Compliance Checks for AI Systems",
        "category": "Audit, Verification, & System Review",
        "description": "Implement periodic compliance checks for high-risk AI systems, ensuring they remain compliant with regulatory, ethical, and industry-specific standards throughout their life cycle.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Annex VI: Conformity Assessment Procedure Based on Internal Control, NIST 800-53: CA-2: Control Assessments\nCA-5: Plan of Action and Milestones\nCA-7: Continuous Monitoring\nCM-3: Configuration Change Control\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSR-2: Supply Chain Risk Management Plan\nSR-6: Supplier Assessments and Reviews, SCF: CPL-01.2: Compliance Scope - Defining the scope of compliance to regularly check against for AI systems.\nCPL-03: Cybersecurity & Data Privacy Assessments - Regular assessments to ensure ongoing compliance with cybersecurity and data privacy standards in AI systems.\nCPL-04: Audit Activities - Regular audit activities to ensure that AI systems meet compliance standards.\nMON-01.11: Automated Response to Suspicious Events - Detect and respond to compliance issues in AI systems through automated monitoring of suspicious activities.\nMON-01.12: Automated Alerts - Utilize automated alerts for continuous monitoring and compliance checks of AI systems.\nMON-01.8: Reviews & Updates - Continuous review and update of monitoring strategies to ensure AI systems' compliance.\nMON-02: Centralized Collection of Security Event Logs - Collecting and analyzing security event logs to evaluate AI systems' compliance regularly.\nIAC-15.4: Automated Audit Actions - Automated audits to routinely check compliance of AI systems with established access control policies.\nRSK-07: Risk Assessment Update - Regular updates to risk assessments for AI systems to ensure they adapt to new threats and remain compliant.\nRSK-11: Risk Monitoring - Ongoing monitoring of risk associated with AI systems to ensure continuous compliance.",
        "explainability": "The Routine Compliance Checks for AI Systems control involves the practice of conducting regular and scheduled compliance checks on AI systems to ensure they continue to meet regulatory, ethical, and industry-specific standards. The rationale for this control is to maintain ongoing compliance, mitigate risk, and ensure responsible and ethical AI system operations.",
        "evidence": "Rationale Explanation: Compliance Check Plan: A document outlining the plan and procedures for conducting routine compliance checks on AI systems, including compliance criteria, testing methodologies, and reporting processes. \n\nRationale for Compliance Checks: A report explaining the reasons for performing routine compliance checks, such as maintaining ongoing compliance, risk mitigation, and responsible AI system operations. \n\nCompliance Check Records: Records of the compliance check activities documenting findings, compliance status, and any corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Routine Compliance Checks for ongoing compliance, risk mitigation, and responsible AI system operations. | Responsibility Explanation: AI System Compliance Check Protocol: A protocol outlining the process for routine compliance checks of AI systems. \n\nCompliance Check Records: Records documenting each compliance check conducted, including findings and any actions taken. \n\nOngoing Compliance Reports: Regular reports summarizing the results of the routine compliance checks and the status of AI system adherence. | Data Explanation: Schedules and protocols for routine compliance assessments. \n\nCompliance check reports including any corrective actions taken. \n\nRecords of compliance-related communications with regulatory bodies. | Fairness Explanation: Periodic Compliance Check Protocol: A protocol that details the schedule and scope of routine compliance checks with an emphasis on fairness. \n\nLife Cycle Compliance and Fairness Logs: Logs that record the results of periodic compliance checks and their relevance to fairness. \n\nContinuous Fairness Compliance Briefs: Briefs that provide updates on the AI system's continuous adherence to fairness standards. | Safety & Performance  Explanation: Deliverables for this control include: \n\nCompliance Check Schedule: Outlining the frequency and scope of the checks. \n\nCompliance Checklists: Detailing the specific criteria and standards that the AI systems must meet. \n\nCompliance Reports: Documenting the findings of each check. \n\nCorrective Action Plans:  Addressing any areas of noncompliance identified during the checks. | Impact Explanation: Compliance Check Plan: A documented plan outlining the strategies, criteria, and schedules for routine compliance checks. \n\nCompliance Check Reports: Regular reports summarizing the outcomes of routine compliance checks, highlighting compliance status and any identified issues."
    },
    {
        "control_name": "AI System Conformity Reporting",
        "category": "Audit, Verification, & System Review",
        "description": "Require authorized representatives to maintain and provide detailed logs and information to national competent authorities, demonstrating the conformity of high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nAU-11: Audit Record Retention\nCA-2: Control Assessments\nCA-7: Continuous Monitoring\nCM-3: Configuration Change Control\nCM-11: User-Installed Software\nPL-4: Rules of Behavior\nPM-9: Risk Management Strategy\nSA-4: Acquisition Process, SCF: CPL-01.1: Non-Compliance Oversight - Monitor for any non-compliance issues to be included in conformity reports.\nCPL-01.2: Compliance Scope - Define the scope of compliance for AI systems to ensure relevant information is reported.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of controls to ensure reporting requirements are met.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Review the functionality of AI systems to ensure reporting aligns with national standards.\nCPL-04: Audit Activities - Conduct audit activities that include maintaining and reporting logs and information to demonstrate compliance.\nMON-01.9: Proxy Logging - Utilize proxy logging for additional data in conformity reports.\nMON-02: Centralized Collection of Security Event Logs - Collect security event logs centrally for easy reporting to authorities.\nMON-03: Content of Event Logs - Ensure event logs contain detailed information required for demonstrating AI system conformity.\nMON-03.6: Centralized Management of Planned Audit Record Content - Manage and centralize audit records to efficiently report AI system conformity.\nIAO-02.4: Security Assessment Report (SAR) - Develop SARs that can be used to demonstrate AI system conformity.",
        "explainability": "The AI System Conformity Reporting control involves the practice of regularly generating and providing reports on AI system conformity with relevant regulations, standards, and ethical guidelines. The rationale for this control is to ensure transparency, demonstrate compliance, and promote responsible and ethical AI system operations.",
        "evidence": "Rationale Explanation: Conformity Reporting Process: A document outlining the process for generating and delivering AI system conformity reports, including the frequency of reporting, content, and recipients. \n\nRationale for Conformity Reporting: A report explaining the reasons for conducting AI system conformity reporting, such as ensuring transparency, demonstrating compliance, and promoting responsible and ethical AI system operations. \n\nConformity Reports: Actual conformity reports that detail AI system compliance with regulations, standards, and ethical guidelines. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI System Conformity Reporting for transparency, compliance demonstration, and responsible AI operations. | Responsibility Explanation: Conformity Reporting Guidelines: Guidelines detailing the type of information and logs to be maintained for demonstrating AI system conformity. \n\nConformity Logs and Records: Detailed logs and records documenting the AI system's compliance with the relevant standards. \n\nSubmission Reports: Reports summarizing the information provided to national competent authorities for conformity verification. | Data Explanation: Conformity logs that document compliance over time. \n\nDetailed reports prepared for regulatory authorities. \n\nRecords of interactions and submissions to national oversight agencies. | Fairness Explanation: Conformity Reporting Fairness Procedure: A procedure outlining the requirements for maintaining and providing conformity records with fairness considerations. \n\nConformity Log Fairness Standards: Standards that dictate the fairness criteria to be met in the logs and reports. \n\nCompliance Reporting Fairness Verification Checklist: A checklist for verifying the fairness of the information provided in conformity reports. | Safety & Performance  Explanation: Deliverables for this control include: \n\nConformity Log Templates: Systematically record compliance information.\n \nDetailed Conformity Reports: Compile evidence of the AI system's adherence to regulations. \n\nDocumentation of Interactions: With competent authorities, including any requests for information and the responses provided. \n\nAudit Trail Records: Capture the chronological documentation of the AI system's compliance activities. | Impact Explanation: Conformity Reporting Framework: A documented framework outlining the structure, content, and frequency of AI system conformity reports. \n\nConformity Reports: Regular reports summarizing the conformity status of AI systems, highlighting adherence and any identified nonconformities."
    },
    {
        "control_name": "Whistleblower Protection",
        "category": "Compliance with Principles",
        "description": "Implement and document policies and practices to protect individuals who report unethical, illegal or noncompliant behavior related to the AI system.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 8: Compliance with the Requirements, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nAU-11: Audit Record Retention\nPE-1: Physical and Environmental Protection Policy and Procedures\nPS-3: Personnel Screening\nPS-6: Access Agreements\nPS-8: Personnel Sanctions\nAT-3: Role-Based Training\nPM-13: Security and Privacy Workforce\nPL-4: Rules of Behavior, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring compliance with laws and regulations related to whistleblower protection.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of data privacy controls related to whistleblower reports.\nCPL-05: Legal Assessment of Investigative Inquires - Legal assessment of whistleblower reports and protection measures.\nCPL-05.1: Investigation Request Notifications - Documenting notifications related to whistleblower reports.\nCPL-05.2: Investigation Access Restrictions - Managing access to whistleblower reports to ensure confidentiality.\nMON-01.4: System Generated Alerts - Generating alerts for potential breaches reported by whistleblowers.\nHRS-05.7: Policy Familiarization & Acknowledgement - Ensuring staff are familiar with whistleblower protection policies.\nHRS-07: Personnel Sanctions - Document actions taken against personnel in breach cases reported by whistleblowers.\nHRS-07.1: Workplace Investigations - Conducting investigations based on whistleblower reports and documenting findings.\nHRS-09.2: High-Risk Terminations - Documenting termination procedures in whistleblower cases.",
        "explainability": "The Whistleblower Protection control involves the establishment of policies and practices to protect individuals who report unethical, illegal, or noncompliant behavior related to AI systems. The rationale for this control is to create a safe and anonymous reporting environment, encourage ethical behavior, and ensure accountability within the organization.",
        "evidence": "Rationale Explanation: Whistleblower Protection Policy: A document outlining the policies and procedures for protecting whistleblowers, including reporting channels, confidentiality measures, and protection against retaliation.\n \nRationale for Whistleblower Protection: A report explaining the reasons for implementing whistleblower protection, such as creating a safe reporting environment, fostering ethical behavior, and ensuring accountability. \n\nWhistleblower Reports: Documentation of whistleblower reports and actions taken in response to reported incidents. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Whistleblower Protection for creating a safe reporting environment, encouraging ethical behavior, and ensuring accountability. | Responsibility Explanation: Whistleblower Protection Policy: A policy that outlines the steps taken to protect whistleblowers.\n \nBreach Report Logs: Logs documenting reported breaches, including the actions taken in response.\n \nProtection Measures Documentation: Documentation of the measures implemented to ensure the safety and confidentiality of whistleblowers. | Data Explanation: Records of reported breaches and the company’s responses. \n\nPolicies and procedures that outline the protection afforded to whistleblowers. \n\nDocumentation of whistleblower protection training programs. | Fairness Explanation: Whistleblower Protection Fairness Protocol: A protocol to protect whistleblowers with fairness at the forefront. \n\nBreach Reporting and Fairness Protection Plan: A plan that outlines steps to protect whistleblowers, ensuring their actions in promoting fairness do not result in retaliation. \n\nProtection Measures Fairness Logs: Logs documenting the measures taken to protect whistleblowers and the fairness of those measures. | Safety & Performance  Explanation: Deliverables for this control include: \n\nWhistleblower Policy Document: Outlining the procedures for reporting breaches and the protections offered. \n\nIncident Report Logs: Confidentially record the details of the breach reports. \n\nWhistleblower Protection Steps: Detailing the measures taken to ensure the safety and anonymity of the whistleblower. \n\nAudit Trails: For all actions taken in response to reported breaches. | Impact Explanation: Whistleblower Protection Policy: A documented policy outlining the procedures, safeguards, and legal protections in place for whistleblowers in the context of AI systems. \n\nIncident Reports: Regular reports summarizing the outcomes of whistleblower incidents, highlighting actions taken and any improvements made."
    },
    {
        "control_name": "Robust Redressal Systems",
        "category": "Compliance with Principles",
        "description": "Ensure robust grievance redressal systems for affected persons and ensure that all AI systems have undergone rigorous testing for safety and rights adherence.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 56: Codes of Practice, NIST 800-53: AC-2: Account Management\nCA-2: Control Assessments\nCA-7: Continuous Monitoring\nIR-1: Incident Response Policy and Procedures\nIR-4: Incident Handling\nPL-4: Rules of Behavior\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSI-2: Flaw Remediation, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensuring AI systems' grievance redressal mechanisms comply with relevant laws and regulations.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of controls related to grievance redressal and rights adherence.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Functional review to ensure grievance systems are effectively addressing cybersecurity and privacy concerns.\nCPL-04: Audit Activities - Regular audits of grievance redressal systems to ensure compliance with safety and rights standards.\nMON-01.12: Automated Alerts - Setting up automated alerts for issues in grievance redressal systems.\nIRO-02.5: Correlation with External Organizations - Correlating grievance redressal systems with external standards for safety and rights adherence.\nIRO-04.3: Continuous Incident Response Improvements - Continuous improvement of grievance and redressal systems based on incident response experiences.\nPRM-07: Secure Development Life Cycle (SDLC) Management - Integrating grievance redressal considerations into the AI system's SDLC for safety and rights adherence.\nRSK-04: Risk Assessment - Assessing risk associated with grievance redressal systems in AI deployments.\nTDA-09.5: Application Penetration Testing - Penetration testing of AI systems to ensure they meet safety and rights adherence standards.",
        "explainability": "The Robust Redressal Systems control involves the establishment of mechanisms and procedures to address grievances, complaints, and disputes related to AI systems and their operations. The rationale for this control is to provide a reliable and accessible redressal system for individuals or entities affected by AI system decisions, ensuring accountability and fairness.",
        "evidence": "Rationale Explanation: Redressal System Framework: A document outlining the framework for handling grievances, complaints, and disputes related to AI systems, including processes, escalation procedures, and resolution methods. \n\nRationale for Redressal Systems: A report explaining the reasons for implementing robust redressal systems, such as providing accessible and fair grievance handling, ensuring accountability, and maintaining trust. \n\nRedressal Records: Documentation of grievances, complaints, and disputes, along with actions taken for resolution. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Robust Redressal Systems for providing accessible and fair grievance handling, ensuring accountability, and maintaining trust. | Responsibility Explanation: Grievance Redressal Framework: A framework outlining the grievance redressal process for AI system-related complaints. \n\nTesting and Compliance Reports: Reports detailing the rigorous testing conducted for safety and rights adherence of AI systems. \n\nRedressal Case Logs: Logs of all grievances reported and the outcomes of the redressal process. | Data Explanation: Protocols and procedures for grievance redressal. \n\nSafety and compliance test reports for AI systems. \n\nDocumentation on the resolution of reported grievances. | Fairness Explanation: Grievance Redressal Fairness Framework: A framework that ensures grievance redressal systems are robust and fair. \n\nAffected Person Fairness Response Protocol: A protocol for responding to grievances from affected persons with an emphasis on fairness. \n\nSafety and Rights Adherence Testing Reports: Reports on the rigorous testing of AI systems for safety and adherence to rights, focusing on fairness in outcomes. | Safety & Performance  Explanation: Deliverables for this control include: \n\nGrievance Redressal Policy: Detailing the procedures for lodging and resolving complaints. \n\nTesting and Compliance Records: Demonstrate the AI systems have been rigorously evaluated for safety and rights adherence. \n\nRedressal Process Documentation: Records every step taken in the grievance process. \n\nRemediation Reports: Summarizing the outcomes of grievances and any compensatory actions taken. | Impact Explanation: Redressal System Framework: A documented framework outlining the procedures, channels, and timelines for addressing grievances related to AI systems. \n\nRedressal Reports: Regular reports summarizing the outcomes of redressal activities, highlighting resolutions, and any identified areas for improvement."
    },
    {
        "control_name": "Business Continuity Planning",
        "category": "AI Resilience & Recovery Strategies",
        "description": "Develop a robust business continuity plan (BCP) that addresses potential AI system failures, data breaches, or other disruptions. This plan should include recovery and back-to-normal strategies, communication protocols, and designated response teams.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 77: Emphasizes the need to consider risk to the cyber resilience of an AI system., NIST 800-53: CP-2: Contingency Plan\nCP-4: Contingency Plan Testing\nCP-6: Alternate Storage Site\nCP-7: Alternate Processing Site\nCP-8: Telecommunications Services\nIR-1: Incident Response Policy and Procedures\nIR-4: Incident Handling\nIR-8: Incident Response Plan\nPL-2: System Security and Privacy Plans, SCF: BCD-01: Business Continuity Management System (BCMS) - Developing and maintaining a BCMS that includes AI system disruptions.\nBCD-02: Identify Critical Assets - Identifying critical AI system components in business continuity planning.\nBCD-03: Contingency Training - Training for response to AI system failures or data breaches.\nBCD-04: Contingency Plan Testing & Exercises - Regular testing of business continuity plans with scenarios involving AI system disruptions.\nBCD-05: Contingency Planning & Updates - Regular updates to the business continuity plan based on AI system evolution and risk.\nBCD-06: Alternative Security Measures - Identifying alternative security measures for AI systems in the continuity plan.\nBCD-09.5: Inability to Return to Primary Site - Planning for extended AI system disruptions in business continuity.\nBCD-10: Telecommunications Services Availability - Ensuring communication protocols are robust in AI system disruptions.\nBCD-11.6: Transfer to Alternate Storage Site - Planning for data storage and access during AI system disruptions.\nBCD-12: Information System Recovery & Reconstitution - Recovery strategies for AI systems post-disruption.",
        "explainability": "The Business Continuity Planning control involves the development of strategies and plans to ensure the uninterrupted operation of AI systems and related processes during unforeseen events or disruptions. The rationale for this control is to maintain business operations, minimize downtime, and ensure the reliability and resilience of AI systems.",
        "evidence": "Rationale Explanation: Business Continuity Plan: BCP: A comprehensive document outlining the strategies, procedures, and protocols for ensuring the continuity of AI system operations during disruptions or emergencies. \n\nRationale for Business Continuity Planning: A report explaining the reasons for developing business continuity plans, BCPs, such as maintaining business operations, minimizing downtime, and ensuring AI system reliability and resilience. \n\nBusiness Continuity Records: Documentation of business continuity planning activities, including testing, drills, and responses to disruptions. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Business Continuity Planning for maintaining operations, minimizing downtime, and ensuring reliability and resilience. | Responsibility Explanation: BCP A detailed plan outlining strategies for responding to AI system failures, data breaches, or disruptions. \n\nRecovery Strategy Documentation: Documentation of the strategies and steps for recovery post incident. \n\nCommunication Protocol and Team Roles: Defined communication protocols and roles of designated response teams in the event of a disruption. | Data Explanation: BCPs detailing recovery approaches. \n\nEmergency communication plans to stakeholders. \n\nRosters and responsibilities of response and recovery teams. | Fairness Explanation: Business Continuity and Fairness Planning Document: A document outlining business continuity plans that incorporate fairness considerations. \n\nDisruption Response Fairness Protocols: Protocols that ensure responses to AI system failures or disruptions uphold fairness. \n\nRecovery Strategy Fairness Assessment Reports: Reports assessing the fairness of recovery strategies and communication protocols. | Safety & Performance  Explanation: Deliverables for this control include: \n\nBCP: With detailed recovery strategies and protocols. \n\nCommunication Plan: Outlining how to disseminate information during a disruption. \n\nResponse Team Assignments: Designating specific roles and responsibilities. \n\nDisaster Recovery Drill Records: Showing testing of the BCP and team readiness. | Impact Explanation: BCP: A documented plan outlining strategies, roles, and responsibilities for maintaining the operation of AI systems during disruptions. \n\nIncident Response Reports: Regular reports summarizing the outcomes of incident response activities, highlighting the effectiveness of business continuity measures."
    },
    {
        "control_name": "AI System Resilience Planning",
        "category": "AI Disaster Recovery & Response",
        "description": "Develop and regularly update a disaster recovery plan tailored for AI systems, focusing on minimizing downtime and data loss. This plan should account for the unique dependencies and complexities of AI, such as data pipelines, model retraining procedures, and specialized hardware.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 77: Emphasizes the need to consider risk to the cyber resilience of an AI system., NIST 800-53: CP-2: Contingency Plan\nCP-4: Contingency Plan Testing\nCP-6: Alternate Storage Site\nCP-7: Alternate Processing Site\nCP-9: System Backup\nCP-10: System Recovery and Reconstitution\nIR-4: Incident Handling\nSA-8: Security and Privacy Engineering Principles\nRA-9: Criticality Analysis, SCF: BCD-01.3: Transfer to Alternate Processing / Storage Site - Planning for AI system recovery in alternate locations.\nBCD-02.3: Resume Essential Missions & Business Functions - Ensuring essential AI functions are part of the recovery plan.\nBCD-04.2: Alternate Storage & Processing Sites - Identifying alternate sites for AI data and processing in case of disasters.\nBCD-06: Contingency Planning & Updates - Regular updates to disaster recovery plans for AI systems.\nBCD-09: Alternate Processing Site - Establishing alternate sites for AI processing during recovery.\nBCD-11.6: Transfer to Alternate Storage Site - Preparing for data transfer to secure locations during AI system disruptions.\nBCD-12: Information System Recovery & Reconstitution - Strategies for AI system recovery and reconstitution post-disaster.\nBCD-12.2: Failover Capability - Ensuring AI systems have failover capabilities in disaster scenarios.\nBCD-12.4: Restore Within Time Period - Setting time objectives for AI system restoration after disasters.\nMNT-05.6: Remote Maintenance Comparable Security & Sanitization - Secure remote maintenance of AI systems as part of disaster recovery.",
        "explainability": "The AI System Resilience Planning control involves the development of strategies and plans to ensure the resilience of AI systems in the face of disruptions, attacks, or adverse conditions. The rationale for this control is to enhance the robustness and availability of AI systems, minimize downtime, and maintain reliable and secure operations.",
        "evidence": "Rationale Explanation: Resilience Plan for AI Systems: A comprehensive document outlining the strategies, procedures, and protocols for enhancing the resilience of AI systems during disruptions, attacks, or adverse conditions. \n\nRationale for Resilience Planning: A report explaining the reasons for developing resilience plans for AI systems, such as enhancing robustness, minimizing downtime, and ensuring reliable and secure operations. \n\nResilience Planning Records: Documentation of resilience planning activities, including testing, mitigation measures, and responses to disruptions.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of AI System Resilience Planning for enhancing resilience, minimizing downtime, and ensuring reliable and secure AI system operations. | Responsibility Explanation: AI Disaster Recovery Plan: A specific plan addressing the disaster recovery needs of AI systems, considering their unique dependencies and complexities.\n \nData Pipeline and Retraining Procedures: Detailed procedures for maintaining data pipelines and retraining models post disaster.\n\nSpecialized Hardware and System Recovery Plans: Plans for the quick recovery and restoration of specialized AI hardware and systems. | Data Explanation: Disaster recovery plans specifically for AI operational continuity. \n\nDocumentation on backup procedures for AI data and models. \n\nGuidelines on restoring AI functionality after an incident. | Fairness Explanation: AI Disaster Recovery Fairness Protocol: A protocol that ensures the AI system's disaster recovery plans are updated with fairness considerations in potential disaster scenarios. \n\nResilience Plan Fairness Impact Reports: Reports analyzing the fairness impact of the AI system's resilience plans, including data pipelines and hardware dependencies. \n\nModel Retraining Fairness Procedures: Documentation of procedures for model retraining in disaster recovery, focusing on maintaining fairness. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAI-Specific Disaster Recovery Plan (DRP): Includes recovery strategies for data pipelines, retraining of models, and restoration of specialized hardware. \n\nAI System Dependency Maps: Outline the interconnections and dependencies of the AI system components. \n\nRetraining Protocols: For models post disruption.\n\nHardware and Software Restoration Procedures: Detailing steps for quick recovery. | Impact Explanation: Resilience Planning Document: A documented plan outlining strategies, technologies, and roles for enhancing the resilience of AI systems. \n\nResilience Assessment Reports: Regular reports summarizing the outcomes of resilience assessments, highlighting areas of strength and improvement."
    },
    {
        "control_name": "AI Performance Standards Agreement",
        "category": "AI Service Level Agreements (SLAs)",
        "description": "Craft SLAs specific to AI systems that stipulate performance metrics, such as prediction accuracy, processing time, and availability. These SLAs should consider the dynamic nature of AI systems, ensuring that they remain effective and fair over time and that there are clear protocols for updates and improvements.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 56: Codes of Practice, NIST 800-53: PL-2: System Security and Privacy Plans\nSA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nSR-2: Supply Chain Risk Management Plan\nRA-9: Criticality Analysis\nSA-17: Software, Firmware, and Information Integrity\nCA-1: Assessment, Authorization, and Monitoring\nCM-9: Configuration Management Plan\nPM-16: Threat Awareness Program, SCF: BCD-01.4: Recovery Time / Point Objectives (RTO / RPO) - Setting clear objectives for AI system recovery time and data recovery points in SLAs.\nCAP-01: Capacity & Performance Management - Managing the capacity and performance of AI systems as per SLA stipulations.\nCAP-03: Capacity Planning - Planning for AI system capacity to meet performance standards in SLAs.\nCAP-04: Performance Monitoring - Continuously monitoring AI system performance against SLA metrics.\nNET-14.5: Work From Anywhere (WFA) - Telecommuting Security - Ensuring AI system performance standards are met in remote working environments as per SLAs.\nPRM-02: Cybersecurity & Data Privacy Resource Management - Allocating resources to meet AI SLA requirements.\nRSK-05: Risk Ranking - Assessing and ranking risk associated with not meeting AI SLA performance standards.\nOPS-01.1: Standardized Operating Procedures (SOP) - Developing SOPs for AI systems that align with SLA performance standards.\nTDA-06.2: Threat Modeling - Modeling potential threats that could impact AI performance as stipulated in SLAs.\nVPM-05.3: Time To Remediate / Benchmarks For Corrective Action - Setting benchmarks for corrective actions in AI systems as per SLA terms.",
        "explainability": "The AI Performance Standards Agreement control involves the establishment of agreements or contracts that define the performance standards and expectations for AI systems. The rationale for this control is to set clear performance benchmarks, ensure accountability, and align AI system behavior with desired outcomes.",
        "evidence": "Rationale Explanation: AI Performance Standards Agreement: A formal document that outlines the agreed-upon performance standards and expectations for AI systems, including criteria, benchmarks, and metrics. \n\nRationale for Performance Standards: A report explaining the reasons for defining performance standards for AI systems, such as ensuring clear expectations, accountability, and alignment with desired outcomes. \n\nPerformance Monitoring Records: Documentation of performance monitoring activities, including data, metrics, and compliance with the standards agreement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI Performance Standards Agreements for setting clear performance benchmarks, ensuring accountability, and achieving desired outcomes. | Responsibility Explanation: Detailed AI System Performance SLA: A comprehensive document that includes performance metrics like prediction accuracy, processing time, and system availability. This SLA should also define responsibilities for maintaining and updating the AI system, outlining roles and responsibilities for developers, operators, and oversight committees. \n\nAI System Maintenance and Update Protocol: A document describing the procedures for regularly updating the AI system. This should detail the roles of individuals or teams responsible for implementing updates, monitoring performance, and ensuring ongoing compliance with ethical standards. \n\nAI System Oversight Committee Charter: A charter for a committee responsible for overseeing the AI system's performance and fairness, including their responsibilities and how they will interact with other stakeholders. | Data Explanation: SLA documents outlining performance criteria for AI systems.\n\nPerformance tracking reports and dashboards. \n\nChange management records for AI system updates. | Fairness Explanation: AI SLA Fairness Framework: A framework for crafting SLAs that includes fairness as a core component of AI performance metrics. \n\nPerformance Metric Fairness Evaluation Guides: Guides that establish how performance metrics, such as accuracy and processing time, are evaluated for fairness. \n\nSLA Update and Fairness Documentation: Documentation that outlines the protocols for SLA updates, ensuring they continue to reflect fairness in the system's performance. | Safety & Performance  Explanation: Deliverables for this control include: \n\nAI-Specific SLAs: Detail performance metrics and expectations. \n\nPerformance Tracking Reports: Document compliance with SLA standards. \n\nUpdate and Improvement Protocols: Describing the processes for periodic AI system evaluation and enhancement. \n\nFairness and Effectiveness Reviews: Ensure the AI system continues to act without bias and with equitable outcomes. | Impact Explanation: Performance Standards Agreement: A documented agreement specifying the performance benchmarks, criteria, and quality standards that AI systems must meet. \n\nPerformance Monitoring Reports: Regular reports summarizing the outcomes of performance monitoring activities, highlighting adherence to standards and any identified performance issues."
    },
    {
        "control_name": "Redundancy & Failover Processes",
        "category": "AI Resilience & Recovery Strategies",
        "description": "Establish redundancy in AI systems and define failover processes to maintain service continuity in case of a system failure.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 77: Emphasizes the need to consider risk to the cyber resilience of an AI system., NIST 800-53: CP-2: Contingency Plan\nCP-7: Alternate Processing Site\nCP-10: System Recovery and Reconstitution\nCP-9: System Backup\nSC-24: Fail in Known State\nSC-36: Distributed Processing and Storage\nRA-9: Criticality Analysis\nSA-17: Software, Firmware, and Information Integrity\nCP-12: Safe Mode, SCF: BCD-01.3: Transfer to Alternate Processing / Storage Site - Preparing for AI system failover to alternate sites for continuity.\nBCD-04.2: Alternate Storage & Processing Sites - Utilizing alternate sites for AI system redundancy.\nCFG-02.2: Automated Central Management & Verification - Central management of AI systems to ensure effective redundancy and failover processes.\nIAO-07: Security Authorization - Authorizing security measures including redundancy and failover for AI systems.\nMNT-05.6: Remote Maintenance Comparable Security & Sanitization - Ensuring remote maintenance aligns with AI system redundancy and failover processes.\nNET-06.5: Direct Internet Access Restrictions - Restricting access as part of failover and redundancy strategies for AI systems.\nNET-15.1: Authentication & Encryption - Ensuring secure failover processes in wireless networking of AI systems.\nRSK-06.2: Compensating Countermeasures - Implementing compensating measures in AI systems for redundancy and failover.\nTDA-17.1: Alternate Sources for Continued Support - Considering alternative sources for AI system support as part of redundancy planning.\nVPM-11.7: Redundant Secondary System - Setting up redundant systems for AI as a failover strategy.",
        "explainability": "The Redundancy & Failover Processes control involves implementing redundancy and failover mechanisms to ensure the availability and resilience of AI systems in the event of failures or disruptions. The rationale for this control is to minimize downtime, enhance reliability, and ensure continuous AI system operation.",
        "evidence": "Rationale Explanation: Redundancy and Failover Plan: A document outlining the strategies, processes, and protocols for redundancy and failover to maintain the availability and resilience of AI systems during failures or disruptions. \n\nRationale for Redundancy and Failover: A report explaining the reasons for implementing redundancy and failover processes, such as minimizing downtime, enhancing reliability, and ensuring continuous AI system operation. \n\nRedundancy and Failover Records: Documentation of redundancy and failover activities, including testing, response to failures, and system recovery. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Redundancy and & Failover Processes for minimizing downtime, enhancing reliability, and ensuring continuous AI system operation. | Responsibility Explanation: Redundancy Design Document: Details how redundancy is integrated into the AI system. \n\nFailover Process Manual: Outlines failover protocols and team responsibilities. \n\nSystem Reliability Analysis Report: A comprehensive report analyzing the reliability and effectiveness of the redundancy and failover processes. | Data Explanation: Technical documentation on redundancy architecture. \n\nFailover process guides and protocols. \n\nTesting records demonstrating the effectiveness of failover mechanisms. | Fairness Explanation: Redundancy Planning with Fairness Considerations: Plans for system redundancy that include considerations for maintaining fairness in service access. \n\nFailover Process Fairness Protocols: Protocols for failover processes that ensure continuity in fair service provision. \n\nService Continuity and Fairness Assurance Reports: Reports assuring that redundancy and failover processes are established with fairness in mind. | Safety & Performance  Explanation: Deliverables for this control include: \n\nRedundancy Plan: Outlining backup systems and components for the AI system.\n \nFailover Procedures Document: Detailing the steps to switch operations to the backup systems. \n\nSystem Failure and Recovery Logs: Recording any instances of failure and the subsequent failover to backups. \n\nRegular Testing Reports: Ensures the effectiveness of the failover process. | Impact Explanation: Redundancy and Failover Plan: A documented plan outlining the redundancy architecture, failover processes, and recovery strategies for AI systems. \n\nIncident Response Reports: Regular reports summarizing the outcomes of incident response activities related to redundancy and failover, highlighting effectiveness and any identified areas for improvement."
    },
    {
        "control_name": "Resilience Testing",
        "category": "AI Resilience & Recovery Strategies",
        "description": "Regularly test AI systems to ensure they can recover from failures, through processes such as chaos engineering and disaster simulation exercises.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 77: Emphasizes the need to consider risk to the cyber resilience of an AI system., NIST 800-53: CP-4: Contingency Plan Testing\nCP-10: System Recovery and Reconstitution\nSA-11: Developer Testing and Evaluation\nRA-9: Criticality Analysis\nSI-14: Non-Persistence\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSA-15: Development Process, Standards, and Tools, SCF: BCD-04.3: Continuous Incident Response Improvements - Implementing ongoing improvements in incident response, crucial for AI resilience testing.\nBCD-05: Contingency Planning & Updates - Updating contingency plans as part of AI system resilience testing.\nBCD-06: Alternative Security Measures - Alternative security measures for AI systems in resilience testing.\nIRO-02.3: Dynamic Reconfiguration - AI systems' ability to dynamically reconfigure as part of resilience testing.\nIRO-06: Incident Response Testing - Testing incident response capabilities of AI systems.\nMNT-03.2: Predictive Maintenance - Implementing predictive maintenance as a resilience strategy for AI systems.\nRSK-07: Risk Assessment Update - Updating risk assessments in line with AI systems' resilience testing results.\nTDA-09.4: Malformed Input Testing - Testing AI systems against malformed inputs, a form of resilience testing.\nTDA-09.5: Application Penetration Testing - Conducting penetration testing on AI applications as part of resilience checks.\nVPM-10: Red Team Exercises - Conducting red team exercises as part of resilience testing for AI systems.",
        "explainability": "The practice of conducting testing procedures to assess the resilience and robustness of AI systems in the face of adverse conditions, disruptions, and attacks. The rationale for this control is to identify vulnerabilities, enhance the system's capacity to withstand challenges, and ensure reliable and continuous AI system operation.",
        "evidence": "Rationale Explanation: Resilience Testing Plan: A document outlining the strategies, processes, and protocols for resilience testing of AI systems, including test scenarios, criteria, and methodologies.\n \nRationale for Resilience Testing: A report explaining the reasons for conducting resilience testing, such as identifying vulnerabilities, enhancing system capacity to withstand challenges, and ensuring reliable and continuous AI system operation. \n\nResilience Testing Records: Documentation of resilience testing activities, including test results, identified vulnerabilities, and actions taken for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Resilience Testing for identifying vulnerabilities, enhancing resilience, and ensuring reliable AI system operation. | Responsibility Explanation: Resilience Test Plan: Outlines resilience tests to be conducted. \n\nResilience Test Reports: Reports of test outcomes. \n\nSystem Resilience Enhancement Strategy: A strategic document outlining long-term plans for enhancing system resilience. | Data Explanation: Resilience testing plans and schedules. \n\nReports on chaos engineering sessions and findings. \n\nDisaster simulation exercise logs and recovery action plans. | Fairness Explanation: Resilience Testing Fairness Guidelines: Guidelines that define how resilience testing should be conducted to ensure fairness is considered. \n\nChaos Engineering Fairness Analysis: Analysis that examines the application of chaos engineering to AI systems with a focus on fairness. \n\nDisaster Simulation Fairness Review Procedures: Procedures for reviewing the outcomes of disaster simulations, ensuring fairness in system recovery. | Safety & Performance  Explanation: Deliverables for this control include: \n\nResilience Testing Plan: Specifies how and when the AI systems will be tested. \n\nChaos Engineering Protocols: Describing the methods used to intentionally inject failure scenarios to test system robustness. \n\nDisaster Simulation Exercise Reports: Documenting the outcomes and lessons learned from each test. \n\nSystem Recovery Procedure Manuals: Provide step-by-step instructions for restoring AI system functionality after a failure. | Impact Explanation: Resilience Testing Plan: A documented plan outlining the strategies, scenarios, and criteria for resilience testing of AI systems.\n \nResilience Test Reports: Regular reports summarizing the outcomes of resilience testing activities, highlighting areas of strength and any identified weaknesses."
    },
    {
        "control_name": "Data Privacy Protocol Suite",
        "category": "Data Privacy Impact Assessment",
        "description": "Implement a suite of tools that enforce data encryption, anonymization, and access controls. Train personnel using real-world scenarios and utilize periodic penetration testing to validate compliance.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nRecital 62: Emphasizes ability to access and use high quality datasets.\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks., NIST 800-53: AC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAT-2: Literacy Training and Awareness\nAT-3: Role-Based Training\nCA-8: Penetration Testing\nMP-5: Media Transport\nSC-12: Cryptographic Key Establishment and Management\nSC-28: Protection of Information at Rest\nSC-13: Cryptographic Protection, SCF: CPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Reviews of cybersecurity and data privacy controls, relevant for assessing data privacy protocol suites.\nMON-03.7: Database Logging - Logging database activities as part of access control and monitoring in data privacy protocols.\nCRY-05: Encrypting Data At Rest - Encryption of data at rest, a fundamental aspect of data privacy protocol suites.\nCRY-09.4: Control & Distribution of Cryptographic Keys - Managing cryptographic keys, essential for encryption in data privacy protocol suites.\nDCH-01.3: Sensitive / Regulated Media Records - Handling sensitive data, crucial for data privacy protocols.\nDCH-07.2: Encrypting Data In Storage Media - Encryption protocols for data in storage media, a key component of a data privacy protocol suite.\nIAC-10.2: PKI-Based Authentication - PKI-based authentication as part of access control strategies in data privacy protocols.\nIAC-10.5: Protection of Authenticators - Protection of authenticators aligns with access control measures in data privacy protocols.\nTDA-09.7: Manual Code Review - Manual review of code can include examining data privacy protocols and tools.\nVPM-09.4: Penetration Testing - Penetration testing to validate compliance and effectiveness of data privacy protocols.",
        "explainability": "The Data Privacy Protocol Suite control involves the implementation of a suite of protocols and procedures to protect the privacy and confidentiality of data used by AI systems. The rationale for this control is to ensure compliance with data privacy regulations, maintain data security, and safeguard sensitive information.",
        "evidence": "Rationale Explanation: Data Privacy Protocol Suite: A collection of documents and procedures outlining data privacy protocols, encryption methods, access controls, and other measures to protect data privacy and confidentiality. \n\nRationale for Data Privacy Protocols: A report explaining the reasons for implementing data privacy protocols, such as ensuring compliance with data privacy regulations, maintaining data security, and safeguarding sensitive information. \n\nData Privacy Compliance Records: Documentation of compliance with data privacy protocols, including audit results, incident reports, and privacy impact assessments. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Data Privacy Protocol Suite for ensuring data privacy, compliance, and data security. | Responsibility Explanation: Data Privacy Implementation Guide: Guides the implementation of data privacy tools. \n\nData Privacy Training Modules: Training materials for personnel.\n \nData Privacy Compliance Report: A report documenting the state of data privacy compliance and any identified gaps. | Data Explanation: Data privacy policy documents. \n\nTraining materials and records for staff on data privacy protocols. \n\nReports from penetration tests conducted to assess system vulnerabilities. | Fairness Explanation: Privacy Protocol Fairness Framework: A framework that outlines data privacy protocols with an emphasis on fairness. \n\nEncryption and Anonymization Fairness Procedures: Procedures that ensure data encryption and anonymization practices are fair and do not lead to discrimination. \n\nPenetration Testing and Fairness Compliance Reports: Reports on penetration testing results that assess compliance with data privacy and fairness standards. | Safety & Performance  Explanation: Deliverables for this control include: \n\nData Privacy Tools and Protocols Documentation: Detailing encryption, anonymization, and access control measures. \n\nStaff Training Programs and Materials: Focused on data privacy practices. \n\nPenetration Testing Reports: Assess system vulnerabilities and validate privacy measures. \n\nData Access and Privacy Logs: Track compliance with privacy protocols. | Impact Explanation: Data Privacy Protocols: A suite of documented protocols outlining how data privacy is maintained throughout the AI system's life cycle, including collection, processing, storage, and sharing.\n \nPrivacy Compliance Reports: Regular reports summarizing the outcomes of privacy compliance assessments, highlighting adherence to protocols and any identified privacy issues."
    },
    {
        "control_name": "Breach Reporting Channels",
        "category": "Data Privacy Impact Assessment",
        "description": "Establish proper channels for reporting breaches and procedures for protecting reporting persons.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AC-2: Account Management\nAU-6: Audit Record Review, Analysis, and Reporting\nIR-1: Incident Response Policy and Procedures\nIR-4: Incident Handling\nIR-5: Incident Monitoring\nIR-6: Incident Reporting\nIR-8: Incident Response Plan\nPS-3: Personnel Screening\nPS-6: Access Agreements, SCF: CPL-01.1: Non-Compliance Oversight - Oversight mechanisms for non-compliance, including breach reporting failures.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Reviews that can include the efficacy of breach reporting channels.\nCPL-05: Legal Assessment of Investigative Inquires - Legal assessment procedures can encompass breach reporting protocols.\nCPL-05.1: Investigation Request Notifications - Procedures for notifying relevant parties about investigation requests, aligning with breach reporting channels.\nCPL-05.2: Investigation Access Restrictions - Controls for limiting access during investigations, important for protecting reporters in breach cases.\nHRS-07: Personnel Sanctions - Sanction procedures for non-compliance, relevant for enforcing breach reporting rules.\nHRS-07.1: Workplace Investigations - Procedures for conducting investigations, relevant for breach reports.\nIRO-04.1: Data Breach - Controls for managing and reporting data breaches, crucial for breach reporting channels.\nIRO-10: Incident Stakeholder Reporting - Reporting protocols for stakeholders, essential for breach reporting channels.\nIRO-10.2: Cyber Incident Reporting for Sensitive Data - Reporting mechanisms for incidents involving sensitive data, part of breach reporting channels.",
        "explainability": "The Breach Reporting Channels control involves the establishment of clear and accessible channels and procedures for reporting data breaches or security incidents related to AI systems. The rationale for this control is to facilitate timely reporting, incident response, and compliance with data protection regulations.",
        "evidence": "Rationale Explanation: Breach Reporting Procedures: A document outlining the procedures and contact information for reporting data breaches or security incidents related to AI systems, including escalation processes and response mechanisms. \n\nRationale for Breach Reporting Channels: A report explaining the reasons for implementing breach reporting channels, such as facilitating timely reporting, incident response, and compliance with data protection regulations. \n\nBreach Reporting Records: Documentation of reported breaches or security incidents, actions taken for response, and compliance with reporting procedures. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Breach Reporting Channels for timely reporting, incident response, and compliance with data protection regulations. | Responsibility Explanation: Breach Reporting Procedure Manual: Details reporting steps and roles. \n\nWhistleblower Protection Policy: Outlines measures to protect reporters.\n \nBreach Response and Reporting Protocol: A protocol defining the steps to be taken in response to reported breaches, including immediate actions and long-term measures. | Data Explanation: Guidelines and procedures for breach notification. \n\nWhistleblower protection policies. \n\nTraining modules on breach reporting protocols. | Fairness Explanation: Breach Reporting Fairness Protocols: Protocols that ensure breach reporting channels are established and operated with fairness. \n\nReporting Channel Protection and Fairness Plans: Plans that detail the protection of reporting persons and the fairness of the reporting process. \n\nBreach Response Fairness Documentation: Documentation of breach responses, focusing on how they uphold fairness and protect reporters. | Safety & Performance  Explanation: Deliverables for this control include: \n\nBreach Reporting Guidelines: Outline the procedures for reporting security incidents.\n \nWhistleblower Protection Policy: Safeguard the interests of the reporting individuals. \n\nReporting Channels Implementation Details: Such as hotlines, secure email systems, or web portals. \n\nTraining Materials: On how to use the reporting channels and the protections provided to reporters. | Impact Explanation: Breach Reporting Channels Documentation: A documented plan outlining the procedures, channels, and contact information for reporting data breaches related to AI systems. \n\nBreach Incident Reports: Regular reports summarizing the outcomes of breach incident responses, highlighting response times and any identified areas for improvement."
    },
    {
        "control_name": "AI Infringement Detection & Notification",
        "category": "Data Monitoring & Auditing",
        "description": "Develop advanced monitoring systems to detect infringements and notify appropriate regulatory officials (e.g., EU Data Protection Supervisor), detailing the extent of the breach and involved entities.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nAU-12: Audit Record Generation\nIR-4: Incident Handling\nIR-6: Incident Reporting\nSI-4: System Monitoring\nAC-2: Account Management\nRA-5: Vulnerability Monitoring and Scanning\nCA-7: Continuous Monitoring\nAC-8: System Use Notification, SCF: CPL-05.1: Investigation Request Notifications - Mechanisms for notifying authorities about investigation requests related to AI infringements.\nMON-01: Continuous Monitoring - Implementing continuous monitoring tools that can detect AI data infringements.\nMON-01.11: Automated Response to Suspicious Events - Automated responses to potential AI infringements detected in monitoring systems.\nMON-02: Centralized Collection of Security Event Logs - Collecting and analyzing security event logs to identify potential infringements in AI systems.\nMON-03: Content of Event Logs - Ensuring event logs contain sufficient information to detect AI infringements.\nMON-05: Response To Event Log Processing Failures - Responding to and analyzing event log processing failures that might indicate an infringement.\nMON-06: Monitoring Reporting - Reporting mechanisms within monitoring systems, essential for infringement notification.\nIRO-04: Incident Response Plan (IRP) - Plans that include procedures for detecting and responding to AI infringements.\nIRO-10.1: Automated Reporting - Automated reporting mechanisms for detected infringements, relevant for regulatory notifications.\nIRO-11: Incident Reporting Assistance - Assistance structures for reporting AI system infringements.",
        "explainability": "The AI Infringement Detection & Notification control involves the implementation of mechanisms to detect potential infringements of AI-related intellectual property, copyrights, or patents and notifying relevant parties. The rationale for this control is to protect intellectual property, ensure compliance with legal requirements, and maintain ethical behavior in AI development and deployment.",
        "evidence": "Rationale Explanation: Infringement Detection and Notification Framework: A document outlining the framework and processes for detecting potential infringements of AI-related intellectual property and notifying relevant parties, including notification protocols and escalation procedures. \n\nRationale for Infringement Detection: A report explaining the reasons for implementing infringement detection and notification, such as protecting intellectual property, ensuring legal compliance, and maintaining ethical behavior. \n\nInfringement Detection Records: Documentation of detected infringements, notifications sent, and actions taken for resolution. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI Infringement Detection and Notification for protecting intellectual property, legal compliance, and ethical AI development and deployment. | Responsibility Explanation: Infringement Detection System Design Document: Describes the design and implementation of systems to detect AI infringements. \n\nNotification Procedure Manual: Details the process for notifying authorities, including roles and responsibilities. \n\nCompliance Officer Training Program: Training material for compliance officers on infringement detection and notification procedures. | Data Explanation: Infringement detection system specifications and operational guidelines. \n\nNotification protocols for regulatory bodies.\n\nIncident reports submitted to authorities like the European Data Protection Supervisor. | Fairness Explanation: Infringement Detection Fairness Protocol: A set of procedures for detecting AI infringements that includes steps to ensure fairness is considered in the notification process. \n\nRegulatory Notification Fairness Reports: Detailed reports to regulatory officials that outline the nature of AI infringements with an emphasis on the fairness implications for affected entities. \n\nBreach Extent and Fairness Impact Documentation: Documentation detailing breaches and their potential impact on fairness, especially concerning data subjects' rights. | Safety & Performance  Explanation: Deliverables for this control include: \n\nInfringement Detection System Design Document: Outlines the technical specifications of the monitoring systems. \n\nNotification Protocols: Detailing the procedures for alerting authorities about breaches for regulatory bodies. \n\nBreach Reports: Provide comprehensive details on detected infringements. \n\nContact Lists: For regulatory officials to be notified in the event of a breach. | Impact Explanation: Infringement Detection Protocols: Documented protocols outlining the strategies, criteria, and technologies used for detecting potential infringements related to AI systems. \n\nNotification Reports: Regular reports summarizing the outcomes of infringement detection activities, highlighting identified infringements, notifications, and actions taken."
    },
    {
        "control_name": "Data Integrity & Access Audit",
        "category": "Data Monitoring & Auditing",
        "description": "Regularly audit the integrity of training data and monitor who has access to it.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 75: Mutual Assistance, Market Surveillance and Control of General Purpose AI Systems\nArticle 28: Notifying Authorities, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nAU-9: Protection of Audit Information\nAC-6: Least Privilege\nAC-4: Information Flow Enforcement\nSI-7: Software, Firmware, and Information Integrity\nCM-6: Configuration Settings\nMP-5: Media Transport\nRA-5: Vulnerability Monitoring and Scanning, SCF: CFG-02.2: Automated Central Management & Verification - Central management and automated verification systems that could be used to audit data integrity.\nCFG-06: Configuration Enforcement - Enforcing configurations that may include access controls and data integrity checks for AI systems.\nMON-01.2: Automated Tools for Real-Time Analysis - Tools for real-time analysis of data integrity and access patterns.\nMON-01.4: System Generated Alerts - Alerts generated by systems in case of integrity breaches or unauthorized access to AI training data.\nMON-01.6: Host-Based Devices - Monitoring devices that host AI training data for integrity and access violations.\nMON-01.7: File Integrity Monitoring (FIM) - Monitoring the integrity of files, including AI training data, to detect unauthorized changes.\nMON-02.2: Central Review & Analysis - Centralized review and analysis of data integrity and access logs.\nMON-03.7: Database Logging - Logging database activities, which is useful for auditing access to AI training data stored in databases.\nDCH-06.2: Sensitive Data Inventories - Maintaining inventories of sensitive data, including AI training data, which helps in auditing its integrity.\nIAC-15: Account Management - Managing accounts that have access to AI training data, ensuring only authorized access.",
        "explainability": "The Data Integrity & Access Audit control involves implementing processes and audits to ensure the integrity and security of AI system data, along with controlled access to it. The rationale for this control is to maintain data accuracy, prevent unauthorized access, and uphold data security and compliance with data protection regulations.",
        "evidence": "Rationale Explanation: Data Integrity and Access Audit Procedures: A document outlining the procedures and protocols for auditing data integrity, data access controls, and security measures in AI systems. \n\nRationale for Data Integrity and Access Audit: A report explaining the reasons for implementing data integrity and access audits, such as maintaining data accuracy, preventing unauthorized access, and upholding data security and compliance. \n\nData Audit Records: Documentation of audit results, identified data integrity issues, access violations, and corrective actions taken. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Data Integrity and & Access Audit for data accuracy, security, and compliance with data protection regulations. | Responsibility Explanation: Data Integrity Audit Plan: A plan for conducting regular audits of data integrity. \n\nData Access Control Policy: A policy outlining who can access data and under what circumstances. \n\nAudit Report Template: A standardized template for reporting audit findings. | Data Explanation: Audit reports on data integrity. \n\nAccess logs and control lists. \n\nProcedures and findings of data access reviews. | Fairness Explanation: Data Integrity Audit Procedures: Procedures for auditing data integrity with a focus on maintaining fairness in the training process. \n\nAccess Control Fairness Assessment Reports: Reports that assess who has access to training data and whether these access controls are aligned with fairness principles. \n\nTraining Data Access Fairness Logs: Logs documenting access to training data and any fairness-related issues identified during audits. | Safety & Performance  Explanation: Deliverables for this control include: \n\nData Integrity Audit Protocols: Outlining how data integrity checks are conducted.\n \nAccess Monitoring Reports: Detail who has accessed the training data, when, and for what purpose. \n\nData Quality Metrics: Define the standards for data integrity. \n\nAudit Trail Systems: Log all interactions with the training data. | Impact Explanation: Data Integrity Protocols: Documented protocols outlining the strategies and technologies used to maintain data integrity throughout the AI system's life cycle.\n\nAccess Audit Reports: Regular reports summarizing the outcomes of access audits, highlighting access patterns, anomalies, and any identified risk."
    },
    {
        "control_name": "AI Data Backup & Recovery",
        "category": "Data Storage & Archiving",
        "description": "Implement automated data backup solutions for AI systems, ensuring data integrity and availability. Regularly test data recovery procedures to ensure minimal downtime during disruptions.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity, NIST 800-53: CP-9: System Backup\nCP-10: System Recovery and Reconstitution\nCP-4: Contingency Plan Testing\nSA-11: Developer Testing and Evaluation\nSI-7: Software, Firmware, and Information Integrity\nCP-6: Alternate Storage Site\nCP-7: Alternate Processing Site\nSC-24: Fail in Known State\nCM-6: Configuration Settings, SCF: BCD-11: Data Backups - Implementing data backup strategies specifically for AI systems, ensuring data availability and integrity.\nBCD-11.1: Testing for Reliability & Integrity - Testing backup solutions for reliability and integrity, crucial for AI systems' data.\nBCD-11.4: Cryptographic Protection - Using cryptographic methods to protect the backup data of AI systems.\nBCD-11.5: Test Restoration Using Sampling - Regularly testing data recovery procedures for AI systems to ensure effectiveness and reliability.\nBCD-11.6: Transfer to Alternate Storage Site - Procedures for transferring AI data backups to alternate sites for enhanced data protection.\nBCD-11.7: Redundant Secondary System - Establishing redundant systems for AI data backups, ensuring data availability.\nMON-04: Event Log Storage Capacity - Monitoring the storage capacity of backup logs for AI systems to ensure sufficient backup data retention.\nMON-05.1: Real-Time Alerts of Event Logging Failure - Monitoring the backup process of AI systems and alerting in real-time for failures.\nMNT-03.2: Predictive Maintenance - Implementing predictive maintenance, which can include proactive checks on AI data backup systems.\nMNT-10: Maintenance Validation - Validating that maintenance activities, including data backups and recovery procedures for AI systems, are effective.",
        "explainability": "The AI Data Backup & Recovery control involves the establishment of processes and mechanisms for regularly backing up AI system data and ensuring efficient data recovery in case of data loss or system failures. The rationale for this control is to prevent data loss, maintain data availability, and ensure business continuity.",
        "evidence": "Rationale Explanation: Data Backup & Recovery Plan: A document outlining the strategies, procedures, and protocols for data backup and recovery in AI systems, including backup schedules, data retention policies, and recovery mechanisms. \n\nRationale for Data Backup & Recovery: A report explaining the reasons for implementing data backup and recovery, such as preventing data loss, maintaining data availability, and ensuring business continuity. \n\nData Backup and Recovery Records: Documentation of data backup schedules, data recovery actions, and data retention policies. \n\nRationale Document: A comprehensive nontechnical  report justifying the importance of AI Data Backup & Recovery for preventing data loss, maintaining data availability, and ensuring business continuity. | Responsibility Explanation: Data Backup Solution Design Document: Documenting the design of the backup solutions. \n\nData Recovery Test Plan: A plan for regularly testing data recovery procedures. \n\nBackup and Recovery Training Modules: Training materials for personnel involved in backup and recovery processes. | Data Explanation: Data backup and recovery strategy documents. \n\nLogs of backup activities and recovery drills. \n\nRecovery time objective (RTO) and recovery point objective (RPO) metrics. | Fairness Explanation: Data Backup Fairness Procedures: Procedures for data backups that incorporate fairness, ensuring data essential for fair AI operation is recoverable. \n\nRecovery Plan Fairness Assessment Reports: Reports assessing the fairness of data recovery plans, especially in maintaining the integrity of AI operations. \n\nBackup Solution Fairness Testing Logs: Logs from testing backup solutions, documenting their ability to maintain fairness during recovery operations. | Safety & Performance  Explanation: Deliverables for this control include: \n\nData Backup and Recovery Plan: Specifying backup schedules, methods, and storage solutions. \n\nBackup Process Documentation: Outlining the procedures for data backup.\n \nData Recovery Test Reports: Detailing the results of recovery drills and actual recovery scenarios. \n\nBackup System Logs: Providing an audit trail of backup activities and data integrity checks. | Impact Explanation: Backup and Recovery Plan: A documented plan outlining the strategies, schedules, and technologies used for data backup and recovery in AI systems. \n\nBackup and Recovery Reports: Regular reports summarizing the outcomes of backup and recovery activities, highlighting recovery times and any identified areas for improvement."
    },
    {
        "control_name": "AI System Backup & Recovery Assessments",
        "category": "Data Storage & Archiving",
        "description": "Ensure regular backups of the system's code, training data, and other sensitive information. Assess the frequency, success rate, and integrity of these backups to ensure system resilience.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity, NIST 800-53: CP-9: System Backup\nCP-4: Contingency Plan Testing\nCP-10: System Recovery and Reconstitution\nCM-3: Configuration Change Control\nSA-11: Developer Testing and Evaluation\nSI-7: Software, Firmware, and Information Integrity\nAU-6: Audit Record Review, Analysis, and Reporting\nRA-5: Vulnerability Monitoring and Scanning\nCM-2: Baseline Configuration, SCF: BCD-11: Data Backups - Implementing backup strategies for AI systems' code, training data, and sensitive information.\nBCD-11.1: Testing for Reliability & Integrity - Assessing the reliability and integrity of backups for AI systems.\nBCD-11.4: Cryptographic Protection - Employing cryptographic methods to secure backup data.\nBCD-11.5: Test Restoration Using Sampling - Conducting sample restoration tests to validate the success rate of backups.\nBCD-11.6: Transfer to Alternate Storage Site - Ensuring backups are stored securely at alternate sites for resilience.\nBCD-11.7: Redundant Secondary System - Establishing redundant systems for AI data backups, ensuring data availability.\nMON-04: Event Log Storage Capacity - Monitoring the storage capacity of backup logs to ensure adequate data retention.\nMON-05.1: Real-Time Alerts of Event Logging Failure - Providing real-time alerts for backup process failures in AI systems.\nMNT-03.2: Predictive Maintenance - Including AI data backup systems in predictive maintenance strategies.\nMNT-10: Maintenance Validation - Validating that maintenance activities, including data backups and recovery procedures for AI systems, are effective.",
        "explainability": "The Backup & Recovery Assessments control involves the practice of conducting assessments to evaluate the effectiveness of backup and recovery mechanisms for AI systems. The rationale for this control is to ensure the reliability and readiness of data recovery processes in case of data loss or system failures.",
        "evidence": "Rationale Explanation: Backup and Recovery Assessment Plan: A document outlining the procedures and criteria for conducting assessments of backup and recovery mechanisms in AI systems, including assessment schedules, test scenarios, and evaluation criteria.\n \nRationale for Backup and Recovery Assessments: A report explaining the reasons for conducting backup and recovery assessments, such as ensuring the reliability and readiness of data recovery processes and preventing data loss. \n\nAssessment Records: Documentation of assessment results, identified areas of improvement, and actions taken for enhancement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Backup and & Recovery Assessments for ensuring reliable data recovery and preventing data loss. | Responsibility Explanation: Backup Strategy Document: A detailed document outlining the backup strategy. \n\nBackup Success and Integrity Report Format: A format for reporting the success rate and integrity of backups. \n\nBackup Process Training Guide: A guide for training personnel on the backup process. | Data Explanation: Backup schedules and protocols. \n\nBackup success and failure reports. \n\nIntegrity verification results for backups. | Fairness Explanation: Recovery Assessment Fairness Framework: A framework outlining the assessment of backup and recovery operations with a focus on fairness. \n\nSystem Code and Data Backup Fairness Reports: Reports detailing the fairness considerations in the backup of system code, training data, and other sensitive information. \n\nRecovery Success Rate Fairness Analysis: An analysis of the recovery success rate, examining the implications for fairness in AI system functionality. | Safety & Performance  Explanation: The deliverables must include a detailed Backup Policy Document, Recovery Procedure Guide, and Backup Integrity and Success Logs. Additionally, this should include a Data Resilience Analysis report that outlines the potential risk and the mitigating strategies in place, demonstrating a proactive approach to system recovery. | Impact Explanation: Assessment Plan: A documented plan outlining the strategies, criteria, and schedules for assessing the effectiveness of backup and recovery procedures and systems. \n\nAssessment Reports: Regular reports summarizing the outcomes of backup and recovery assessments, highlighting strengths, weaknesses, and any identified areas for improvement."
    },
    {
        "control_name": "Secure AI Data Storage Solutions",
        "category": "Data Storage & Archiving",
        "description": "Store AI training data securely using encryption, secure data transfer protocols, and firewall protections.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SC-13: Cryptographic Protection\nSC-8: Transmission Confidentiality and Integrity\nSC-7: Boundary Protection\nAC-3: Access Enforcement\nSC-28: Protection of Information at Rest\nSI-7: Software, Firmware, and Information Integrity\nMP-5: Media Transport\nAC-4: Information Flow Enforcement, SCF: CFG-08: Sensitive / Regulated Data Access Enforcement - Enforcing access control on AI data storage systems to prevent unauthorized access.\nCFG-08.1: Sensitive / Regulated Data Actions - Monitoring and controlling actions on AI data storage systems to ensure secure handling.\nCRY-05: Encrypting Data At Rest - Utilizing encryption to secure AI training data when stored.\nCRY-05.1: Storage Media - Implementing security controls on storage media used for AI training data.\nCRY-05.3: Database Encryption - Encrypting databases where AI training data is stored.\nCRY-10: Transmission of Cybersecurity & Data Privacy Attributes - Securing the transmission of data privacy attributes related to AI data.\nNET-03: Boundary Protection - Using firewalls and other boundary protection mechanisms to safeguard AI data storage networks.\nNET-03.7: Isolation of Information System Components - Isolating AI data storage systems from other network segments for enhanced security.\nNET-12: Safeguarding Data Over Open Networks - Implementing secure data transfer protocols to protect AI data during transit.\nNET-14.2: Protection of Confidentiality / Integrity Using Encryption - Ensuring the confidentiality and integrity of AI data using encryption during storage and transfer.",
        "explainability": "The Secure AI Data Storage Solutions control involves the implementation of secure and compliant data storage solutions for AI systems. The rationale for this control is to safeguard data confidentiality, integrity, and availability, as well as ensure compliance with data protection regulations.",
        "evidence": "Rationale Explanation: Secure AI Data Storage Plan: A document outlining the strategies and solutions for secure data storage in AI systems, including encryption methods, access controls, and data retention policies. \n\nRationale for Secure Data Storage: A report explaining the reasons for implementing secure data storage solutions, such as safeguarding data confidentiality, integrity, availability, and ensuring compliance with data protection regulations. \n\nData Storage Compliance Records: Documentation of compliance with data storage protocols, audit results, incident reports, and privacy impact assessments. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Secure AI Data Storage Solutions for safeguarding data, compliance, and data protection. | Responsibility Explanation: Secure Storage Solution Design Document: A document outlining the secure storage solution design. \n\nEncryption and Transfer Protocol Guidelines: Guidelines for using encryption and secure data transfer protocols. \n\nSecurity Protocol Training Materials: Training materials for personnel on security protocols. | Data Explanation: Data storage and encryption policy documents. \n\nSystem architecture diagrams showing firewall protections. \n\nLogs of data transfers and access attempts. | Fairness Explanation: Data Storage Fairness Security Policy: A policy that dictates secure storage solutions for AI training data with an emphasis on maintaining fairness. \n\nEncryption and Transfer Protocol Fairness Guidelines: Guidelines for encrypting and transferring AI data, with considerations for how these protocols affect fairness. \n\nSecure Storage Fairness Compliance Certificates: Certificates that affirm the secure storage solutions for AI data uphold fairness standards. | Safety & Performance  Explanation: Comprehensive encryption policy documents, certified encryption algorithm implementations, Secure Sockets Layer (SSL)/Transport Layer Security (TLS) certificates, detailed firewall setup and maintenance logs, secure data transfer protocol configurations, and third-party audit compliance reports that validate the adherence to industry-specific security standards. | Impact Explanation: Secure Storage Protocols: Documented protocols outlining the strategies and technologies used to secure AI system data storage, including encryption, access controls, and monitoring. \n\nSecurity Compliance Reports: Regular reports summarizing the outcomes of security compliance assessments for AI data storage solutions, highlighting adherence to protocols and any identified security issues."
    },
    {
        "control_name": "Secure Model Repositories",
        "category": "Data Storage & Archiving",
        "description": "Ensure model repositories or \"zoos\" are secured against tampering or malicious insertions.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-3: Access Enforcement\nAC-6: Least Privilege\nSI-7: Software, Firmware, and Information Integrity\nCM-6: Configuration Settings\nSC-28: Protection of Information at Rest\nSC-8: Transmission Confidentiality and Integrity\nSA-11: Developer Testing and Evaluation\nSA-17: Software, Firmware, and Information Integrity\nRA-5: Vulnerability Monitoring and Scanning, SCF: CFG-02: System Hardening Through Baseline Configurations - Hardening model repositories to protect against unauthorized access and tampering.\nCFG-02.8: Respond To Unauthorized Changes - Implementing mechanisms to respond to unauthorized changes in model repositories.\nCFG-06: Configuration Enforcement - Enforcing secure configurations in model repositories to prevent vulnerabilities.\nMON-01.7: File Integrity Monitoring (FIM) - Monitoring the integrity of files in model repositories to detect tampering.\nCRY-01.4: Conceal / Randomize Communications - Concealing or randomizing communications to and from model repositories to protect against data leakage or interception.\nCRY-02: Cryptographic Module Authentication - Using cryptographic modules to authenticate access to model repositories.\nCRY-05: Encrypting Data At Rest - Encrypting data within model repositories to secure against unauthorized access.\nNET-03.2: External Telecommunications Services - Securing external telecommunications services connected to model repositories.\nNET-03.7: Isolation of Information System Components - Isolating model repositories from other network segments for enhanced security.\nNET-04.10: Detection of Unsanctioned Information - Detecting and preventing the insertion of unsanctioned or malicious information into model repositories.",
        "explainability": "The Secure Model Repositories control involves implementing secure and compliant repositories for storing AI models and related assets. The rationale for this control is to safeguard the confidentiality, integrity, and availability of AI models, as well as ensure compliance with data protection regulations and intellectual property protection.",
        "evidence": "Rationale Explanation: Secure Model Repository Plan: A document outlining the strategies and solutions for secure model repositories, including access controls, encryption methods, versioning mechanisms, and audit trails. \n\nRationale for Secure Model Repositories: A report explaining the reasons for implementing secure model repositories, such as safeguarding model confidentiality, integrity, availability, and ensuring compliance with data protection and intellectual property regulations. \n\nModel Repository Compliance Records: Documentation of compliance with repository protocols, audit results, access logs, and incident reports. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Secure Model Repositories for safeguarding models, compliance, and intellectual property protection. | Responsibility Explanation: Model Repository Security Protocol: A document outlining security measures for protecting model repositories. \n\nTamper Detection System Implementation Guide: A guide for implementing systems to detect tampering. \n\nSecurity Training for Repository Managers: Training materials focused on security practices for those managing repositories. | Data Explanation: Security protocols for model repository management. \n\nAudit logs of repository access and change history. \n\nIncident response plans for security breaches. | Fairness Explanation: Model Repository Security and Fairness Protocol: A protocol that ensures model repositories are secure and that the security measures uphold fairness.\n \nTampering Prevention Fairness Guidelines: Guidelines focused on preventing tampering or malicious insertions in model repositories with fairness in mind. \n\nRepository Security Fairness Audit Reports: Audit reports that assess the security of model repositories and their adherence to fairness standards. | Safety & Performance  Explanation: Access control policies, user authentication and authorization records, model repository security protocols, change management logs, code integrity verification reports, and incident response plans. These should also include version control histories to trace model updates and security patches applied over time. | Impact Explanation: Model Repository Security Guidelines: Documented guidelines outlining the strategies, access controls, and monitoring mechanisms used to secure AI model repositories. \n\nSecurity Compliance Reports: Regular reports summarizing the outcomes of security compliance assessments for model repositories, highlighting adherence to guidelines and any identified security issues."
    },
    {
        "control_name": "Data Storage & Backup Security",
        "category": "Data Storage & Archiving",
        "description": "Ensure that training data, models, and backups are stored securely with regular security checks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity, NIST 800-53: CP-9: System Backup\nSI-7: Software, Firmware, and Information Integrity\nSC-28: Protection of Information at Rest\nRA-5: Vulnerability Monitoring and Scanning\nCM-6: Configuration Settings\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAU-6: Audit Record Review, Analysis, and Reporting\nMP-5: Media Transport, SCF: CFG-02.8: Respond To Unauthorized Changes - Implementing measures to respond to any unauthorized changes in stored data and backups.\nCFG-06: Configuration Enforcement - Enforcing secure configurations for data storage and backup systems to prevent vulnerabilities.\nMON-01.4: System Generated Alerts - Generating alerts for unusual activities in data storage and backup systems.\nMON-01.7: File Integrity Monitoring (FIM) - Monitoring the integrity of stored data and backups to detect tampering or corruption.\nCRY-01.4: Conceal / Randomize Communications - Protecting data transfers to and from storage and backup systems.\nCRY-05: Encrypting Data At Rest - Encrypting training data, models, and backups to secure them against unauthorized access.\nMNT-04.3: Prevent Unauthorized Removal - Preventing unauthorized removal or alteration of data in storage and backups.\nMNT-05.3: Remote Maintenance Cryptographic Protection - Applying cryptographic protection for remote maintenance of data storage and backup systems.\nNET-03.2: External Telecommunications Services - Securing connections to external data storage and backup locations.\nVPM-05.1: Centralized Management of Flaw Remediation Processes - Centrally managing the security of data storage and backup systems.",
        "explainability": "The Data Storage & Backup Security control involves implementing measures to ensure the security of data storage and backup systems for AI. The rationale for this control is to protect sensitive data from unauthorized access, data breaches, and data loss while maintaining data integrity.",
        "evidence": "Rationale Explanation: Data Storage and Backup Security Protocols: A document outlining the security protocols, encryption methods, access controls, and backup procedures for data storage and backup systems used in AI.\n \nRationale for Data Storage and Backup Security: A report explaining the reasons for implementing data storage and backup security measures, such as protecting sensitive data, preventing unauthorized access, and maintaining data integrity. \n\nSecurity Compliance Records: Documentation of compliance with security protocols, audit results, access logs, and backup records. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Data Storage and & Backup Security for safeguarding sensitive data, compliance, and data integrity. | Responsibility Explanation: Data and Backup Security Strategy: A comprehensive strategy for securing data and backups. \n\nSecurity Check Procedures Manual: A manual detailing procedures for regular security checks. \n\nSecurity Awareness Training Program: A program for training relevant personnel in data and backup security. | Data Explanation: Security policies for data and backup storage. \n\nSecurity assessment reports for storage solutions. \n\nLogs of security checks and identified vulnerabilities. | Fairness Explanation: Data Security Fairness Protocol: A protocol outlining data storage and backup security measures with fairness implications considered. \n\nSecurity Audit Fairness Guidelines: Guidelines for regular security checks that incorporate fairness evaluations. \n\nStorage and Backup Fairness Reports: Reports that document the security status of stored data and backups, assessing fairness implications. | Safety & Performance  Explanation: Data storage and backup policy documents, encryption compliance certifications, backup schedule logs, redundancy system configurations, disaster recovery plans, and regular security check reports. It should also include evidence of encrypted backup validation and data restoration testing. | Impact Explanation: Storage and Backup Security Protocols: Documented protocols outlining the strategies, access controls, encryption, and monitoring mechanisms used to secure AI data storage and backup processes. \n\nSecurity Compliance Reports: Regular reports summarizing the outcomes of security compliance assessments for data storage and backup, highlighting adherence to protocols and any identified security issues."
    },
    {
        "control_name": "Training Data Recovery Protection",
        "category": "Data Storage & Archiving",
        "description": "Establish mechanisms to prevent the recovery of original training data by inverting the model outputs.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SC-28: Protection of Information at Rest\nSC-13: Cryptographic Protection\nSI-7: Software, Firmware, and Information Integrity\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSC-7: Boundary Protection\nSA-17: Software, Firmware, and Information Integrity\nSI-14: Non-Persistence, SCF: CFG-02.8: Respond To Unauthorized Changes - Implementing measures to respond to any unauthorized attempts to invert or recover training data.\nMON-01.4: System Generated Alerts - Generating alerts for unusual activities that may indicate attempts to recover training data through inversion.\nMON-01.7: File Integrity Monitoring (FIM) - Monitoring the integrity of stored training data to detect and prevent unauthorized inversion attempts.\nCRY-01.4: Conceal / Randomize Communications - Protecting data transfers to prevent leakage or inversion of training data.\nCRY-05: Encrypting Data At Rest - Encrypting training data to protect it against inversion or recovery through model outputs.\nCRY-05.3: Database Encryption - Encrypting databases containing training data to prevent recovery from model outputs.\nCRY-10.6: No Embedded Unencrypted Static Authenticators - Ensuring training data is encrypted to prevent recovery from model outputs.\nNET-03.5: Prevent Unauthorized Exfiltration - Preventing unauthorized exfiltration of training data via model outputs.\nSEA-09: Information Output Filtering - Filtering outputs of AI models to prevent data recovery through inversion.\nVPM-05.1: Centralized Management of Flaw Remediation Processes - Centrally managing security to prevent data recovery through model inversion.",
        "explainability": "The Training Data Recovery Protection control involves implementing measures to protect and ensure the recoverability of training data used for AI models. The rationale for this control is to safeguard the integrity and availability of training data, which is critical for maintaining model performance and reducing risk associated with data loss.",
        "evidence": "Rationale Explanation: Training Data Recovery Protection Plan: A document outlining the strategies and mechanisms for safeguarding and recovering training data, including backup procedures, data versioning, and recovery protocols. \n\nRationale for Training Data Recovery Protection: A report explaining the reasons for implementing training data recovery protection, such as safeguarding the integrity and availability of training data, maintaining model performance, and reducing data loss risk. risk.\n\nTraining Data Protection Records: Documentation of data protection measures, backup records, recovery actions, and data retention policies. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Training Data Recovery Protection for safeguarding training data, model performance, and data loss risk reduction. | Responsibility Explanation: Data Recovery Prevention Plan: A plan detailing mechanisms to prevent data recovery. \n\nModel Output Security Guidelines: Guidelines for ensuring model outputs do not allow data recovery. \n\nData Protection Training Modules: Training modules on data protection and preventing data recovery from model outputs. | Data Explanation: Protocols for preventing training data inversion. \n\nTechnological measures implemented (like differential privacy, encryption, etc.). \n\nRecords of testing these protections against model inversion techniques. | Fairness Explanation: Data Inversion Protection Fairness Procedures: Procedures that prevent the inversion of model outputs to recover original training data, with an emphasis on fairness. \n\nRecovery Protection Fairness Measures: Measures implemented to protect against data recovery, assessed for their fairness. \n\nInversion Protection Fairness Impact Assessment: An assessment evaluating the impact of inversion protection measures on maintaining fairness. | Safety & Performance  Explanation: Protocols for differential privacy implementation, documentation of output perturbation methods, records of privacy-preserving model training techniques, and proof of compliance with data protection regulations such as GDPR. Documentation should clearly delineate procedures for maintaining the balance between data utility and privacy. | Impact Explanation: Training Data Recovery Protocols: Documented protocols outlining the strategies, access controls, encryption, and monitoring mechanisms used to protect the recovery of training data. \n\nSecurity Compliance Reports: Regular reports summarizing the outcomes of security compliance assessments for training data recovery processes, highlighting adherence to protocols and any identified security issues."
    },
    {
        "control_name": "Metadata Leakage",
        "category": "Metadata Security",
        "description": "Ensure metadata (like model version, timestamps, etc.) is not leaked through outputs or errors, which can provide attackers with more information.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-4: Information Flow Enforcement\nAC-6: Least Privilege\nSC-28: Protection of Information at Rest\nSC-8: Transmission Confidentiality and Integrity\nSI-4: System Monitoring\nSI-10: Information Input Validation\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSC-7: Boundary Protection, SCF: CFG-02.2: Automated Central Management & Verification - Centrally managing and verifying system configurations to avoid unintended metadata exposure.\nMON-01.4: System Generated Alerts - Generating alerts for activities that may lead to metadata leakage.\nMON-02.6: Audit Level Adjustments - Adjusting audit levels to avoid excessive detail that might lead to metadata leakage.\nMON-03.4: Verbosity Logging for Boundary Devices - Ensuring that logging verbosity levels are appropriately set to prevent accidental metadata leakage.\nMON-03.7: Database Logging - Ensuring database logs do not inadvertently reveal sensitive metadata.\nMON-08.3: Cryptographic Protection of Event Log Information - Protecting event logs, including metadata, from unauthorized disclosure.\nCRY-10.5: Protection of Authenticators - Safeguarding authentication information, including metadata, to prevent unintended disclosure.\nNET-04.5: Metadata - Managing and securing metadata to prevent leakage and unauthorized access.\nNET-18.2: Visibility of Encrypted Communications - Monitoring encrypted communications to prevent metadata leakage.\nSEA-09: Information Output Filtering - Filtering outputs of AI models to ensure metadata is not inadvertently disclosed.",
        "explainability": "The Metadata Leakage control involves the implementation of measures to prevent unintentional leakage of metadata associated with AI models and data. The rationale for this control is to protect sensitive metadata, maintain data privacy, and reduce the risk of unintended exposure of critical information.",
        "evidence": "Rationale Explanation: Metadata Leakage Prevention Plan: A document outlining strategies, controls, and safeguards to prevent metadata leakage in AI systems, including data handling practices, access controls, and monitoring mechanisms. \n\nRationale for Metadata Leakage Prevention: A report explaining the reasons for implementing metadata leakage prevention measures, such as protecting sensitive metadata, maintaining data privacy, and reducing the risk of unintended exposure. \n\nMetadata Protection Records: Documentation of metadata protection measures, incident reports, and corrective actions. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Metadata Leakage Prevention for safeguarding metadata, data privacy, and reducing the risk of unintended exposure. | Responsibility Explanation: Metadata Leakage Prevention Protocol: A protocol outlining measures to prevent metadata leakage. \n\nError Handling Guidelines: Guidelines for handling errors to avoid metadata leakage. \n\nMetadata Security Training Materials: Training materials focused on preventing metadata leakage. | Data Explanation: Security guidelines on metadata handling. \n\nIncident reports detailing any past metadata leaks. \n\nSystem configuration settings that prevent metadata exposure. | Fairness Explanation: Metadata Security Fairness Policy: A policy that dictates measures to prevent metadata leakage in a manner that upholds fairness. \n\nMetadata Leakage Prevention Fairness Protocols: Protocols ensuring metadata is not leaked in a way that could affect fairness. \n\nMetadata Handling Fairness Audit Reports: Audit reports that evaluate the handling of metadata and any potential fairness implications. | Safety & Performance  Explanation: Policies and procedures for secure metadata management, logs of output and error monitoring, redaction tools and techniques documentation, incident response plans concerning metadata breaches, and audit trails showing the enforcement of metadata privacy controls. | Impact Explanation: Metadata Leakage Prevention Protocols: Documented protocols outlining the strategies, access controls, and monitoring mechanisms used to prevent metadata leakage. \n\nSecurity Compliance Reports: Regular reports summarizing the outcomes of security compliance assessments for metadata leakage prevention, highlighting adherence to protocols and any identified issues."
    },
    {
        "control_name": "Centralized AI Database Management",
        "category": "Data Life Cycle & Governance Framework",
        "description": "Ensure the organization establishes and manages a centralized database for high-risk AI system details, conducting regular compliance checks and fostering interdepartmental communication to validate information accuracy.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 49: Registration\nArticle 71: EU Database for High-Risk AI Systems Listed in Annex III, NIST 800-53: CM-8: System Component Inventory\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-2: Baseline Configuration\nPL-4: Rules of Behavior\nPM-9: Risk Management Strategy\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nIR-6: Incident Reporting\nSA-10: Developer Configuration Management, SCF: CFG-02.2: Automated Central Management & Verification - Implementing centralized management and verification systems for database accuracy and compliance.\nCFG-02.8: Respond To Unauthorized Changes - Responding promptly to unauthorized changes in the database to maintain data integrity.\nCFG-04.1: Open Source Software - Managing open source software used in database management and ensuring compliance.\nCFG-08: Sensitive / Regulated Data Access Enforcement - Strictly enforcing access controls for sensitive AI data in the database.\nMON-02.1: Correlate Monitoring Information - Correlating various data sources within the database for better management and compliance verification.\nMON-02.2: Central Review & Analysis - Centralizing review and analysis processes to ensure database integrity and compliance.\nMON-02.7: System-Wide / Time-Correlated Audit Trail - Maintaining a time-correlated audit trail across all systems, including databases.\nMON-03.7: Database Logging - Ensuring comprehensive logging within the database to track access and changes.\nMON-05.1: Real-Time Alerts of Event Logging Failure - Generating real-time alerts for any failures in database event logging.\nTDA-04.2: Software Bill of Materials (SBOM) - Maintaining a detailed list of components used in the database system, contributing to its integrity and security.",
        "explainability": "The Centralized AI Database Management control involves the centralization and management of databases used in AI systems. The rationale for this control is to streamline data management, ensure data integrity, and enhance data security while simplifying data access and maintenance.",
        "evidence": "Rationale Explanation: Centralized AI Database Management Plan: A document outlining the strategies, protocols, and mechanisms for centralized database management in AI systems, including data integration methods, access controls, and data maintenance procedures. \n\nRationale for Centralized Database Management: A report explaining the reasons for implementing centralized database management, such as streamlining data management, ensuring data integrity, enhancing data security, and simplifying data access. \n\nDatabase Management Records: Documentation of database management measures, audit results, data integration records, and data access logs. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Centralized AI Database Management for efficient data management, data integrity, security, and simplified data access. | Responsibility Explanation: Centralized Database Design and Management Plan: A plan for the design and management of the centralized database. \n\nCompliance Check Procedure Document: A document outlining procedures for regular compliance checks. \n\nInterdepartmental Communication Framework: A framework to foster communication between departments for information validation. | Data Explanation: Structure and access protocols for the centralized AI database.\n\nCompliance check records. \n\nCommunication logs between departments regarding database updates and verifications. | Fairness Explanation: AI Database Management Fairness Framework: A framework that establishes procedures for centralized database management with an emphasis on fairness. \n\nCompliance Check Fairness Protocols: Protocols for conducting compliance checks that incorporate fairness assessments. \n\nDatabase Information Fairness Verification Reports: Reports that verify the accuracy of AI system information in the database, focusing on fairness. | Safety & Performance  Explanation: Centralized AI database architecture designs, data governance policies, compliance check protocols,  interdepartmental communication logs, and system detail verification records. Documentation on database access controls and audit trails for information verification and updates are also key deliverables. | Impact Explanation: Centralized Database Management Plan: A documented plan outlining the strategies, access controls, and monitoring mechanisms used for managing AI databases centrally. \n\nDatabase Management Reports: Regular reports summarizing the outcomes of database management activities, highlighting efficiency, security, and any identified areas for improvement."
    },
    {
        "control_name": "Human Oversight of High-Risk AI",
        "category": "Automated Decision-Making Oversight",
        "description": "Ensure the design and development of high-risk AI systems incorporate mechanisms for effective human oversight. Individuals overseeing these systems are AI literate and equipped to investigate incidents.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: AT-3: Role-Based Training\nPS-7: External Personnel Security\nIR-4: Incident Handling\nSA-11: Developer Testing and Evaluation\nSA-15: Development Process, Standards, and Tools\nPM-13: Security  and Privacy Workforce\nPL-2: System Security and Privacy Plans\nAC-2: Account Management\nSI-4: System Monitoring, SCF: CHG-02.3: Cybersecurity & Data Privacy Representative for Asset Lifecycle Changes - Involving trained representatives in decisions and changes related to AI systems.\nMON-01.15: Privileged User Oversight - Monitoring and overseeing the activities of privileged users involved in AI system management.\nHRS-02.1: Users With Elevated Privileges - Ensuring individuals with oversight roles over high-risk AI systems possess the necessary privileges and AI literacy.\nHRS-03.2: Competency Requirements for Security-Related Positions - Setting competency requirements for those overseeing AI systems to ensure they can effectively investigate incidents and manage the AI system.\nHRS-13.2: Identify Vital Cybersecurity & Data Privacy Staff - Identifying key staff responsible for AI oversight as part of vital cybersecurity and data privacy roles.\nHRS-13.3: Establish Redundancy for Vital Cybersecurity & Data Privacy Staff - Creating redundancy in the team overseeing AI systems to ensure continuous and effective oversight.\nIAC-06: Multi-Factor Authentication (MFA) - Implementing secure access controls, such as MFA, for individuals responsible for overseeing high-risk AI systems.\nIAC-15.4: Automated Audit Actions - Implementing automated audits to support human oversight in monitoring AI systems.\nIAC-21.4: Auditing Use of Privileged Functions - Auditing the use of privileged functions by those overseeing AI systems to ensure responsible use and effective oversight.\nSAT-03.5: Privileged Users - Providing specific training to privileged users who oversee AI systems, enhancing their ability to respond to incidents.",
        "explainability": "The Human Oversight of High-Risk AI control involves the establishment of mechanisms for human oversight of AI systems that are deemed high risk. The rationale for this control is to ensure human control, intervention, and accountability in cases where high-risk AI systems make critical decisions to prevent unintended consequences and ensure responsible AI use.",
        "evidence": "Rationale Explanation: Human Oversight Plan for High-Risk AI: A document outlining the procedures, protocols, and roles for human oversight of high-risk AI systems, including decision-making criteria, escalation procedures, and intervention mechanisms. \n\nRationale for Human Oversight: A report explaining the reasons for implementing human oversight in high-risk AI, such as ensuring human accountability, intervention, and responsible AI use to prevent unintended consequences. \n\nOversight Compliance Records: Documentation of compliance with oversight protocols, records of interventions, and incident reports. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Human Oversight of High-Risk AI for ensuring human control, accountability, and responsible AI use. | Responsibility Explanation: Oversight Mechanism Design Document: A document outlining the mechanisms for human oversight in high-risk AI systems. \n\nAI Literacy and Training Program for Oversight Personnel: A comprehensive training program to ensure AI literacy among those overseeing AI systems. \n\nIncident Investigation Protocol: A protocol detailing how oversight personnel should investigate AI-related incidents. | Data Explanation: Documentation on oversight mechanisms and their integration into AI systems. \n\nTraining records for individuals responsible for oversight. \n\nIncident investigation protocols and reports. | Fairness Explanation: Human Oversight Fairness Training Modules: Training modules that prepare individuals for effective oversight of high-risk AI systems, with a focus on fairness. \n\nOversight Mechanism Fairness Guidelines: Guidelines that establish mechanisms for human oversight to identify and rectify fairness concerns.\n \nIncident Investigation Fairness Protocols: Protocols that ensure fairness is central to the investigation of incidents involving high-risk AI systems. | Safety & Performance  Explanation: Oversight policy documents, training records for AI literacy programs, AI incident response protocols, logs of human interventions in AI system operations, and reports from investigations into AI incidents. Documentation should also include the guidelines and criteria used for classifying AI systems as high risk. | Impact Explanation: Human Oversight Protocols: Documented protocols outlining the strategies, criteria, and mechanisms for incorporating human oversight in high-risk AI decision making.\n\nOversight Reports: Regular reports summarizing the outcomes of human oversight activities, highlighting intervention instances, monitoring results, and any identified areas for improvement."
    },
    {
        "control_name": "AI Ethics & Compliance Committee",
        "category": "AI Ethics Board or Committee Oversight",
        "description": "Ensure the organization establishes a dedicated committee focused on AI ethics and compliance, working alongside sector-specific legislation.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: PM-1: Information Security Program Plan\nPL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nAT-1: Awareness and Training Policy and Procedures\nSA-11: Developer Testing and Evaluation\nPM-13: Security and Privacy Workforce\nPM-11: Mission and Business Process Definition\nPM-18: Privacy Program Plan\nPM-6: Measures of Performance\nPM-31: Continuous Monitoring Strategy, SCF: CPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of AI controls, an essential function of the AI Ethics Committee.\nPRI-01.2: Privacy Act Statements - Ensuring AI ethics committee is informed about privacy considerations in AI systems.\nPRI-01.6: Security of Personal Data - Addressing the security of personal data in AI systems, a key concern for the AI Ethics Committee.\nIAC-01: Identity & Access Management (IAM) - Implementing IAM processes that ensure secure access for committee members to AI systems and data.\nIAO-02.4: Security Assessment Report (SAR) - Generating reports that could be used by the AI Ethics Committee to assess compliance and ethical considerations.\nPRM-04: Cybersecurity & Data Privacy In Project Management - Integrating AI ethics and compliance considerations in project management processes.\nRSK-01.2: Risk Management Resourcing - Allocating resources for risk management in AI projects, guided by the committee.\nRSK-01.3: Risk Tolerance - Assessing and defining the organization's risk tolerance in AI implementations, a key focus of the AI Ethics Committee.\nSAT-03: Role-Based Cybersecurity & Data Privacy Training - Providing specialized training for committee members on AI ethics and compliance.\nTHR-04: Insider Threat Program - Including considerations of insider threats in AI systems as part of the committee’s oversight.",
        "explainability": "The AI Ethics and Compliance Committee control involves the formation of a dedicated committee responsible for overseeing and ensuring the ethical and regulatory compliance of AI systems. The rationale for this control is to establish a governance structure that addresses ethical concerns, monitors compliance, and promotes responsible AI use.",
        "evidence": "Rationale Explanation: AI Ethics and Compliance Committee Charter: A document outlining the committee's purpose, responsibilities, members, and decision-making processes, including ethical guidelines, compliance monitoring, and reporting mechanisms. \n\nRationale for AI Ethics and Compliance Committee: A report explaining the reasons for establishing the committee, such as addressing ethical concerns, ensuring regulatory compliance, and promoting responsible AI use. \n\nCommittee Meeting Minutes: Records of committee meetings, discussions, decisions, and actions taken, including ethical assessments, compliance monitoring results, and recommendations. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of the AI Ethics and Compliance Committee for addressing ethical issues, ensuring compliance, and promoting responsible AI use. | Responsibility Explanation: Committee Charter and Membership: A document outlining the charter of the ethics and compliance committee and its members. \n\nEthics and Compliance Framework: A framework detailing the committee's operating procedures and compliance guidelines. \n\nSector-Specific Legislation Guide: A guide to relevant sector-specific legislation that the committee should consider. | Data Explanation: Charter documents defining the committee's roles and responsibilities. \n\nMeeting minutes and decision records of the committee. \n\nReports on AI ethics and compliance initiatives. | Fairness Explanation: Ethics Committee Fairness Charter: A charter for the AI ethics and compliance committee that embeds fairness into its core functions. \n\nCompliance Committee Fairness Review Process: A process by which the committee reviews AI projects and practices for their adherence to fairness standards. \n\nEthics Committee Fairness Action Reports: Reports detailing actions taken by the committee to address fairness concerns within AI systems. | Safety & Performance  Explanation: Formation charter for the AI Ethics and Compliance Committee, member selection criteria, records of meetings and decisions, ethical guidelines tailored to AI development, compliance checklists, and sector-specific legislative analysis documents. Also, this should include training materials for committee members on current AI ethics and compliance issues. | Impact Explanation: AI Ethics and Compliance Committee Charter: A documented charter outlining the mission, scope, responsibilities, and membership of the AI Ethics and Compliance Committee. \n\nEthics and Compliance Reports: Regular reports summarizing the outcomes of ethics and compliance activities, highlighting adherence to ethical guidelines and any identified areas for improvement."
    },
    {
        "control_name": "Evaluation of AI Codes of Conduct",
        "category": "AI Ethics Oversight & Auditing",
        "description": "Conduct evaluations of the impact and effectiveness of codes of conduct for AI systems, excluding high-risk AI systems, at specified intervals.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: CA-2: Control Assessments\nCA-7: Continuous Monitoring\nPM-1: Information Security Program Plan\nPM-9: Risk Management Strategy\nPM-6: Measures of Performance\nPL-2: System Security and Privacy Plans\nSA-11: Developer Testing and Evaluation\nSA-9: External System Services\nAT-1: Awareness and Training Policy and Procedures\nAT-2: Literacy Training and Awareness, SCF: CPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversight of AI ethics and conduct-related controls.\nCPL-03.2: Functional Review Of Cybersecurity & Data Privacy Controls - Functionally reviewing how codes of conduct are implemented in AI systems.\nPRI-01.2: Privacy Act Statements - Evaluating AI codes of conduct in the context of privacy legislation compliance.\nPRI-01.6: Security of Personal Data - Evaluating the impact of AI codes of conduct on the security of personal data.\nPRI-03: Choice & Consent - Ensuring AI codes of conduct respect user choice and consent, fundamental to ethical AI governance.\nIAC-01: Identity & Access Management (IAM) - Implementing IAM processes that ensure secure and authorized access for evaluators to AI systems and data.\nRSK-04.1: Risk Register - Maintaining a risk register that includes risk related to the adherence and effectiveness of AI codes of conduct.\nSAT-02: Cybersecurity & Data Privacy Awareness Training - Providing training on AI ethics and codes of conduct for relevant personnel.\nTHR-10: Threat Analysis - Analyzing threats to AI systems that could arise from non-compliance with codes of conduct.",
        "explainability": "The Evaluation of AI Codes of Conduct control involves the systematic assessment of AI codes of conduct, which are guidelines and ethical standards for AI system development and usage. The rationale for this control is to ensure that AI codes of conduct are aligned with ethical principles, regulatory requirements, and responsible AI practices.",
        "evidence": "Rationale Explanation: AI Code of Conduct Evaluation Framework: A document outlining the criteria and methodologies for evaluating AI codes of conduct, including ethical principles, regulatory alignment, and responsible AI practices. \n\nRationale for AI Code of Conduct Evaluation: A report explaining the reasons for conducting evaluations of AI codes of conduct, such as ensuring alignment with ethical standards, regulatory compliance, and responsible AI use. \n\nEvaluation Reports: Documentation of evaluations performed on AI codes of conduct, including findings, recommendations, and areas of improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of the Evaluation of AI Codes of Conduct for ensuring ethical, regulatory, and responsible AI practices. | Responsibility Explanation: Codes of Conduct Evaluation Plan: A plan detailing how evaluations of AI codes of conduct will be conducted. \n\nImpact Assessment Reports: Reports assessing the impact and effectiveness of the codes of conduct. \n\nEvaluation Methodology Guide: A guide explaining the methodologies used for evaluations. | Data Explanation: Strategies for evaluating codes of conduct. \n\nAssessment reports on the effectiveness of AI codes of conduct. \n\nTimelines and schedules for periodic evaluations. | Fairness Explanation: Codes of Conduct Fairness Evaluation Framework: A framework for evaluating AI codes of conduct, focusing on their contribution to fairness. \n\nCode Impact Fairness Study Guidelines: Guidelines that direct how to study the impact of codes of conduct on fairness. \n\nConduct Code Fairness Assessment Reports: Reports assessing the effectiveness of codes of conduct in promoting fairness within AI systems. | Safety & Performance  Explanation: Evaluation schedules, assessment frameworks for codes of conduct, impact analysis reports, effectiveness measurement tools, and documentation of recommendations for code amendments. These artifacts serve to provide a structured approach to the ongoing review of AI conduct guidelines. | Impact Explanation: AI Code of Conduct Assessment Framework: A documented framework outlining the criteria, processes, and methodologies used for evaluating AI codes of conduct. \n\nAssessment Reports: Regular reports summarizing the outcomes of AI code of conduct evaluations, highlighting adherence to ethical principles and legal requirements, and identifying any areas for improvement."
    },
    {
        "control_name": "AI Systems & Standards",
        "category": "AI Ethics Oversight & Auditing",
        "description": "Evidence AI systems adhere to standards of health, safety, fundamental rights, democracy, rule of law, and the environment.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: RA-3: Risk Assessment\nCA-2: Control Assessments\nSA-11: Developer Testing and Evaluation\nPL-2: System Security and Privacy Plans\nPE-1: Physical and Environmental Protection Policy and Procedures\nPM-9: Risk Management Strategy\nAC-1: Access Control Policy and Procedures\nAU-6: Audit Record Review, Analysis, and Reporting\nSI-2: Flaw Remediation, SCF: PRI-01: Data Privacy Program - Establishing a data privacy program that aligns with fundamental rights and legal requirements.\nHRS-01: Human Resources Security Management - Managing human resources to ensure adherence to ethical standards in AI systems.\nRSK-01: Risk Management Program - Implementing a comprehensive risk management program that considers health, safety, and rights-related risk in AI systems.",
        "explainability": "The AI Systems & Standards control involves the adherence to established industry standards and best practices for the development and operation of AI systems. The rationale for this control is to ensure that AI systems meet quality, safety, and ethical standards, providing transparency and accountability.",
        "evidence": "Rationale Explanation: AI Systems and Standards Compliance Framework: A document outlining the framework for assessing AI systems' adherence to industry standards, quality metrics, safety protocols, and ethical guidelines. \n\nRationale for AI Systems and Standards Compliance: A report explaining the reasons for adhering to industry standards and best practices, such as ensuring quality, safety, transparency, and accountability in AI systems. \n\nCompliance Assessment Reports: Documentation of compliance assessments conducted on AI systems, including findings, corrective actions, and recommendations. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI Systems & and Standards for ensuring quality, safety, transparency, and accountability in AI systems. | Responsibility Explanation: Standards Compliance Checklist: A checklist for assessing compliance with various standards. \n\nAI System Audit Reports: Reports from audits conducted to assess compliance. \n\nCompliance Training Program for AI Teams: A training program focused on ensuring AI teams understand and adhere to the required standards. | Data Explanation: Compliance certificates or reports from recognized standard-setting bodies. \n\nDocumentation of AI system testing against health and safety standards. \n\nAssessments of AI impact on fundamental rights and the environment. | Fairness Explanation: AI Standards Adherence Reports: Reports documenting the AI system's adherence to health, safety, and fundamental rights standards. \n\nDemocracy and Rule of Law Compliance Logs: Logs that track compliance with democratic principles and the rule of law. \n\nEnvironmental Standards Fairness Review: A review of how AI systems align with environmental standards, considering fairness implications. | Safety & Performance  Explanation: Compliance documentation with health and safety standards, human rights alignment reports, democratic principles adherence statements, rule of law compliance certifications, and environmental impact assessments. These documents provide evidence of the AI systems' conformity with the relevant standards and values. | Impact Explanation: Impact Assessment Report: A detailed report assessing the potential and observed impacts on individuals and society due to the implementation of AI systems and standards. \n\nMonitoring Plan: A document outlining the ongoing process for monitoring and evaluating the impact over time."
    },
    {
        "control_name": "AI System Development & Law",
        "category": "AI Ethics Oversight & Auditing",
        "description": "Ensure documentation showing the developmental stage of an AI system abides by fundamental rights and local law (e.g., EU Law), if applicable.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: SA-11: Developer Testing and Evaluation\nSA-4: Acquisition Process\nPL-2: System Security and Privacy Plans\nRA-3: Risk Assessment\nPM-18: Privacy Program Plan\nSA-10: Developer Configuration Management\nAT-2: Literacy Training and Awareness\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-2: Baseline Configuration, SCF: GOV-01: Governance Framework - Establishing a governance framework that oversees AI system development in accordance with legal and rights requirements.\nPRI-02: Privacy by Design - Incorporating privacy considerations in AI systems from the earliest stages of development, as mandated by law.\nRSK-02: Risk Assessment during Development - Assessing risk associated with AI systems at each developmental stage, focusing on legal and rights compliance.",
        "explainability": "AI System Development & Law control involves ensuring that AI system development adheres to relevant legal requirements and regulations. The rationale for this control is to avoid legal issues, liability, and compliance violations while promoting ethical and responsible AI development.",
        "evidence": "Rationale Explanation: AI Development Legal Compliance Guidelines: A document outlining the legal requirements and regulations that AI development must adhere to, including data protection laws, intellectual property rights, and liability standards. \n\nRationale for AI System Development and Law Compliance: A report explaining the reasons for adhering to legal requirements and regulations in AI development, such as avoiding legal issues, ensuring compliance, and promoting responsible AI Development & Law.\n \nCompliance Records: Documentation of legal compliance assessments, compliance actions, and evidence of adherence to relevant laws and regulations.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of AI System Development 5 & Law for avoiding legal issues, ensuring compliance, and promoting ethical and responsible AI development. | Responsibility Explanation: Development Stage Documentation: Documentation outlining each stage of AI system development. \n\nLegal Compliance Review Protocol: A protocol for reviewing compliance with fundamental rights and EU law. \n\nFundamental Rights and Legal Training Modules: Training modules for AI teams on fundamental rights and legal compliance. | Data Explanation: Development documentation aligned with legal and rights frameworks. \n\nLegal review reports at various stages of AI system development. \n\nCompliance checklists and sign-offs at each development milestone. | Fairness Explanation: Legal Compliance Documentation: Documentation that evidences the AI system's development stages and compliance with fundamental rights and EU law. \n\nDevelopment Stage Fairness Analysis: Analysis of the AI system's development process, focusing on adherence to fairness principles. \n\nLaw Abidance and Fairness Verification Checklists: Checklists used to verify that AI development stages respect fundamental rights and abide by EU law, with fairness as a central concern. | Safety & Performance  Explanation: Developmental stage documentation, legal compliance checklists, fundamental rights impact assessments, internal audit reports, legal advisory statements, and EU law alignment verifications. The documentation should reflect a clear traceability of compliance from the early stages of AI system development. | Impact Explanation: Legal Compliance Report: A report detailing how the AI system complies with relevant laws and regulations. \n\nEthical Compliance Checklist: A checklist ensuring adherence to ethical principles and guidelines."
    },
    {
        "control_name": "Team Composition Transparency",
        "category": "AI Ethics Oversight & Auditing",
        "description": "Ensure there is diverse representation in relation to the system use functional and/or technical requirements.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: AT-2: Literacy Training and Awareness\nPL-4: Rules of Behavior\nPM-13: Security and Privacy Workforce\nPM-18: Privacy Program Plan\nSI-22: Information Diversity\nAC-1: Access Control Policy and Procedures\nPS-7: External Personnel Security\nSR-2: Supply Chain Risk Management Plan\nSR-5: Acquisition Strategies, Tools, and Methods, SCF: GOV-02: Governance Framework for Team Diversity - Establishing governance structures that advocate for team diversity in AI projects.",
        "explainability": "The Team Composition Transparency control involves providing clear information about the composition of teams responsible for AI system development and deployment. The rationale for this control is to enhance transparency, accountability, and ethical considerations by disclosing the roles and backgrounds of team members.",
        "evidence": "Rationale Explanation: Team Composition Transparency Guidelines: A document outlining the guidelines for disclosing team composition information, including roles, expertise, and affiliations of team members involved in AI development.\n \nRationale for Team Composition Transparency: A report explaining the reasons for providing transparency in team composition, such as enhancing accountability, ethical considerations, and trust in AI systems. \n\nTransparency Reports: Documentation of information disclosures regarding team composition, roles, expertise, and affiliations of team members.\n \nRationale Document: A comprehensive nontechnical report justifying the importance of Team Composition Transparency for enhancing transparency, accountability, and ethical considerations in AI development. | Responsibility Explanation: Team Composition Report: A document detailing the diversity of the AI team and how it relates to the system’s use.\n \nDiversity and Inclusion Policy: A policy document outlining the organization's commitment to diversity in AI team composition. \n\nDiversity Training Program: A training program for hiring managers and team leaders to promote diversity in team composition. | Data Explanation: Public disclosures on team diversity. \n\nDocumentation of efforts to align team composition with diversity goals. \n\nAssessments of team diversity in relation to system use and impact. | Fairness Explanation: Team Diversity Transparency Reports: Reports that disclose the composition of development teams and the representation of diversity in relation to the AI system's use. \n\nDiversity Representation Fairness Guidelines: Guidelines ensuring that team composition reflects diversity and supports fairness. \n\nDevelopment Team Fairness Audit Logs: Audit logs that record team composition and assess its impact on the fairness of the AI system. | Safety & Performance  Explanation: Diversity and inclusion policies, demographic breakdowns of the AI team, reports on team composition over the course of development, and analyses of how the team's diversity correlates with the system's intended users. These artifacts serve to demonstrate the organization's commitment to diversity and its impact on system development. | Impact Explanation: Team Composition Report: A document detailing the roles, expertise, and affiliations of team members involved in AI system development. \n\nEthical Guidelines Acknowledgment: A document where team members acknowledge adherence to ethical guidelines."
    },
    {
        "control_name": "Annotation & Labeling Diversity",
        "category": "AI Ethics Oversight & Auditing",
        "description": "Develop and communicate annotation guidelines  to human annotators to label the data, ensuring they don't encode biases.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: PL-4: Rules of Behavior\nAT-2: Literacy Training and Awareness\nAT-3: Role-Based Training\nSA-11: Developer Testing and Evaluation\nRA-5: Vulnerability Monitoring and Scanning\nSI-22: Information Diversity\nSA-8: Security and Privacy Engineering Principles\nPM-18: Privacy Program Plan\nAC-1: Access Control Policy and Procedures, SCF: GOV-03: Governance Framework for Data Annotation - Creating a governance framework that oversees the data annotation process to ensure diversity and reduce biases.",
        "explainability": "The Annotation & Labeling Diversity control involves ensuring diversity and representativeness in the annotation and labeling of data used in AI training. The rationale for this control is to mitigate bias, improve fairness, and enhance the accuracy and robustness of AI systems by incorporating diverse perspectives.",
        "evidence": "Rationale Explanation: Annotation and Labeling Diversity Guidelines: A document outlining guidelines and methodologies for achieving diversity and representativeness in the annotation and labeling of AI training data, including considerations for diverse perspectives. \n\nRationale for Annotation and Labeling Diversity: A report explaining the reasons for promoting diversity in annotation and labeling, such as mitigating bias, improving fairness, and enhancing accuracy and robustness of AI systems. \n\nDiversity Assessment Reports: Documentation of assessments and findings related to the diversity and representativeness of annotated and labeled data. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Annotation and & Labeling Diversity for mitigating bias, improving fairness, and enhancing accuracy and robustness in AI systems. | Responsibility Explanation: Annotation Guidelines: A set of guidelines provided to human annotators to prevent bias. \n\nBias Monitoring Protocol: A protocol for monitoring and identifying biases in data annotation. \n\nAnnotator Training Modules: Training modules focused on bias-free annotation practices. | Data Explanation: Comprehensive annotation guidelines. \n\nTraining materials for annotators on avoiding bias. \n\nQuality control reports on annotation outcomes. | Fairness Explanation: Annotation Guidelines Fairness Policy: A policy that ensures annotation guidelines are designed to prevent the encoding of biases and promote fairness. \n\nLabeling Process Fairness Review Procedures: Procedures for reviewing the labeling process to ensure it upholds fairness. \n\nAnnotator Training Fairness Manuals: Manuals that train human annotators on fair labeling practices to prevent bias. | Safety & Performance  Explanation: Detailed annotation guidelines, training materials for annotators, documentation of the annotation process and quality checks, records of annotator training sessions, and bias audit reports for the labeling process. These documents are crucial for demonstrating the measures taken to avoid bias in data labeling. | Impact Explanation: Diversity in Annotation Report: A document outlining the measures taken to incorporate diverse perspectives in the annotation and labeling process. \n\nData Bias Mitigation Plan: A plan detailing how biases identified in the annotation process will be addressed."
    },
    {
        "control_name": "Respect for Fundamental Rights",
        "category": "AI Ethics Oversight & Auditing",
        "description": "Ensure that AI systems respect and uphold fundamental human rights, including privacy, freedom of expression, and nondiscrimination.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules.\nRecital 110: Emphasizes harmful bias, discrimination, threats to privacy and human rights., NIST 800-53: AC-1: Policy and Procedures\nPL-2: System Security and Privacy Plans\nAU-6: Audit Record Review, Analysis, and Reporting\nRA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nSI-2: Flaw Remediation\nPS-7: External Personnel Security\nPM-27: Privacy Reporting\nAT-2: Literacy Training and Awareness, SCF: GOV-05: AI Governance and Human Rights - Establishing a governance framework that includes oversight of AI systems' compliance with human rights.\nPRI-01: Privacy Protection in AI Systems - Implementing strong privacy protections in AI systems to safeguard personal data and individual privacy rights.",
        "explainability": "The Respect for Fundamental Rights control involves ensuring that AI systems and their operations respect and uphold fundamental human rights, such as privacy, freedom of expression, and nondiscrimination. The rationale for this control is to safeguard individuals' rights and prevent harm, discrimination, or privacy violations caused by AI systems.",
        "evidence": "Rationale Explanation: Fundamental Rights Compliance Guidelines: A document outlining guidelines and principles for respecting and upholding fundamental human rights in AI system development, deployment, and operation, including considerations for privacy, freedom of expression, and non-discrimination. nondiscrimination.\n\nRationale for Respect for Fundamental Rights: A report explaining the reasons for ensuring respect for fundamental human rights in AI systems, such as safeguarding individuals' rights, preventing harm, discrimination, and privacy violations. \n\nCompliance Records: Documentation of compliance with fundamental rights guidelines, records of assessments, and actions taken to respect and uphold rights. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Respect for Fundamental Rights for safeguarding individuals' rights, preventing harm, discrimination, and privacy violations by AI systems. | Responsibility Explanation: Fundamental Rights Compliance Checklist: A checklist to ensure AI systems comply with fundamental rights. \n\nHuman Rights Impact Assessment Report: A report assessing the AI system's impact on human rights. \n\nCompliance Training Program: A training program for AI development teams on respecting fundamental rights. | Data Explanation: Policy documents detailing human rights considerations in AI.\n\nImpact assessment reports on fundamental rights. \n\nTraining programs for AI system designers and operators. | Fairness Explanation: Fundamental Rights Adherence Framework: A framework that delineates how AI systems should be designed to respect fundamental rights and promote fairness.\n \nRights Respect Fairness Action Plans: Action plans that outline steps to ensure AI systems do not infringe on human rights and maintain fairness.\n \nNondiscrimination Fairness Review Reports: Reports that review AI systems for nondiscrimination, ensuring they respect fundamental rights and fairness. | Safety & Performance  Explanation: Human rights impact assessments, privacy policy documents, nondiscrimination protocols, records of compliance with human rights legislation, and documentation of system design choices that promote freedom of expression. These artifacts demonstrate the organization's commitment to fundamental rights. | Impact Explanation: Fundamental Rights Compliance Assessment: A comprehensive assessment report detailing how the AI system respects and upholds fundamental rights. \n\nHuman Rights Impact Statement: A document outlining the potential impact of the AI system on human rights and how it aligns with international standards."
    },
    {
        "control_name": "AI Environmental Impact Assessment Protocol",
        "category": "Environmental Impact of AI Systems",
        "description": "Evaluate the environmental footprint of AI systems throughout their life cycle. This assessment covers energy consumption, carbon footprint during development and deployment phases, the impact of data center operations, and end-of-life handling of AI hardware. The protocol includes guidelines for minimizing negative environmental impacts through the selection of energy-efficient algorithms, use of green data centers, eco-friendly hardware, and responsible AI scaling strategies. It ensures that environmental considerations are integrated into the decision-making processes for the use of AI technologies, supporting the transition to more sustainable practices in the AI industry.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 27: Emphasizes the need to develop and use systems in accordance with privacy and data protection rules., NIST 800-53: PE-1: Physical and Environmental Protection Policy and Procedures\nSR-1: Supply Chain Risk Management Policy and Procedure\nSR-2: Supply Chain Risk Management Plan\nSR-10: Inspection of Systems or Components\nPM-6: Life Cycle Cost Estimation and Control\nRA-3: Risk Assessment\nSI-2: Flaw Remediation\nCA-2: Control Assessments\nCM-4: Impact Analysis\nPL-8: Security and Privacy Architecture, SCF:  AAT-17.2: AI & Autonomous Technologies Environmental Impact & Sustainability - Assess the environmental impact and sustainability of high-risk AI systems before their deployment.",
        "explainability": "The AI Environmental Impact Assessment Protocol control involves the development and implementation of a protocol to assess the environmental impact of AI systems and their operations. The rationale for this control is to evaluate and mitigate environmental consequences, such as energy consumption and carbon footprint, associated with AI technologies.",
        "evidence": "Rationale Explanation: AI Environmental Impact Assessment Protocol: A document outlining the protocol for assessing the environmental impact of AI systems, including energy usage, carbon emissions, and sustainable practices. \n\nRationale for AI Environmental Impact Assessment: A report explaining the reasons for conducting environmental impact assessments, such as addressing energy consumption, reducing carbon footprint, and promoting sustainable AI practices. \n\nEnvironmental Impact Assessment Reports: Documentation of assessments and findings related to the environmental impact of AI systems, including recommendations for sustainability improvements. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of the AI Environmental Impact Assessment Protocol for addressing environmental consequences, energy efficiency, and sustainable AI practices. | Responsibility Explanation: Environmental Impact Assessment Protocol: A protocol for assessing the environmental impact of AI systems. \n\nSustainability Implementation Guide: A guide for minimizing negative environmental impacts. \n\nGreen Technology Training Program: Training materials focused on the use of eco-friendly technologies in AI development. | Data Explanation: Environmental impact assessment reports. \n\nPolicies on energy-efficient computing and use of green data centers. \n\nDocumentation on the eco-friendly disposal and recycling of AI hardware. | Fairness Explanation: Environmental Impact Fairness Assessment Protocol: A protocol that integrates environmental impact assessments with fairness considerations for AI systems. \n\nGreen Algorithm Selection Fairness Policies: Policies that guide the selection of energy-efficient algorithms and data center practices, with an emphasis on fairness and environmental sustainability. \n\nLife Cycle Environmental Fairness Reports: Reports documenting the AI system's environmental impact throughout its life cycle, considering fairness to future generations. | Safety & Performance  Explanation: Environmental impact assessment reports, energy consumption records for AI systems, carbon footprint calculations, data center sustainability certifications, hardware disposal procedures, and documentation of algorithm selection based on energy efficiency. Guidelines for using green data centers and eco-friendly hardware, as well as strategies for responsible AI scaling, are also key artifacts. | Impact Explanation: Environmental Impact Assessment Report: A comprehensive report detailing the environmental impact of the AI system, including resource consumption and potential pollutants. \n\nSustainability Plan: A document outlining strategies and actions to mitigate environmental impact and promote sustainability."
    },
    {
        "control_name": "Premarket Compliance Verification",
        "category": "Third-Party Compliance Verification",
        "description": "Verify high-risk AI systems compliance using a checklist of necessary requirements, ensuring they meet all operational, ethical, and technical standards before deploying to the market or placing in service.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 16: Obligations of Providers of High-Risk AI Systems\nArticle 25: Responsibilities Along the AI Value Chain, NIST 800-53: SA-11: Developer Testing and Evaluation\nSR-2: Supply Chain Risk Management Plan\nSR-3: Supply Chain Controls and Processes\nCA-2: Control Assessments\nRA-3: Risk Assessment\nPL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nSA-17: Software, Firmware, and Information Integrity\nCM-6: Configuration Settings\nSA-4: Acquisition Process, SCF: RSK-05: Risk Assessment Compliance - Assess high-risk AI systems against potential risk, ensuring compliance with risk management protocols.",
        "explainability": "The Premarket Compliance Verification control involves conducting thorough compliance verification before the market release of an AI system. The rationale for this control is to ensure that the AI system complies with regulatory requirements, ethical standards, and industry best practices, reducing the risk of legal issues and harm.",
        "evidence": "Rationale Explanation: Pre-Market Premarket Compliance Verification Plan: A document outlining the plan for verifying compliance before the market release of the AI system, including testing methodologies, ethical considerations, and regulatory compliance checks. \n\nRationale for Premarket Pre-Market Compliance Verification: A report explaining the reasons for conducting compliance verification before market release, such as ensuring regulatory compliance, adherence to ethical standards, and reducing legal risk. \n\nCompliance Verification Reports: Documentation of the results of compliance verification, including findings, corrective actions, and evidence of adherence to regulatory and ethical standards. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Pre-Market Compliance Verification for ensuring regulatory compliance, ethical standards, and reducing legal risk before market release. | Responsibility Explanation: Premarket Compliance Checklist: A checklist of requirements for high-risk AI systems before market deployment. \n\nCompliance Verification Report: A report documenting the compliance verification process and outcomes. \n\nQuality Assurance Training Modules: Training modules for quality assurance personnel on compliance verification. | Data Explanation: Premarket compliance checklist. \n\nVerification reports certifying compliance with standards. \n\nDocumentation of the review and approval process. | Fairness Explanation: Fairness Assessment Report: A comprehensive document detailing the results of fairness evaluations, including bias detection and mitigation strategies. \n\nCompliance Verification Checklist with Fairness Criteria: A checklist that includes specific fairness standards against which the AI system is evaluated. \n\nEquity Impact Analysis: An analysis that predicts the potential impact of the AI system on different demographics to ensure equitable outcomes. | Safety & Performance  Explanation: Compliance verification checklists, premarket test results, certification of ethical standards adherence, technical standards compliance documentation, operational readiness reports, and final approval documents. These artifacts establish that the AI system is ready for market introduction in a compliant manner. | Impact Explanation: Compliance Verification Report: A detailed report outlining the verification process and confirming compliance with applicable regulations and standards. \n\nRegulatory Checklist: A document summarizing the specific regulations and standards the AI system adheres to."
    },
    {
        "control_name": "Scheduled Compliance Audits",
        "category": "Third-Party Compliance Verification",
        "description": "Schedule and implement regular compliance audits, with a special emphasis on high-risk AI systems. These audits should assess both the system's performance and its adherence to internal and external standards.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 16: Obligations of Providers of High-Risk AI Systems\nArticle 25: Responsibilities Along the AI Value Chain\nAnnex VII: Conformity Based on Assessment of Quality Management System and Assessment of Technical Documentation, NIST 800-53: CA-2: Control Assessments\nCA-7: Continuous Monitoring\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-4: Impact Analysis\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nAC-6: Least Privilege\nSR-2: Supply Chain Risk Management Plan\nSR-6: Supplier Assessments and Reviews\nPL-2: System Security and Privacy Plans, SCF: RSK-07: Risk Management Audits - Conduct risk management audits to assess how high-risk AI systems manage and mitigate potential risk.",
        "explainability": "The Scheduled Compliance Audits control involves conducting regular and planned compliance audits to assess and ensure ongoing adherence to regulatory requirements, ethical standards, and industry best practices. The rationale for this control is to proactively identify and address compliance issues, mitigate risk, and demonstrate a commitment to ethical and lawful AI practices.",
        "evidence": "Rationale Explanation: Scheduled Compliance Audit Plan: A document outlining the plan for conducting scheduled compliance audits, including audit schedule, audit scope, compliance criteria, and audit methodologies. \n\nRationale for Scheduled Compliance Audits: A report explaining the reasons for conducting scheduled compliance audits, such as proactively identifying and addressing compliance issues, mitigating risk, risk, and demonstrating a commitment to ethical and lawful AI practices. \n\nCompliance Audit Reports: Documentation of the results of scheduled compliance audits, including findings, corrective actions, and evidence of ongoing adherence to regulatory and ethical standards. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Scheduled Compliance Audits for proactively ensuring ongoing compliance, mitigating risk, and demonstrating commitment to ethical AI practices. | Responsibility Explanation: Compliance Audit Schedule: A schedule outlining the timing and scope of regular audits. \n\nAudit Reporting Template: A standardized template for documenting audit findings. \n\nAuditor Training Program: A training program for auditors on the specifics of auditing AI systems. | Data Explanation: Audit schedule outlining the frequency and scope of compliance checks. \n\nDetailed audit reports with findings and recommendations. \n\nRecords of corrective actions taken in response to audit outcomes. | Fairness Explanation: Fairness Audit Schedule: A detailed timetable for when fairness audits will occur. \n\nFairness Monitoring Reports: Regular reports that document compliance with fairness standards over time. \n\nFairness Audit Protocols: Specific guidelines used to conduct the audits, ensuring consistent evaluation of fairness. | Safety & Performance  Explanation: Audit schedules, compliance audit reports, performance evaluation documents, records of adherence to internal policies and external regulations, action plans for addressing any identified issues, and documentation of any changes made to the AI systems following the audits. | Impact Explanation: Compliance Audit Reports: Periodic reports detailing the outcomes of scheduled compliance audits, including any identified issues and remediation plans. \n\nCompliance Calendar: A documented schedule outlining the dates and scope of upcoming compliance audits."
    },
    {
        "control_name": "Independence of Notified Bodies",
        "category": "Third-Party Compliance Verification",
        "description": "Ensure notified bodies maintain independence from providers, economic stakeholders, and competitors of high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 16: Obligations of Providers of High-Risk AI Systems\nArticle 25: Responsibilities Along the AI Value Chain\nArticle 31: Requirements Relating to Notified Bodies, NIST 800-53: PS-7: External Personnel Security\nAC-1: Access Control Policy and Procedures\nPS-3: Personnel Screening\nPL-4: Rules of Behavior\nSR-2: Supply Chain Risk Management Plan\nPM-15: Security and Privacy Groups and Associations\nPM-13: Security and Privacy Workforce\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-5: System Inventory, SCF: GOV-04: Governance of External Relationships - Govern relationships with external entities to preserve the independence of notified bodies.\nRSK-05: Risk Assessment of External Entities - Assess risk associated with external entities involved in the AI system's lifecycle, ensuring they do not compromise the independence of notified bodies.",
        "explainability": "The Independence of Notified Bodies control involves ensuring that bodies notified for conformity assessment are independent, unbiased, and free from conflicts of interest. The rationale for this control is to maintain the integrity of conformity assessments, provide reliable evaluations, and instill trust in the certification process.",
        "evidence": "Rationale Explanation: Independence Guidelines for Notified Bodies: A document outlining guidelines and criteria for ensuring the independence of bodies notified for conformity assessment, including requirements for independence, unbiased evaluations, and conflict of interest management. \n\nRationale for Independence of Notified Bodies: A report explaining the reasons for emphasizing the independence of notified bodies, such as maintaining assessment integrity, providing reliable evaluations, and instilling trust in the certification process. \n\nIndependence Assessment Reports: Documentation of assessments and findings related to the independence of notified bodies, including evidence of adherence to independence guidelines. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Independence of Notified Bodies for maintaining assessment integrity, ensuring reliable evaluations, and instilling trust in the certification process. | Responsibility Explanation: Independence Policy Document: A document outlining the policies ensuring independence. \n\nConflict of Interest Declarations: Forms for declaring potential conflicts of interest by notified bodies. \n\nIndependence Training Modules: Training materials focused on maintaining independence and identifying conflicts of interest. | Data Explanation: Policies and procedures that establish and protect the independence of notified bodies. \n\nDocumentation of organizational and operational structures that support impartiality. \n\nConflict of interest assessments and mitigation reports. | Fairness Explanation: Independence Verification Certificates: Documents certifying the notified body's independence from AI providers. \n\nConflict of Interest Policies: Policies that outline measures to prevent conflicts of interest. \n\nIndependence Audit Logs: Records of audits performed by the notified body, demonstrating adherence to independence requirements. | Safety & Performance  Explanation: In-depth organizational structure charts that delineate clear separations from AI industry entities, detailed conflict of interest avoidance strategies, comprehensive financial reports that demonstrate economic independence, rigorous transparency reports submitted to oversight agencies, and formal declarations of impartiality signed by all key personnel. These artifacts are designed to evidence the autonomous operation of the bodies and bolster trust in their evaluations and certifications. | Impact Explanation: Independence Verification Report: A report confirming the independence and impartiality of notified bodies involved in compliance assessments. \n\nConflict of Interest Policy: A document outlining policies and procedures to identify and mitigate conflicts of interest."
    },
    {
        "control_name": "Independence in Conformity Assessments",
        "category": "Third-Party Compliance Verification",
        "description": "Ensure employees tasked with performing conformity assessments of high-risk AI systems must maintain independence, having no prior or future affiliations with the AI system's provider within defined timeframes.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: PS-7: External Personnel Security\nAC-1: Access Control Policy and Procedures\nPS-3: Personnel Screening\nPS-6: Access Agreements\nAT-3: Role-Based Training\nPL-4: Rules of Behavior\nSA-11: Developer Testing and Evaluation\nPM-13: Security and Privacy Workforce, SCF: GOV-14: Clear Separation of Duties - Enforce a clear separation of duties between assessors and AI system providers to preserve independence.\nRSK-15: Assessor Background Checks - Conduct thorough background checks on assessors to identify any potential conflicts of interest with AI system providers.",
        "explainability": "The Independence in Conformity Assessments control involves ensuring that conformity assessments are conducted independently, without conflicts of interest, to provide reliable and unbiased evaluations of AI systems' compliance with standards and regulations. The rationale for this control is to maintain the credibility of conformity assessments, enhance trust, and ensure that assessments accurately reflect the adherence to established criteria.",
        "evidence": "Rationale Explanation: Independence Guidelines for Conformity Assessments: A document outlining guidelines and criteria for ensuring the independence of conformity assessments, including requirements for independence, unbiased evaluations, and conflict of interest management. \n\nRationale for Independence in Conformity Assessments: A report explaining the reasons for emphasizing the independence of conformity assessments, such as maintaining assessment credibility, enhancing trust, and ensuring accurate reflections of compliance with standards and regulations. \n\nIndependence Assessment Reports: Documentation of assessments and findings related to the independence of conformity assessments, including evidence of adherence to independence guidelines. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Independence in Conformity Assessments for maintaining assessment credibility, enhancing trust, and ensuring accurate reflections of compliance. | Responsibility Explanation: Conformity Assessment Guidelines: Guidelines for performing independent conformity assessments. \n\nEmployee Independence Agreement: An agreement for employees to maintain independence. \n\nConflict of Interest Training Program: A training program on identifying and avoiding conflicts of interest. | Data Explanation: Guidelines for assessor independence. \n\nRecords of employment and contractual agreements to ensure no conflicts of interest. \n\nAttestations of independence signed by conformity assessors. | Fairness Explanation: Assessor Independence Guidelines: Guidelines ensuring assessors have no vested interest in the AI system's outcomes. \n\nIndependence Training Certifications: Certifications that assessors have been trained to avoid potential conflicts of interest. \n\nConformity Assessment Fairness Logs: Logs that document the assessment process and any potential conflicts of interest. | Safety & Performance  Explanation: Comprehensive conflict of interest policies, in-depth vetting documentation for each assessor, a record of \"cooling-off\" period adherence, detailed training logs on ethical standards, ongoing monitoring reports of assessors' compliance with independence protocols, and formalized procedures for declaring and managing any potential conflicts that may arise. | Impact Explanation: Independence Assessment Report: A detailed report confirming the independence and impartiality of entities conducting conformity assessments. \n\nConflict of Interest Mitigation Plan: A document outlining strategies to identify and address potential conflicts of interest during conformity assessments."
    },
    {
        "control_name": "Reference to Third-Party Conformity Assessments",
        "category": "Third-Party Compliance Verification",
        "description": "Delineate clearly between AI system classifications and third-party conformity assessments, ensuring clarity in compliance processes.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: CA-2: Control Assessments\nCM-2: Baseline Configuration\nCM-4: Impact Analysis\nPM-9: Risk Management Strategy\nPL-2: System Security and Privacy Plans\nSR-2: Supply Chain Risk Management Plan\nSA-15: Development Process, Standards, and Tools\nPM-22: Personally Identifiable Information Quality Management\nSI-22: Information Diversity, SCF: GOV-22: AI System Classification Documentation - Maintain detailed documentation that clearly classifies AI systems and outlines requirements for third-party assessments.\nRSK-24: Risk Assessment of Conformity Procedures - Conduct risk assessments to ensure conformity procedures are clear and effectively mitigate risk associated with AI system classifications.",
        "explainability": "The Reference to Third-Party Conformity Assessments control involves incorporating references to third-party conformity assessments in AI documentation, ensuring transparency and credibility in demonstrating compliance with standards and regulations. The rationale for this control is to leverage the expertise of trusted third parties, enhance trust, and provide stakeholders with reliable evidence of compliance.",
        "evidence": "Rationale Explanation: Guidelines for Referencing Third-Party Conformity Assessments: A document outlining guidelines and criteria for referencing third-party conformity assessments in AI documentation, including requirements for transparency, accuracy, and credibility. \n\nRationale for Referencing Third-Party Conformity Assessments: A report explaining the reasons for incorporating references to third-party assessments, such as leveraging expertise, enhancing trust, and providing reliable evidence of compliance. \n\nReference Documentation: Inclusion of references to third-party conformity assessments in AI system documentation, specifying the standards, criteria, and results achieved through third-party assessments. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Reference to Third-party Conformity Assessments for leveraging expertise, enhancing trust, and providing reliable evidence of compliance. | Responsibility Explanation: AI System Classification Guide: A guide delineating AI system classifications. \n\nThird-Party Assessment Framework: A framework for third-party conformity assessments. \n\nCompliance Process Documentation: Documentation outlining the compliance process involving third-party assessments. | Data Explanation: Guidelines outlining third-party assessment procedures. \n\nDocumentation categorizing AI systems and mapping them to assessment protocols.\n\nAgreements with third-party assessors detailing the scope of work. | Fairness Explanation: Third-Party Assessment Fairness Standards: Standards that define how third-party assessments contribute to the overall fairness of the AI system.\n \nClassification and Assessment Clarity Guidelines: Guidelines to ensure clarity in the roles and processes of third-party assessments. \n\nThird-Party Compliance Fairness Reports: Reports that document third-party assessment findings with a focus on their implications for fairness. | Safety & Performance  Explanation: Detailed guides that describe the classification of AI systems, the scope and scale of third-party assessments, comprehensive process documentation that maps out each step in the conformity assessment, and clear communication materials that elucidate the roles and responsibilities of third-party assessors. | Impact Explanation: Third-Party Conformity Assessment Reports: Documents from third-party assessments confirming compliance with relevant standards. \n\nReference Verification Checklist: A checklist confirming that references to third-party assessments are accurate and up to date."
    },
    {
        "control_name": "Supervisory Authority-Organized Testing",
        "category": "Third-Party Compliance Verification",
        "description": "Supervisory authorities must implement a verification process to ensure that high-risk AI systems conform to regulatory standards. This process should check for counterfeit systems or falsified documentation and verify system behavior associated with the AI system.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 16: Obligations of Providers of High-Risk AI Systems\nArticle 25: Responsibilities Along the AI Value Chain\nArticle 60: Testing of High-Risk AI Systems in Real World Conditions Outside AI Regulatory Sandboxes\nArticle 76: Supervision of Testing in Real World Conditions by Market Surveillance Authorities, NIST 800-53: CA-2: Control Assessments\nSR-2: Supply Chain Risk Management Plan\nRA-3: Risk Assessment\nAT-2: Literacy Training and Awareness\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-8: System Component Inventory\nPL-2: System Security and Privacy Plans\nSA-11: Developer Testing and Evaluation\nSA-15: Development Process, Standards, and Tools, SCF: GOV-33: Falsified Documentation Checks - Develop and implement checks to identify falsified documentation associated with AI systems.\nRSK-34: Risk Management for Imported AI Systems - Conduct risk assessments for imported AI systems to ensure they adhere to regulatory and safety standards.",
        "explainability": "The Supervisory Authority-Organized Testing control involves the organization and execution of AI system testing by supervisory authorities to ensure compliance with regulations and ethical standards. The rationale for this control is to enhance oversight, verify system behavior, and validate adherence to established criteria under the guidance of regulatory bodies.",
        "evidence": "Rationale Explanation: Testing Plan Organized by Supervisory Authority: A document outlining the testing plan for AI systems organized and executed by supervisory authorities, specifying the testing scope, methodologies, and criteria for compliance verification. \n\nRationale for Supervisory Authority-Organized Testing: A report explaining the reasons for conducting testing organized by supervisory authorities, such as enhancing oversight, verifying system behavior, and validating adherence to regulations and ethical standards. \n\nTesting Reports: Documentation of the results of testing organized by supervisory authorities, including findings, compliance verification, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Supervisory Authority-Organized Testing for enhancing oversight, verifying system behavior, and validating adherence to regulations and ethical standards. | Responsibility Explanation: Verification Process Guide: A guide outlining the verification process for importers. \n\nCounterfeit Detection Protocol: A protocol for detecting counterfeit AI systems or documentation. \n\nRegulatory Compliance Checklist: A checklist to ensure AI systems meet regulatory standards. | Data Explanation: Verification protocols for imported AI systems. \n\nChecklists for identifying counterfeit systems. \n\nReports on verification outcomes and any fraudulent findings. | Fairness Explanation: Supervisory Testing Fairness Framework: A set of protocols for supervisory authority-organized testing that emphasizes fairness in the verification process. \n\nVerification Process Fairness Checklists: Detailed checklists that guide the verification process to prevent counterfeit or falsified documentation. \n\nImporter Fairness Verification Reports: Reports by importers that document their verification processes and adherence to fairness standards. | Safety & Performance  Explanation: Verification process guidelines, records of compliance checks, certificates of conformity, reports on counterfeit detection efforts, and documentation of any falsified documentation associated with AI systems. These documents are critical for proving the due diligence performed by importers to adhere to regulatory standards. | Impact Explanation: Supervisory Testing Report: A comprehensive report detailing the outcomes and findings of testing conducted by the supervisory authority. \n\nTesting Framework: A document outlining the criteria and procedures used by the supervisory authority in organizing and conducting tests."
    },
    {
        "control_name": "Distributor Oversight Mechanism",
        "category": "Third-Party AI Components & Data Source Governance",
        "description": "Implement a mechanism to ensure distributors of AI systems adhere to established standards, ethical guidelines, and legal requirements and take timely corrective actions for  nonconforming AI systems. This should also involve informing all relevant stakeholders of any issues and actions taken.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 16: Obligations of Providers of High-Risk AI Systems\nArticle 25: Responsibilities Along the AI Value Chain, NIST 800-53: SR-2: Supply Chain Risk Management Plan\nCA-7: Continuous Monitoring\nIR-4: Incident Handling\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-4: Impact Analysis\nPM-15: Security and Privacy Groups and Associations\nAC-2: Account Management\nRA-5: Vulnerability Monitoring and Scanning\nPL-4: Rules of Behavior, SCF: GOV-43: Oversight of Distributor Activities - Develop governance mechanisms to oversee distributor actions regarding AI system non-conformities.\nRSK-44: Risk Management in Distribution - Apply risk management principles to distribution activities, focusing on identifying and addressing non-conformities.",
        "explainability": "Distributor Oversight Mechanism control involves the implementation of a mechanism to oversee and regulate the activities of distributors involved in the distribution of AI systems. The rationale for this control is to ensure that distributors adhere to established standards, ethical guidelines, and legal requirements, mitigating risk associated with unauthorized distribution and misuse.",
        "evidence": "Rationale Explanation: Distributor Oversight Mechanism Guidelines: A document outlining guidelines and criteria for the distributor oversight mechanism, including requirements for monitoring, regulation, and compliance enforcement. \n\nRationale for Distributor Oversight Mechanism: A report explaining the reasons for implementing a distributor oversight mechanism, such as ensuring adherence to standards, ethical guidelines, and legal requirements, and mitigating risk associated with unauthorized distribution and misuse. \n\nOversight Reports: Documentation of the results of distributor oversight, including findings, compliance status, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of the Distributor Oversight Mechanism for ensuring adherence to standards, ethical guidelines, and legal requirements, and mitigating risk associated with unauthorized distribution and misuse. | Responsibility Explanation: Distributor Oversight Policy: A policy outlining the responsibilities of distributors regarding nonconforming AI systems. \n\nCorrective Action Procedure Manual: A manual detailing the procedures for corrective actions. \n\nStakeholder Communication Protocol: A protocol for informing stakeholders about issues and actions. | Data Explanation: Oversight protocols for distributor compliance. \n\nCommunication templates for notifying stakeholders of nonconformities. \n\nRecords of corrective actions undertaken by distributors. | Fairness Explanation: Distributor Fairness Action Plan: A plan that outlines the steps for distributors to take in the event of a fairness-related issue. \n\nStakeholder Notification Fairness Procedures: Procedures detailing how and when stakeholders are informed about fairness issues. \n\nCorrective Action Fairness Reports: Reports documenting the corrective actions taken and their impact on the fairness of the AI system. | Safety & Performance  Explanation: Oversight strategy documents, records of nonconformity cases and corrective actions taken, communication logs with stakeholders regarding nonconforming AI systems, and protocols for rapid issue resolution. Documentation that outlines the distributor's roles and responsibilities in maintaining oversight and ensuring compliance is essential. | Impact Explanation: Oversight Mechanism Report: A document outlining the procedures and outcomes of the oversight mechanism used to monitor distributors. \n\nDistributor Compliance Records: Documents confirming the compliance status of individual distributors."
    },
    {
        "control_name": "Importer's AI Conformity Assurance",
        "category": "Third-Party AI Components & Data Source Governance",
        "description": "Importers of high-risk AI systems must have procedures in place to validate the conformity of these systems with established regulations before introducing them to the market.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 43: Conformity Assessment\nAnnex VI: Conformity assessment procedure based on internal control, NIST 800-53: CA-2: Control Assessments\nSR-2: Supply Chain Risk Management Plan\nSR-4: Provenance\nRA-3: Risk Assessment\nCM-2: Baseline Configuration\nSA-11: Developer Testing and Evaluation\nPL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nCM-4: Impact Analysis\nSA-4: Acquisition Process, SCF: GOV-53: Governance of Import Processes - Establish governance mechanisms to oversee the import process of high-risk AI systems.\nRSK-54: Risk Management for Imported AI Systems - Apply risk management principles to the import of AI systems, focusing on regulatory compliance.",
        "explainability": "The Importer's AI Conformity Assurance control involves implementing a system to assure the conformity of AI systems imported into a jurisdiction. The rationale for this control is to ensure that imported AI systems meet the required standards, ethical guidelines, and legal requirements, preventing potential risk associated with noncompliant systems.",
        "evidence": "Rationale Explanation: Importer's AI Conformity Assurance Guidelines: A document outlining guidelines and criteria for the importer's AI conformity assurance system, including requirements for assessment, verification, and compliance enforcement. \n\nRationale for Importer's AI Conformity Assurance: A report explaining the reasons for implementing an importer's AI conformity assurance system, such as ensuring adherence to standards, ethical guidelines, and legal requirements for imported AI systems, and preventing potential risk associated with non-compliant noncompliant systems. \n\nConformity Assurance Reports: Documentation of the results of AI conformity assurance activities, including findings, compliance status, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of the Importer's AI Conformity Assurance for ensuring adherence to standards, ethical guidelines, and legal requirements, and preventing potential risk associated with noncompliant non-compliant systems. | Responsibility Explanation: Conformity Assurance Process Guide: A guide outlining the process for validating AI system conformity. \n\nRegulatory Compliance Checklist: A checklist for ensuring compliance with relevant regulations. \n\nImporter Training Program: A training program for importers on conformity assurance procedures. | Data Explanation: Conformity assurance processes and checklists for importers. \n\nDocumentation of conformity assessments and validation efforts. \n\nCertificates of conformity or compliance for the imported AI systems. | Fairness Explanation: Conformity Assurance Fairness Checklist: A checklist against which importers validate the fairness of AI systems pre market.\n\nAI System Fairness Validation Reports: Reports by importers documenting the validation of AI system conformity with fairness standards. \n\nRegulatory Compliance Fairness Certificates: Certificates issued to AI systems that meet regulatory standards for fairness. | Safety & Performance  Explanation: Conformity assurance protocols, compliance verification checklists, certificates of conformity, detailed records of testing procedures and outcomes, regulatory compliance reports, and documentation outlining the importer's due diligence processes. These artifacts serve to demonstrate the importer's commitment to ensuring that AI systems are safe, reliable, and compliant with all relevant regulations before market entry. | Impact Explanation: Conformity Assurance Plan: A document outlining the importer's plan for ensuring the conformity of AI systems. \n\nImporter Compliance Records: Documents confirming the compliance status of AI systems imported by the organization."
    },
    {
        "control_name": "External Model Dependency Assessment",
        "category": "Third-Party AI Components & Data Source Governance",
        "description": "Evaluate, assess and document third-party models for vulnerabilities before integration to ensure that third-party models are reliable, secure and compliant with ethical and legal standards.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-11: Developer Testing and Evaluation\nRA-3: Risk Assessment\nSR-2: Supply Chain Risk Management Plan\nCM-4: Impact Analysis\nSI-2: Flaw Remediation\nRA-5: Vulnerability Monitoring and Scanning\nSA-4: Acquisition Process\nCM-2: Baseline Configuration\nPL-2: System Security and Privacy Plans, SCF: GOV-63: Governance of External AI Components - Develop governance mechanisms to manage the integration of third-party AI models, focusing on security and reliability.\nRSK-64: Risk Analysis for Third-Party Models - Conduct risk analysis for third-party AI models to determine their impact on overall system security.",
        "explainability": "The External Model Dependency Assessment control involves assessing and managing dependencies on external models within an AI system. The rationale for this control is to ensure that external models used in the system are reliable, secure, and compliant with ethical and legal standards, mitigating risk associated with external model dependencies.",
        "evidence": "Rationale Explanation: External Model Dependency Assessment Guidelines: A document outlining guidelines and criteria for assessing external model dependencies, including requirements for reliability, security, and compliance with ethical and legal standards. \n\nRationale for External Model Dependency Assessment: A report explaining the reasons for conducting assessments of external model dependencies, such as ensuring reliability, security, and compliance with ethical and legal standards and mitigating risk associated with external dependencies. \n\nDependency Assessment Reports: Documentation of the results of external model dependency assessments, including findings, compliance status, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of External Model Dependency Assessment for ensuring reliability, security, and compliance with ethical and legal standards, and mitigating risk associated with external dependencies. | Responsibility Explanation: Model Dependency Assessment Framework: A framework for evaluating third-party models. \n\nVulnerability Assessment Report Template: A template for documenting vulnerabilities in third-party models.\n \nSecurity Training for Integrators: Training materials for system integrators on assessing model dependencies. | Data Explanation: Assessment framework for third-party models.\n\n Vulnerability assessment reports. \n\nMitigation strategies and implementation records. | Fairness Explanation: Model Dependency Fairness Evaluation Guide: A guide for assessing third-party model dependencies with an emphasis on fairness. \n\nVulnerability Assessment Fairness Reports: Reports documenting the fairness implications of vulnerabilities in third-party models. \n\nExternal Model Fairness Assessment Checklists: Checklists used to systematically assess third-party models for fairness vulnerabilities. | Safety & Performance  Explanation: Detailed vulnerability assessment reports, documentation of third-party model testing procedures, risk analysis records, mitigation strategy documents, and records of any remedial actions taken in response to identified vulnerabilities. | Impact Explanation: Dependency Assessment Report: A detailed report assessing the impact and risk associated with external model dependencies. \n\nDependency Mitigation Plan: A document outlining strategies to address identified risk and ensure compliance."
    },
    {
        "control_name": "Supply Chain Verification",
        "category": "AI Supply Chain Integrity Assurance",
        "description": "Verify the integrity of the entire AI systems' model supply chain, from data providers to software dependencies.",
        "risk_level": "General",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behaviour that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: SR-2: Supply Chain Risk Management Plan\nRA-3: Risk Assessment\nCM-8: System Component Inventory\nSA-4: Acquisition Process\nCM-2: Baseline Configuration\nPM-9: Risk Management Strategy\nSA-10: Developer Configuration Management\nCA-2: Control Assessments\nSA-11: Developer Testing and Evaluation, SCF: GOV-73: Governance of ML Supply Chain - Establish governance mechanisms for managing and securing the ML supply chain.\nRSK-74: Risk Assessment for Software Dependencies - Carry out risk assessments specifically for software dependencies in the ML supply chain.",
        "explainability": "The Supply Chain Verification control involves verifying and ensuring the integrity and security of the AI system's supply chain. The rationale for this control is to mitigate risk associated with the compromise of components, data, or processes within the supply chain, ensuring the overall reliability and security of the AI system.",
        "evidence": "Rationale Explanation: Supply Chain Verification Guidelines: A document outlining guidelines and criteria for verifying the AI system's supply chain, including requirements for integrity, security, and risk mitigation. \n\nRationale for Supply Chain Verification: A report explaining the reasons for conducting supply chain verification, such as ensuring integrity, security, and risk mitigation within the supply chain and ensuring the overall reliability of the AI system. \n\nVerification Reports: Documentation of the results of supply chain verification activities, including findings, compliance status, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Supply Chain Verification for ensuring integrity, security, and risk mitigation within the supply chain, and ensuring the overall reliability of the AI system. | Responsibility Explanation: Supply Chain Integrity Protocol: A protocol for verifying the integrity of the machine learning supply chain. \n\nVerification Process Guide: A guide outlining the process for supply chain verification. \n\nSupply Chain Auditor Training Modules: Training modules for auditors on supply chain verification. | Data Explanation: Supply chain verification protocols. \n\nAudit trails for data provenance and software dependency checks. \n\nSupplier and third-party vendor assessments. | Fairness Explanation: Supply Chain Fairness Verification Framework: A comprehensive framework outlining the process for verifying the fairness of the machine learning supply chain. \n\nData Provider Fairness Audit Reports: Reports auditing data providers for fairness in data sourcing and handling. \n\nSoftware Dependency Fairness Checklists: Checklists to verify that software dependencies uphold fairness standards. | Safety & Performance  Explanation: Extensive supply chain maps that illustrate the flow of data and dependencies, comprehensive audits of all suppliers and partners involved in the supply chain, meticulous documentation of data custody chains, detailed assessments of all software dependencies including open-source components, and a clear record of all security measures implemented at each stage of the supply chain. This documentation not only proves compliance with the highest standards of data security and software integrity but also establishes a framework for ongoing supply chain risk management. | Impact Explanation: Supply Chain Verification Report: A comprehensive report detailing the outcomes of the verification process, including identified risk and mitigation strategies. \n\nSupplier Compliance Records: Documents confirming the compliance status of individual suppliers in the AI system's supply chain."
    },
    {
        "control_name": "Third-Party Trademark Verification",
        "category": "AI Intellectual Property Rights Management",
        "description": "Establish a mechanism to verify if third parties have placed their name or trademark on high-risk AI systems. This involves crossreferencing system branding with authorized trademarks and liaising with trademark owners when discrepancies are identified.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 25: Responsibilities Along the AI Value Chain, NIST 800-53: RA-5: Vulnerability Monitoring and Scanning\nSR-2: Supply Chain Risk Management Plan\nSR-3: Supply Chain Controls and Processes\nPM-15: Security and Privacy Groups and Associations\nAU-6: Audit Record Review, Analysis, and Reporting\nAC-1: Access Control Policy and Procedures\nCM-8: System Component Inventory\nPS-7: External Personnel Security\nPL-4: Rules of Behavior\nPM-9: Risk Management Strategy, SCF: GOV-83: Governance of Third-Party Trademarks in AI - Establish governance frameworks for the use of third-party trademarks in AI systems, including authorization processes and compliance checks.\nRSK-86: Risk Assessment for Trademark Infringement in AI - Perform risk assessments to identify and mitigate potential trademark infringements in AI systems.",
        "explainability": "The Third-Party Trademark Verification control involves verifying and confirming the legitimacy of third-party trademarks associated with the AI system. The rationale for this control is to ensure that the use of third-party trademarks complies with legal standards, mitigating risk related to trademark infringement and enhancing the credibility and legality of the AI system.",
        "evidence": "Rationale Explanation: Third-Party Trademark Verification Guidelines: A document outlining guidelines and criteria for verifying third-party trademarks associated with the AI system, including requirements for legality, legitimacy, and compliance with trademark standards.\n\nRationale for Third-Party Trademark Verification: A report explaining the reasons for conducting third-party trademark verification, such as ensuring legality, legitimacy, and compliance with trademark standards and mitigating risk related to trademark infringement. \n\nVerification Reports: Documentation of the results of third-party trademark verification activities, including findings, compliance status, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of Third-party Trademark Verification for ensuring legality, legitimacy, and compliance with trademark standards, and mitigating risk related to trademark infringement. | Responsibility Explanation: Trademark Verification Procedure: A procedure for verifying third-party trademarks on AI systems.\n \nTrademark Crossreference Database: A database for crossreferencing system branding with authorized trademarks. \n\nLegal Communication Protocol: A protocol for liaising with trademark owners regarding discrepancies. | Data Explanation: Trademark verification protocols. \n\nRecords of trademark ownership and authorization. \n\nReports of actions taken in case of unauthorized use. | Fairness Explanation: Trademark Verification Fairness Guidelines: Guidelines for verifying third-party trademarks, with fairness in marketing and branding as a key consideration. \n\nBranding Fairness Compliance Certificates: Certificates issued when AI systems are verified for proper use of third-party trademarks in a fair manner. \n\nTrademark Discrepancy Fairness Reports: Reports documenting any discrepancies in trademark use and the actions taken to ensure fairness. | Safety & Performance  Explanation: Trademark verification protocols, a registry of authorized trademarks, records of verification checks, correspondence with trademark owners, and reports of any unauthorized trademark usage found during the verification process. These artifacts demonstrate the diligent efforts taken to protect IP rights and ensure market integrity. | Impact Explanation: Trademark Verification Report: A detailed report confirming the proper use and compliance with third-party trademarks in the AI system. \n\nTrademark License Agreements: Documents outlining the terms and conditions of the licenses for third-party trademarks."
    },
    {
        "control_name": "AI System Classification for Elections",
        "category": "AI Model & Algorithm Transparency",
        "description": "Develop and document the AI system's model to ensure that the AI system is not designed to influence election outcomes or voting behaviors. If so, documentation and classification of the AI model needs to be performed.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behaviour that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: RA-3: Risk Assessment\nSR-2: Supply Chain Risk Management Plan\nPL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nCM-2: Baseline Configuration\nSA-11: Developer Testing and Evaluation\nRA-5: Vulnerability Monitoring and Scanning\nCA-2: Control Assessments\nSI-22: Information Diversity, SCF: GOV-91: Classification of Election-Influencing AI Systems - Establish criteria and procedures for classifying AI systems that may influence elections as high-risk, ensuring rigorous oversight and compliance with democratic integrity standards.\nRSK-94: Risk Management for Election AI Systems - Conduct risk assessments to evaluate the potential impact of AI systems on election integrity and implement mitigation strategies.",
        "explainability": "The AI System Classification for Elections control involves classifying AI systems used in election processes based on specific criteria. The rationale for this control is to ensure transparency, fairness, and the proper handling of AI systems in election contexts, preventing biases and safeguarding the integrity of electoral processes.",
        "evidence": "Rationale Explanation: AI System Classification Guidelines for Elections: A document outlining guidelines and criteria for classifying AI systems used in election processes, including requirements for transparency, fairness, and prevention of biases. \n\nRationale for AI System Classification for Elections: A report explaining the reasons for classifying AI systems used in election processes, such as ensuring transparency, fairness, and preventing biases, and safeguarding the integrity of electoral processes. \n\nClassification Reports: Documentation of the results of AI system classification for elections, including findings, compliance status, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI System Classification for Elections in ensuring transparency, fairness, and preventing biases, and safeguarding the integrity of electoral processes. | Responsibility Explanation: Election AI Risk Classification Guidelines: Guidelines for classifying election-influencing AI systems as high-risk. \n\nCompliance Checklist for Election AI Systems: A checklist ensuring election AI systems meet high-risk standards. \n\nRegulatory Training Modules: Training materials for regulators and compliance teams on AI system classification in elections. | Data Explanation: Classification guidelines for AI systems in electoral contexts.\n\nRisk assessment reports evaluating potential influence on elections. \n\nCompliance documentation for high-risk election-related AI systems. | Fairness Explanation: Election Influence AI Classification Framework: A framework for classifying AI systems based on their potential to influence elections, ensuring they are treated with the highest level of fairness scrutiny. \n\nHigh-Risk Classification Fairness Procedures: Procedures that ensure the process of classifying election-influencing AI systems as high risk is conducted fairly. \n\nElection Impact Fairness Review Reports: Reports that review the potential impact of AI systems on elections, focusing on maintaining fairness in electoral processes. | Safety & Performance  Explanation: Extensive classification frameworks detailing the specific criteria that qualify AI systems as high risk within the electoral domain, exhaustive risk profile analyses evaluating the potential for election interference, certificates of compliance with international and national electoral regulations, and transparent disclosure of the AI systems' objectives, functionalities, and operational parameters in relation to electoral activities. | Impact Explanation: Classification Report: A detailed report categorizing AI systems used in elections based on their impact, potential risk, and intended use. \n\nRisk Mitigation Plan: A document outlining strategies to mitigate identified risk associated with classified AI systems."
    },
    {
        "control_name": "AI System Classification for Education",
        "category": "AI Model & Algorithm Transparency",
        "description": "Develop and document the AI system's model to ensure that the AI system is not designed to impact educational and professional outcomes. If so, documentation and classification of the AI model needs to be performed.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Annex XI - Section 2 (2) - provide a detailed description of the measures put in place for the purpose of conducting internal and/or external adversarial testing (e.g., red teaming), model adaptations, including alignment and fine-tuning.\nArticle 15 (5) - Accuracy, robustness and cybersecurity\nArticle 55 1(a)- Obligations for providers of general-purpose AI models with systematic risk\nRecital 56: Emphasizes the need for proper design to minimize impact on educational or professional path of a person.\nRecital 75 - high risk systems should be resilient in relation to harmful or otherwise undesirable behaviour that may result from limitations within the systems or the environment in which the systems operate (e.g., errors, faults, inconsistencies, unexpected situations).\nRecital 76 - Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities., NIST 800-53: RA-3: Risk Assessment\nPL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nSA-11: Developer Testing and Evaluation\nCM-2: Baseline Configuration\nSR-3: Supply Chain Controls and Processes\nCA-2: Control Assessments\nRA-5: Vulnerability Monitoring and Scanning\nSI-22: Information Diversity, SCF: GOV-101: Classification of Educational AI Systems - Establish criteria and procedures for classifying AI systems used in education and vocational training as high-risk, ensuring they align with educational standards and ethical guidelines.\nRSK-104: Risk Management for Educational AI Systems - Conduct risk assessments to evaluate the potential impact of AI systems on educational equity and implement strategies to mitigate any identified risk.",
        "explainability": "The AI System Classification for Education control involves classifying AI systems used in educational contexts based on specific criteria. The rationale for this control is to ensure transparency, fairness, and the responsible use of AI systems in education, preventing biases and safeguarding the educational experience for users.",
        "evidence": "Rationale Explanation: AI System Classification Guidelines for Education: A document outlining guidelines and criteria for classifying AI systems used in educational contexts, including requirements for transparency, fairness, and responsible use. \n\nRationale for AI System Classification for Education: A report explaining the reasons for classifying AI systems used in education, such as ensuring transparency, fairness, and responsible use, preventing biases, and safeguarding the educational experience for users. \n\nClassification Reports: Documentation of the results of AI system classification for education, including findings, compliance status, and recommendations for improvement. \n\nRationale Document: A comprehensive nontechnical report justifying the importance of AI System Classification for Education in ensuring transparency, fairness, responsible use, and preventing biases, safeguarding the educational experience for users. | Responsibility Explanation: Education AI Risk Classification Policy: A policy for classifying educational AI systems as high risk.\n\nEducational Impact Assessment Template: A template for assessing the impact of AI systems on education. \n\nClassification Training for Educational Authorities: Training materials for educational authorities on AI risk classification. | Data Explanation: Criteria for categorizing educational AI as high risk. \n\nImpact analysis reports on learning and vocational training outcomes. \n\nRegulatory compliance records for educational AI applications. | Fairness Explanation: Educational Impact Fairness Report: A detailed analysis of the AI system's impact on educational equity. \n\nHigh-Risk Classification Guidelines: Guidelines that define the criteria for classifying educational AI as high risk.\n\nEducation AI Fairness Standards: Established standards that educational AI must meet to be deemed fair. | Safety & Performance  Explanation: Risk assessment guidelines tailored to educational AI applications, comprehensive impact analyses on student learning outcomes, documentation of algorithmic decision-making processes used in educational settings, and certification of adherence to educational standards and privacy laws relevant to student data. | Impact Explanation: Classification Report: A detailed report categorizing AI systems used in education based on their impact, potential risk, and intended use. \n\nEducational Impact Assessment: A document assessing the potential impact of AI systems on educational outcomes and stakeholders."
    },
    {
        "control_name": "Right to Explanation",
        "category": "AI Model & Algorithm Transparency",
        "description": "Explain decisions made by high-risk AI systems that significantly impact individuals.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nPL-4: Rules of Behavior\nPM-20: Dissemination of Privacy Program Information\nPM-21: Accounting of Disclosures\nPM-22: Personally Identifiable Information Quality Management\nRA-2: Security Categorization\nSA-11: Developer Testing and Evaluation\nAC-2: Account Management\nPS-7: External Personnel Security, SCF: RSK-210: Risk Assessment of AI Explanations - Evaluate the risk associated with not providing explanations for AI decisions and implement measures to mitigate these risk.",
        "explainability": "The Right to Explanation control involves providing individuals with a clear and understandable explanation of the decisions made by AI systems that impact them. The rationale for this control is to uphold transparency, accountability, and fairness, allowing individuals to understand the reasoning behind AI-generated decisions affecting them.",
        "evidence": "Rationale Explanation: Right to Explanation Policy: A document outlining the policy and guidelines for providing individuals with a right to explanation for AI-generated decisions, emphasizing clarity, understandability, and fairness. \n\nExplanation Reports: Documentation of the explanations provided for specific AI-generated decisions, including the factors considered, data used, and the decision-making process. \n\nCommunication Plan: A plan detailing how the organization will communicate the right to explanation to individuals, including channels, formats, and frequency. | Responsibility Explanation: Explanation Protocol for AI Decisions: A protocol outlining how explanations are provided to affected individuals. \n\nAI Decision Documentation Process: A process for documenting AI decisions for explanation purposes. \n\nTraining Program for Customer Service on AI Explanations: Training materials for customer service teams on providing AI decision explanations. | Data Explanation: Explanation protocols for AI system decisions. \n\nCommunication templates and guidelines for explaining AI decisions to affected individuals. \n\nRecords of explanations provided to individuals upon request. | Fairness Explanation: Explanation Right Protocols: Protocols outlining how affected individuals can receive explanations for AI decisions. \n\nDecision Explanation Documents: Documents provided to individuals that clearly explain AI decision making.\n\nImpact Explanation Guides: Guides that help stakeholders understand the significance of AI decisions on individuals. | Safety & Performance  Explanation: Explanation protocols outlining the process for delivering understandable AI decision-making explanations, communication templates for explaining AI decisions to affected individuals, records of explanations provided, and policy documents detailing the individual's right to understand AI decisions. | Impact Explanation: Explanation Mechanism Documentation: A document describing the mechanism used to generate explanations for AI system decisions. \n\nIndividual Explanation Reports: Reports providing specific explanations to individuals regarding the decisions made by the AI system."
    },
    {
        "control_name": "Transparency Facilitation for High-Risk AI",
        "category": "AI Model & Algorithm Transparency",
        "description": "Implement measures to facilitate transparency and access to high-risk AI systems for relevant stakeholders.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-1: Access Control Policy and Procedures\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nPL-4: Rules of Behavior\nPM-10: Authorization Process\nSA-4: Acquisition Process\nPM-15: Security and Privacy Groups and Associations\nSA-11: Developer Testing and Evaluation, SCF: RSK-310: Risk Assessment of AI Transparency - Evaluate the risk associated with the lack of transparency in high-risk AI systems and implement measures to mitigate these risk.",
        "explainability": "The Transparency Facilitation for High-Risk AI control involves facilitating transparency measures for high-risk AI systems, ensuring that stakeholders and the public can understand the AI's functioning, impacts, and potential risk. The rationale for this control is to promote openness, trust, and accountability in the deployment of high-risk AI systems.",
        "evidence": "Rationale Explanation: Transparency Facilitation Plan: A document outlining the plan and strategies for facilitating transparency in high-risk AI systems, including communication channels, disclosure mechanisms, and engagement with stakeholders. \n\nTransparency Reports: Documentation providing detailed information on the functioning, impacts, and potential risk of high-risk AI systems, presented in a clear and accessible manner. \n\nStakeholder Engagement Framework: A framework defining how stakeholders, including the public, will be engaged in the transparency process, including feedback mechanisms and response strategies. | Responsibility Explanation: Transparency Facilitation Plan: A plan detailing measures for facilitating AI system transparency. \n\nStakeholder Access Guidelines: Guidelines on providing stakeholders with access to AI system information. \n\nTransparency Training Program: Training materials for staff on implementing transparency measures. | Data Explanation: Transparency guidelines and access policies. \n\nDocumentation and explanatory materials made available to stakeholders. \n\nLogs of stakeholder inquiries and the information provided. | Fairness Explanation: Transparency Initiative Reports: Reports that describe measures taken to enhance AI system transparency. \n\nStakeholder Access Policies: Policies that ensure stakeholder access to AI system information. \n\nHigh-Risk AI Transparency Logs: Logs that record all transparency-facilitating activities for high-risk AI systems. | Safety & Performance  Explanation: Transparency reports, system documentation accessible to nontechnical stakeholders, user manuals explaining the AI's functionalities, governance frameworks detailing oversight mechanisms, and logs of AI decision-making processes. | Impact Explanation: Transparency Facilitation Plan: A document outlining the strategies and mechanisms in place to enhance transparency for high-risk AI systems. \n\nTransparency Impact Assessment: A report assessing the potential impact of transparency measures on stakeholders and the system's performance."
    },
    {
        "control_name": "AI Usage Disclosure",
        "category": "AI Model & Algorithm Transparency",
        "description": "Implement mechanisms to notify natural persons when they are interacting with or being evaluated by a high-risk AI system, detailing the system's purpose and decision-making criteria.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-8: System Use Notification\nPM-20: Dissemination of Privacy Program Information\nPM-21: Accounting of Disclosures\nPM-25: Minimization of Personally Identifiable Information Used in Testing, Training and Research\nPM-27: Privacy Reporting\nPM-31: Continuous Monitoring Strategy\nAU-6: Audit Record Review, Analysis, and Reporting\nPL-4: Rules of Behavior\nAC-2: Account Management\nPM-15: Security and Privacy Groups and Associations\nSA-11: Developer Testing and Evaluation\nRA-2: Security Categorization, SCF: RSK-413: Risk Assessment for AI Usage Disclosure - Evaluate the risk associated with inadequate AI usage disclosure and implement mitigation strategies.",
        "explainability": "The AI Usage Disclosure control involves disclosing information about the usage of AI systems, including purposes, scope, and potential impact on individuals and society. The rationale for this control is to promote transparency, inform users, and ensure the responsible and ethical use of AI.",
        "evidence": "Rationale Explanation: AI Usage Disclosure Policy: A document outlining the policy and guidelines for disclosing information about the usage of AI systems, specifying the details to be included, such as purposes, scope, and potential impact. \n\nUsage Disclosure Statements: Documentation providing clear and accessible information about the usage of AI systems, presented in a format that is easily understandable by users and the public. \n\nCommunication Channels: Specifications for the channels through which AI usage disclosure information will be communicated, including online platforms, official documents, and user interfaces. | Responsibility Explanation: AI Usage Disclosure Protocol: A protocol for disclosing AI system usage to individuals.\n \nNotification Template for AI Interaction: A standardized template for notifying individuals about AI system interaction. \n\nCommunication Strategy for AI Usage Disclosure: A strategy for effectively communicating AI system usage and criteria. | Data Explanation: Disclosure procedures for AI interactions. \n\nInformational materials explaining the purpose and criteria of the AI system. \n\nRecords of disclosures made to users. | Fairness Explanation: Usage Disclosure Policies: Policies that outline when and how individuals are notified about AI interaction. \n\nUser Interaction Records: Records of each instance an individual is notified of interacting with AI. \n\nDisclosure Compliance Certificates: Certificates proving that AI usage disclosures comply with fairness and transparency standards. | Safety & Performance  Explanation: Notification protocols for AI interactions, clear and accessible explanations of the AI system's purpose and decision-making criteria, consent forms where applicable, and informational materials provided to users prior to AI interaction or evaluation. | Impact Explanation: Usage Disclosure Documents: Documents disclosing information about the purposes, contexts, and stakeholders involved in the use of AI systems. \n\nUsage Impact Assessment: An assessment report evaluating the potential impact of AI usage on individuals and society."
    },
    {
        "control_name": "AI System Interpretability",
        "category": "AI Model & Algorithm Transparency",
        "description": "Implement mechanisms within high-risk AI systems that enhance system transparency and interpretability. This includes visualizations, decision trees, or other means to explain system logic and decision making.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-11: Developer Testing and Evaluation\nRA-3: Risk Assessment\nPL-4: Rules of Behavior\nSA-4: Acquisition Process\nPM-18: Privacy Program Plan\nAC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-15: Security and Privacy Groups and Associations, SCF: RSK-513: Risk Assessment for AI Interpretability - Assess risk associated with lack of interpretability in AI systems and implement countermeasures.",
        "explainability": "The AI System Interpretability control involves ensuring that AI systems are interpretable, meaning that their decisions and functioning can be understood and explained by relevant stakeholders. The rationale for this control is to enhance transparency, accountability, and trust in AI applications.",
        "evidence": "Rationale Explanation: Interpretability Guidelines: A document outlining guidelines and criteria for ensuring the interpretability of AI systems, including details on model transparency, decision processes, and communication mechanisms.\n \nInterpretability Reports: Documentation providing insights into the interpretability of AI systems, including explanations of decisions, model architecture, and factors influencing outcomes. \n\nTraining Material: Educational material for relevant stakeholders, such as users and decision-makers, to understand how to interpret and make sense of AI system outputs. | Responsibility Explanation: Interpretability Mechanism Implementation Plan: A plan outlining how interpretability mechanisms will be integrated into AI systems. \n\nSystem Interpretability Guides: Guides explaining the interpretability features of AI systems. \n\nInterpretability Training for Developers: Training materials for AI developers on creating interpretable systems. | Data Explanation: Tools and features that enhance system transparency. \n\nUser guides and explanatory materials on system interpretability. \n\nExamples of system logic visualizations and decision pathways. | Fairness Explanation: Interpretability Framework Documentation: Documents that outline the AI system's interpretability features. \n\nSystem Logic Visualizations: Visual tools that illustrate how the AI system processes information and makes decisions. \n\nDecision-Making Tree Guides: Guides that explain the logic and decision-making pathways of AI systems. | Safety & Performance  Explanation: Interpretability framework documents, user interface designs featuring visualizations of AI processes, examples of decision trees or other simplified model outputs used to explain decision making, and documentation or guides designed to help stakeholders understand the AI system's logic. | Impact Explanation: Interpretability Design Documentation: Documents detailing the design principles and methods used to enhance the interpretability of the AI system. \n\nInterpretability Testing Results: Reports summarizing the outcomes of testing the interpretability of the AI system."
    },
    {
        "control_name": "Deployer Responsibility Framework",
        "category": "Governance, Strategy, & Accountability",
        "description": "Outline the responsibilities of deployers of high-risk AI systems, emphasizing the protection of fundamental rights, impact assessments, stakeholder engagement, and timely notifications to authorities.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: PL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nPM-18: Privacy Program Plan\nPM-22: Personally Identifiable Information Quality Management\nAC-2: Account Management\nPM-15: Security and Privacy Groups and Associations\nRA-3: Risk Assessment\nPM-1: Information Security Program Plan\nAU-6: Audit Record Review, Analysis, and Reporting\nSI-22: Information Diversity, SCF: MON-613: Monitoring Deployer Actions - Continuously monitor the actions of deployers for adherence to prescribed frameworks.\nGOV-601: Deployer Rights Protection - Ensure deployers are responsible for protecting the fundamental rights affected by AI systems.\nGOV-602: Impact Assessment Compliance - Mandate deployers to conduct thorough impact assessments on high-risk AI systems.\nRSK-607: Risk Management by Deployers - Obligate deployers to manage and mitigate risk associated with AI systems.",
        "explainability": "The Deployer Responsibility Framework control involves establishing a framework outlining the responsibilities of those deploying AI systems, including decision-makers, operators, and administrators. The rationale for this control is to ensure accountability, ethical deployment, and proper management of AI systems throughout their life cycle.",
        "evidence": "Rationale Explanation: Responsibility Framework Document: A comprehensive document outlining the framework for deployer responsibilities in AI system deployment, including decision making, operational aspects, and ongoing management. \n\nTraining Materials: Educational materials for deployers, decision-makers, operators, and administrators, explaining their roles and responsibilities in the deployment process. \n\nAccountability Reports: Documentation providing insights into how deployers are fulfilling their responsibilities, including adherence to ethical guidelines and compliance with legal standards. | Responsibility Explanation: Deployer Responsibility Policy: A policy outlining the responsibilities of AI system deployers. \n\nStakeholder Engagement Plan: A plan for engaging stakeholders in the deployment process. \n\nFundamental Rights Protection Guidelines: Guidelines for deployers on protecting fundamental rights. | Data Explanation: Responsibility framework documents for AI system deployers.\n\nGuidelines on conducting impact assessments. \n\nProtocols for stakeholder engagement and authority notification. | Fairness Explanation: Deployer Responsibility Chart: A chart detailing deployer responsibilities with respect to fairness in AI system operation. \n\nFundamental Rights Protection Plans: Plans that illustrate how deployers will protect fundamental rights when using AI. \n\nStakeholder Engagement Records: Records of how deployers engage with stakeholders to assess and improve the fairness of AI systems. | Safety & Performance  Explanation: Responsibility framework documents outlining deployer obligations, impact assessment reports, stakeholder engagement plans and records, and protocols for timely authority notifications. These artifacts are meant to document and guide the deployers' commitment to ethical AI deployment and operational transparency. | Impact Explanation: Responsibility Framework Document: A document outlining the responsibilities, ethical considerations, and guidelines for those involved in deploying AI systems. \n\nDeployment Compliance Records: Documents confirming adherence to the deployer responsibility framework."
    },
    {
        "control_name": "Establish a Regulatory Liaison",
        "category": "Governance, Strategy, & Accountability",
        "description": "Implement procedures for regular engagement and liaison with the national supervisory authority to ensure ongoing compliance with regulations, especially for high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: PM-15: Security and Privacy Groups and Associations\nCA-2: Control Assessments\nPM-9: Risk Management Strategy\nAU-6: Audit Record Review, Analysis, and Reporting\nPL-2: System Security and Privacy Plans\nPM-18: Privacy Program Plan\nPM-22: Personally Identifiable Information Quality Management\nAC-2: Account Management\nPM-1: Information Security Program Plan\nRA-3: Risk Assessment, SCF: MON-712: Monitoring Regulatory Changes - Continuously monitor regulatory changes and communicate their impact to authorities.\nGOV-701: Regulatory Engagement Framework - Create a framework for ongoing engagement with regulatory bodies for AI compliance.\nRSK-706: Risk Management Reporting - Regularly report AI system risk management activities to supervisory authorities.",
        "explainability": "The Establish a Regulatory Liaison control involves appointing a designated individual or team to serve as a liaison between the organization deploying AI systems and relevant regulatory authorities. The rationale for this control is to facilitate communication, compliance, and collaboration with regulatory bodies overseeing AI-related activities.",
        "evidence": "Rationale Explanation: Regulatory Liaison Appointment Document: A document officially appointing the regulatory liaison and outlining their roles, responsibilities, and reporting mechanisms. \n\nCommunication Protocol: Guidelines specifying how the regulatory liaison will communicate with regulatory authorities, including reporting mechanisms, response timelines, and information-sharing protocols. \n\nCompliance Reports: Documentation providing insights into the organization's compliance with regulatory requirements, submitted to regulatory authorities through the regulatory liaison. | Responsibility Explanation: Regulatory Liaison Procedure: A procedure for regular engagement with supervisory authorities. \n\nCompliance Reporting Template: A template for reporting compliance to supervisory authorities.\n \nRegulatory Liaison Training Program: Training materials for staff on effective regulatory liaison. | Data Explanation: Protocols for engagement with supervisory authorities. \n\nRecords of correspondence and meetings with regulatory bodies. \n\nReports on compliance status and regulatory updates. | Fairness Explanation: Liaison Engagement Reports: Documentation of interactions with regulatory bodies, emphasizing discussions on fairness. \n\nCompliance Process Maps: Visual representations of procedures for maintaining regular regulatory engagement with a focus on fairness. \n\nRegulatory Liaison Fairness Protocols: Protocols that outline the steps for ensuring fairness through regular engagements with supervisory authorities. | Safety & Performance  Explanation: Job descriptions for regulatory liaison roles, records of communications with supervisory authorities, reports submitted to authorities, guidance documents received, and logs of regulatory advisories and actions taken in response. | Impact Explanation: Regulatory Liaison Designation: Document confirming the designation of a regulatory liaison for AI-related matters. \n\nCommunication Protocols: Documentation outlining the protocols for communication between the organization and regulatory bodies."
    },
    {
        "control_name": "Periodic AI Regulation Review",
        "category": "Policy, Values, & Scope",
        "description": "Conduct periodic reviews of internal AI regulations and guidelines. Focus on areas like prohibited practices, transparency mandates, and areas deemed high risk to ensure they remain relevant and up to date.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Annex VI: Conformity Assessment Procedure Based on Internal Control, NIST 800-53: CA-7: Continuous Monitoring\nRA-3: Risk Assessment\nPM-9: Risk Management Strategy\nPM-18: Privacy Program Plan\nPL-2: System Security and Privacy Plans\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-1: Information Security Program Plan\nSA-11: Developer Testing and Evaluation\nSI-22: Information Diversity, SCF: MON-807: Monitoring Prohibited AI Practices - Monitor and review prohibited AI practices to ensure they are effectively enforced.\nGOV-802: AI Governance Framework Evaluation - Evaluate and update the AI governance framework to align with evolving regulations.\nRSK-808: Risk Assessment of AI Regulations - Conduct risk assessments to determine the effectiveness of AI regulations.",
        "explainability": "The Periodic AI Regulation Review control involves conducting regular reviews of relevant AI regulations to ensure ongoing compliance and alignment with evolving legal and ethical standards. The rationale for this control is to stay informed, adapt to changes, and proactively address any regulatory updates impacting AI system deployment.",
        "evidence": "Rationale Explanation: Regulation Review Schedule: A document outlining the schedule for periodic reviews of AI regulations, specifying timelines, responsible parties, and key focus areas. \n\nCompliance Status Reports: Documentation providing insights into the organization's compliance with AI regulations, including updates on any changes made to ensure alignment with legal and ethical standards. \n\nRecommendations for Compliance Enhancement: A report outlining recommendations for enhancing compliance with AI regulations, based on the outcomes of the periodic reviews. | Responsibility Explanation: AI Regulation Review Schedule: A schedule for periodic reviews of AI regulations. \n\nRegulation Update Report: A report documenting updates to AI regulations. \n\nLegal Advisory Sessions on AI Regulations: Sessions with legal advisors to discuss updates to AI regulations. | Data Explanation: Review schedules and procedures for AI regulations. \n\nUpdates to AI policies and guidelines post review. \n\nDocumentation of policy changes and stakeholder notifications. | Fairness Explanation: Regulation Review Fairness Checklists: Checklists that guide the periodic review of AI regulations with an emphasis on fairness. \n\nAI Guidelines Update Logs: Logs that document updates to internal guidelines, highlighting changes made to enhance fairness. \n\nProhibited Practices Fairness Reviews: Detailed reviews of internal prohibited practices to ensure they are fair and nondiscriminatory. | Safety & Performance  Explanation: Schedule of regulation review cycles, updated AI regulatory documents, change logs reflecting amendments to policies, summary reports of review findings, and communications issued to stakeholders about changes in AI regulations and guidelines. | Impact Explanation: Regulation Review Reports: Periodic reports summarizing the results of the AI regulation review, including identified areas for improvement. \n\nUpdated Compliance Documentation: Documents reflecting any changes or updates made to ensure continued compliance with AI regulations."
    },
    {
        "control_name": "National Supervisory Authority Role Definition",
        "category": "Roles, Responsibilities, & Stakeholder Engagement",
        "description": "Clearly define and communicate the roles, responsibilities, and authority of the national supervisory body, particularly concerning oversight of high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems\nArticle 70: Designation of National Competent Authorities and Single Point of Contact, NIST 800-53: PM-1: Information Security Program Plan\nAC-1: Access Control Policy and Procedures\nPM-9: Risk Management Strategy\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-18: Privacy Program Plan\nPL-2: System Security and Privacy Plans\nPM-15: Security and Privacy Groups and Associations\nPS-7: External Personnel Security\nRA-3: Risk Assessment, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Develop governance structures for AI and autonomous technologies, including roles and responsibilities.\nAAT-01.2: Trustworthy AI & Autonomous Technologies - Ensure that AI and autonomous technologies are developed and used in a trustworthy manner, with clear accountability.\nAAT-02.2: AI & Autonomous Technologies Internal Controls - Implement internal controls for AI and autonomous technologies to ensure responsible use and oversight.\nGOV-01: Cybersecurity & Data Protection Governance Program - Establish a governance program that defines roles, responsibilities, and processes for cybersecurity and data protection.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Define and assign specific cybersecurity and data protection responsibilities.",
        "explainability": "The National Supervisory Authority Role Definition control involves clearly defining the roles and responsibilities of the National Supervisory Authority overseeing AI-related activities within a country. The rationale for this control is to establish a structured framework for the authority's functions, fostering effective oversight and regulatory compliance.",
        "evidence": "Rationale Explanation: Role Definition Document: A comprehensive document outlining the roles, responsibilities, and powers of the national supervisory authority in overseeing AI-related activities, including legal mandates, decision-making authority, and collaboration mechanisms.\n \nCommunication Framework: Guidelines specifying how the national supervisory authority will communicate with AI stakeholders, government bodies, and the public, ensuring transparency and accountability. \n\nAnnual Reports: Documentation providing insights into the national supervisory authority's activities, achievements, challenges, and recommendations, submitted to relevant stakeholders and the public. | Responsibility Explanation: Authority Role Definition Document: A document defining the roles and responsibilities of the supervisory authority. \n\nSupervisory Authority Communication Plan: A plan for communicating the authority's role to relevant stakeholders.\n \nOversight Procedure Manual: A manual detailing the oversight procedures of the supervisory authority. | Data Explanation: Official documents outlining the supervisory authority's roles and powers. \n\nGuidance material for AI system providers on interacting with the supervisory authority. \n\nPublic communications detailing the authority's oversight activities. | Fairness Explanation: Authority Role Fairness Frameworks: Frameworks that define the national supervisory authority's roles in overseeing AI fairness. \n\nResponsibility Outline Documents: Documents that clearly communicate the supervisory authority's responsibilities regarding AI fairness. \n\nOversight Authority Fairness Charters: Charters that establish the authority's commitment to upholding fairness in high-risk AI system oversight. | Safety & Performance  Explanation: Official documents defining the supervisory authority's mandate, explanatory materials outlining the authority's role in AI system oversight, procedural guidelines for enforcement actions, and communication plans for interacting with AI system stakeholders. | Impact Explanation: Role Definition Document: A document outlining the specific responsibilities, authorities, and processes for the national supervisory authority in regulating and overseeing AI systems. \n\nOversight Reports: Periodic reports summarizing the outcomes of the national supervisory authority's oversight activities."
    },
    {
        "control_name": "EU Database for High-Risk AI",
        "category": "Stakeholder Feedback Mechanisms",
        "description": "Ensure high-risk AI systems are registered if required by local authorities in collaboration with the appropriate stakeholders (e.g., member states, the EU Commission).",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 49: Registration\nArticle 71: EU Database for High-Risk AI Systems Listed in Annex III, NIST 800-53: PM-15: Security and Privacy Groups and Associations\nCM-8: System Component Inventory\nSA-4: Acquisition Process\nPM-5: System Documentation\nPL-2: System Security and Privacy Plans\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-1: Information Security Program Plan\nAC-2: Account Management\nSA-11: Developer Testing and Evaluation, SCF: AAT-01.3: AI & Autonomous Technologies Value Sustainment - Sustain the value of AI and autonomous technologies through ongoing stakeholder engagement and feedback mechanisms.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain situational awareness of AI and autonomous technologies, including their risk, benefits, and impacts.\nAAT-11.3: AI & Autonomous Technologies End User Feedback - Gather and analyze feedback from end-users and other stakeholders of AI and autonomous technologies.\nGOV-01: Cybersecurity & Data Protection Governance Program - Establish a governance program that defines roles, responsibilities, and processes for cybersecurity and data protection.\nGOV-03: Periodic Review & Update of Cybersecurity & Data Protection Program - Regularly review and update the cybersecurity and data protection program to reflect changes in technology, business operations, and emerging threats.\nGOV-05.2: Key Risk Indicators (KRIs) - Identify and monitor key risk indicators (KRIs) related to cybersecurity and data protection.",
        "explainability": "The EU Database for High-Risk AI control involves the establishment of a centralized database for high-risk AI systems within the European Union. The rationale for this control is to enhance transparency, regulatory oversight, and coordination among EU member states in monitoring and managing high-risk AI applications.",
        "evidence": "Rationale Explanation: Database Establishment Plan: A document outlining the plan for establishing the EU database for High-Risk AI, including technical specifications, data categories, and access controls. \n\nDatabase Access Protocols: Guidelines specifying who has access to the database, the criteria for access, and the procedures for submitting, updating, and retrieving information from the database. \n\nRegular Reporting Mechanism: A mechanism for regular reporting to the database, providing information on high-risk AI systems and their deployment, performance, and any incidents or issues encountered. | Responsibility Explanation: Database Development Plan: A plan for the development of the EU database for high-risk AI systems. \n\nDatabase Registration Guidelines: Guidelines for registering AI systems in the database. \n\nTraining Program for Database Management: Training materials for staff managing the database. | Data Explanation: Framework for the EU database. \n\nRegistration processes and requirements for high-risk AI systems. \n\nDatabase entries with detailed information on registered AI systems. | Fairness Explanation: Database Inclusion Fairness Criteria: Criteria that outline the fairness-related information required for AI systems to be included in the EU database. \n\nRegistration Transparency Reports: Reports that detail the registration process for high-risk AI systems, with an emphasis on fairness.\n \nDatabase Fairness Compliance Certificates: Certificates issued to AI systems that meet the database inclusion criteria, signifying compliance with fairness standards. | Safety & Performance  Explanation: Design and implementation plans for the database, registration protocols, records of registered high-risk AI systems, and public access policies. Additionally, maintenance and update logs of the database would serve as artifacts demonstrating its active management. | Impact Explanation: Database Establishment Documentation: A document outlining the procedures, criteria, and information included in the EU Database for High-Risk AI. \n\nDatabase Compliance Reports: Periodic reports summarizing the compliance status of high-risk AI systems listed in the database."
    },
    {
        "control_name": "Notification &  Decision Tracking",
        "category": "Stakeholder Feedback Mechanisms",
        "description": "Implement systems to track and record notifications, decisions, and areas associated with high-risk AI systems. Assign responsibility for annual reporting and set reminders for timely submission.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nCM-8: System Component Inventory\nPM-9: Risk Management Strategy\nPL-4: Rules of Behavior\nAC-2: Account Management\nPM-6: Measures of Performance\nAU-11: Audit Record Retention\nPM-1: Information Security Program Plan\nPM-18: Privacy Program Plan\nCA-7: Continuous Monitoring, SCF: AAT-01.3: AI & Autonomous Technologies Value Sustainment - Sustain the value of AI and autonomous technologies through ongoing monitoring, reporting, and stakeholder feedback mechanisms.\nAAT-11.2: AI & Autonomous Technologies Ongoing Assessments - Conduct ongoing assessments and track decisions regarding AI and autonomous technologies, ensuring timely reporting and feedback.\nGOV-01.2: Status Reporting To Governing Body - Implement systems for regular reporting to the governing body, including notifications and decisions related to high-risk AI systems.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Publish documentation and reports related to cybersecurity and data protection, including annual reports on high-risk AI systems.\nGOV-05: Measures of Performance - Define and track measures of performance for cybersecurity and data protection, including decision tracking for high-risk AI systems.",
        "explainability": "The Notification & Decision Tracking control involves establishing a system for notifying relevant stakeholders and tracking decisions made in the context of AI system deployment. The rationale for this control is to ensure transparency, accountability, and effective communication throughout the decision-making process.",
        "evidence": "Rationale Explanation: Notification Protocol: Guidelines outlining the procedures for notifying relevant stakeholders, including the timing, content, and recipients of notifications related to AI system deployment decisions. \n\nDecision Tracking System: A system or document for tracking decisions made during the AI system deployment process, including details on the decision, rationale, and individuals involved. \n\nStakeholder Engagement Plan: A plan specifying how stakeholders will be engaged and informed throughout the decision-making process, ensuring their input and feedback are considered. | Responsibility Explanation: Notification and Decision Tracking System: A system for tracking notifications and decisions. \n\nAnnual Reporting Procedure: A procedure for annual reporting on high-risk AI systems. \n\nReminder System for Timely Submission: A system to set reminders for timely submission of reports. | Data Explanation: Comprehensive Data Logs: Detailed records of all data inputs, outputs, and decision points within the AI system. \n\nAnnual AI Data Report: A yearly summary that outlines the datasets used, any changes in data handling, and the effects on AI decision making. \n\nData Lineage Documentation: Complete mapping from data source to decision output, demonstrating the data flow and transformation. | Fairness Explanation: Decision Tracking Fairness Protocols: Protocols for tracking AI decisions that include methods to ensure fairness in notifications and reporting. \n\nNotification Fairness Logs: Logs that document all notifications issued regarding AI decisions, with emphasis on fairness. \n\nAnnual Reporting Fairness Guidelines: Guidelines that ensure annual reporting on AI systems includes comprehensive fairness considerations. | Safety & Performance  Explanation: Tracking system implementation details, logs of notifications and decisions, assignment records indicating reporting responsibilities, annual report documents, and reminder system setups for report submissions. | Impact Explanation: Notification and Decision Tracking System Documentation: A document outlining the procedures and mechanisms for notifying and tracking decisions related to AI systems. \n\nDecision Tracking Records: Records documenting key decisions related to AI systems and their outcomes."
    },
    {
        "control_name": "Interagency Coordination for AI Oversight",
        "category": "Stakeholder Feedback Mechanisms",
        "description": "National supervisory authorities should coordinate their efforts with other relevant national bodies to ensure comprehensive oversight of high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: PM-15: Security and Privacy Groups and Associations\nPM-1: Information Security Program Plan\nCA-2: Control Assessments\nRA-3: Risk Assessment\nPL-2: System Security and Privacy Plans\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-9: Risk Management Strategy\nPM-18: Privacy Program Plan, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Implement governance structures for AI, involving coordination with other relevant national supervisory bodies.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain awareness of AI and autonomous technologies through coordination with other national agencies.\nAAT-11: Robust Stakeholder Engagement for AI & Autonomous Technologies - Engage with various stakeholders, including other national agencies, to ensure comprehensive oversight of AI systems.\nGOV-01: Cybersecurity & Data Protection Governance Program - Establish a governance program that includes coordination mechanisms among different agencies for overseeing high-risk AI systems.\nGOV-04.1: Stakeholder Accountability Structure - Develop a stakeholder accountability structure that ensures coordination among various national bodies for AI oversight.",
        "explainability": "The Interagency Coordination for AI Oversight control involves establishing mechanisms for collaboration and coordination among different regulatory agencies responsible for overseeing AI-related activities. The rationale for this control is to ensure comprehensive oversight, avoid duplication of efforts, and foster a unified approach to regulating AI.",
        "evidence": "Rationale Explanation: Coordination Framework: A document outlining the framework for interagency coordination in AI oversight, specifying roles, responsibilities, communication protocols, and collaboration mechanisms. \n\nJoint Oversight Plans: Collaborative plans developed by multiple regulatory agencies for overseeing specific aspects of AI-related activities, ensuring a unified and comprehensive approach.\n \nCommunication Channels: Established channels for effective communication and information sharing among regulatory agencies involved in AI oversight, promoting transparency and collaboration. | Responsibility Explanation: Coordination Framework Agreement: An agreement outlining the framework for interagency coordination. \n\nCoordination Meeting Schedule: A schedule for coordination meetings between agencies. \n\nOversight Coordination Training Modules: Training materials on effective interagency coordination for oversight purposes. | Data Explanation: Interagency Coordination Framework: A formal document outlining the roles and responsibilities of different supervisory authorities in AI oversight. \n\nJoint Oversight Protocols: Agreed-upon procedures for data sharing, incident reporting, and decision making between agencies. \n\nAI Governance Charter: A collective agreement that defines the principles and standards for AI data governance among the participating bodies. | Fairness Explanation: Coordination Fairness Action Plans: Plans that outline collaborative actions between agencies to oversee AI systems with a focus on fairness. \n\nInteragency Fairness Communication Protocols: Protocols that ensure clear and fair communication between agencies involved in AI oversight. \n\nOversight Synergy Fairness Reports: Reports that document the outcomes of inter-agency coordination efforts, focusing on their contribution to fairness. | Safety & Performance  Explanation: Coordination agreements between agencies, joint oversight frameworks, communication protocols, minutes from interagency meetings, and reports that encapsulate the collaborative efforts and decisions made regarding AI system oversight. | Impact Explanation: Coordination Framework Document: A document outlining the framework for interagency coordination in AI oversight, including roles, responsibilities, and communication protocols. \n\nCollaboration Reports: Periodic reports summarizing the outcomes of collaborative efforts among oversight agencies."
    },
    {
        "control_name": "Deployer Cooperation with National Authorities",
        "category": "Stakeholder Feedback Mechanisms",
        "description": "Deployers of high-risk AI systems must establish procedures to actively cooperate and engage with relevant national regulatory bodies.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems\nArticle 70: Designation of National Competent Authorities and Single Point of Contact, NIST 800-53: PM-15: Security and Privacy Groups and Associations\nPM-1: Information Security Program Plan\nPL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nAU-6: Audit Record Review, Analysis, and Reporting\nAC-2: Account Management\nRA-3: Risk Assessment\nPM-18: Privacy Program Plan\nSA-11: Developer Testing and Evaluation, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Develop governance structures for AI that require deployers to actively engage with national regulatory authorities.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain situational awareness of AI systems through active cooperation between deployers and national authorities.\nAAT-11.2: AI & Autonomous Technologies End User Feedback - Ensure deployers of AI systems establish procedures to engage and provide feedback to national regulatory bodies.\nGOV-01: Cybersecurity & Data Protection Governance Program - Establish a governance program that includes mechanisms for deployers of high-risk AI systems to cooperate with national regulatory bodies.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Define and assign responsibilities, including cooperation with regulatory bodies, for deployers of high-risk AI systems.",
        "explainability": "The Deployer Cooperation with National Authorities control involves establishing a framework for collaboration and cooperation between AI system deployers and national authorities. The rationale for this control is to facilitate information sharing, incident reporting, and overall collaboration to address regulatory compliance and potential issues related to AI system deployment.",
        "evidence": "Rationale Explanation: Cooperation Agreement: A formal document outlining the terms and conditions of cooperation between AI system deployers and national authorities, including the scope of cooperation, information sharing protocols, incident reporting procedures, and responsibilities of each party.\n \nIncident Reporting Protocols: Detailed protocols specifying the procedures for reporting incidents or issues related to AI system deployment to national authorities, ensuring timely and accurate information sharing. \n\nCompliance Reports: Documentation providing insights into the AI system deployer's compliance with regulatory requirements, submitted to national authorities on a regular basis. | Responsibility Explanation: Cooperation Procedure Manual: A manual detailing procedures for cooperation with national authorities. \n\nRegulatory Engagement Plan: A plan for regular engagement with regulatory bodies. \n\nCompliance Officer Training on Cooperation: Training materials for compliance officers on effective cooperation with authorities. | Data Explanation: Regulatory Engagement Plan: A detailed plan that defines how AI deployers will interact with regulatory bodies, including contacts, engagement schedules, and procedures. \n\nCompliance Documentation: Records and proof of adherence to national AI regulations and standards. \n\nIncident Response Protocol: A step-by-step guide that outlines the procedures for communicating with authorities in the event of an AI-related incident. | Fairness Explanation: Cooperation Framework Document: Outlines how deployers will interact with national authorities to ensure fairness. \n\nEngagement Records: Detailed records of all interactions with national authorities regarding fairness oversight. \n\nRegulatory Compliance Reports: Regular reports documenting compliance with national regulations related to fairness. | Safety & Performance  Explanation: Cooperation protocols and procedures, records of correspondence and engagements with regulatory bodies, documentation of compliance activities, and reports on interactions and resolutions of regulatory matters. | Impact Explanation: Cooperation Framework Document: A document outlining the framework for cooperation between deployers and national authorities, including reporting procedures and communication channels. \n\nCooperation Compliance Records: Documents confirming the deployer's adherence to the cooperation framework."
    },
    {
        "control_name": "Stakeholder Consultation for Delegated Acts",
        "category": "Stakeholder Feedback Mechanisms",
        "description": "Prior to adopting delegated acts for high-risk AI systems, the EU Commission should engage in consultations with the AI Office and relevant stakeholders.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 53: Obligations for Providers of General Purpose AI Models, NIST 800-53: PM-15: Security and Privacy Groups and Associations\nPM-1: Information Security Program Plan\nPL-2: System Security and Privacy Plan\nSA-2: Allocation of Resources\nRA-3: Risk Assessment\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-9: Risk Management Strategy\nPM-18: Privacy Program Plan, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Implement governance structures for AI that mandate stakeholder consultations in the process of adopting delegated acts.\nAAT-02: Situational Awareness of AI & Autonomous Technologies - Maintain awareness of AI systems through stakeholder consultations in legislative processes.\nAAT-11: Robust Stakeholder Engagement for AI & Autonomous Technologies - Engage with stakeholders, including the AI Office and relevant parties, in consultations for delegated acts concerning AI systems.\nGOV-01: Cybersecurity & Data Protection Governance Program - Establish a governance program that includes stakeholder consultation mechanisms, especially for adopting delegated acts concerning high-risk AI systems.\nGOV-04.1: Stakeholder Accountability Structure - Develop a stakeholder accountability structure that facilitates consultations with relevant parties before adopting delegated acts for AI systems.",
        "explainability": "The Stakeholder Consultation for Delegated Acts control involves incorporating stakeholder consultation in the decision-making process for delegated acts related to AI regulation. The rationale for this control is to ensure diverse perspectives are considered, promote transparency, and gather valuable insights from stakeholders in the development and implementation of delegated acts.",
        "evidence": "Rationale Explanation: Consultation Plan: A document outlining the plan for stakeholder consultation in the development of delegated acts related to AI regulation, specifying the scope, methods, and timelines for engaging with stakeholders. \n\nConsultation Reports: Documentation summarizing the outcomes of stakeholder consultations, including key insights, concerns raised, and recommendations provided by stakeholders. \n\nFeedback Integration Mechanism: Procedures for integrating stakeholder feedback into the decision-making process, ensuring that diverse perspectives are considered in the development of delegated acts. | Responsibility Explanation: Stakeholder Consultation Strategy: A strategy for engaging stakeholders in consultations. \n\nConsultation Feedback Collection System: A system for collecting feedback during stakeholder consultations. \n\nLegislative Advisory Sessions on Delegated Acts: Sessions with legislative advisors on the adoption of delegated acts. | Data Explanation: Consultation Process Documentation: A record outlining the process, timeline, and methods used for engaging with stakeholders. \n\nStakeholder Submission Summaries: Compilations of the feedback, comments, and recommendations received during the consultation phase. \n\nDelegated Acts Drafting Records: Documentation of the drafting process of delegated acts, reflecting the integration of stakeholder input. | Fairness Explanation: Consultation Procedure Manuals: Manuals describing the process for stakeholder consultations to inform delegated acts. \n\nStakeholder Feedback Summaries: Summaries capturing the range of stakeholder insights on fairness issues.\n\nDelegated Acts Fairness Analysis: Analysis of how stakeholder consultations have influenced the fairness of delegated acts. | Safety & Performance  Explanation: Consultation plans and schedules, lists of consulted stakeholders, summaries of feedback received, draft versions of the delegated acts, and documentation of how stakeholder input was incorporated into the final regulatory actions. | Impact Explanation: Stakeholder Consultation Plan: A document outlining the plan for engaging with stakeholders during the process of delegated acts in AI oversight. \n\nConsultation Reports: Reports summarizing the outcomes of stakeholder consultations, including feedback and considerations."
    },
    {
        "control_name": "Stakeholder Consultation Evidence",
        "category": "Stakeholder Feedback Mechanisms",
        "description": "Maintain records of consultations with the AI Office and other stakeholders during the preparation of delegated acts for high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 53: Obligations for Providers of General Purpose AI Models, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nAU-11: Audit Record Retention\nPM-15: Security and Privacy Groups and Associations\nPM-1: Information Security Program Plan\nPM-9: Risk Management Strategy\nPL-2: System Security and Privacy Plans\nAC-2: Account Management\nPM-18: Privacy Program Plan\nCM-8: System Component Inventory, SCF: AAT-01.3: AI & Autonomous Technologies Value Sustainment - Record and maintain evidence of stakeholder consultations to sustain the value and trustworthiness of AI systems.\nAAT-11.2: AI & Autonomous Technologies End User Feedback - Document and manage feedback and consultations from end-users and stakeholders during the preparation of delegated acts.\nAAT-11.4: AI & Autonomous Technologies Incident & Error Reporting - Establish procedures for recording stakeholder consultations as part of incident and error reporting in AI systems.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Document and publish records of stakeholder consultations in the process of developing delegated acts for high-risk AI systems.\nGOV-05.1: Key Performance Indicators (KPIs) - Develop and maintain key performance indicators (KPIs) for tracking and evaluating stakeholder consultations in AI governance.",
        "explainability": "The Stakeholder Consultation Evidence control involves providing evidence of stakeholder consultation in the development and implementation of AI-related policies or regulatory frameworks. The rationale for this control is to demonstrate transparency, accountability, and the inclusion of diverse perspectives in decision-making processes.",
        "evidence": "Rationale Explanation: Consultation Documentation: Records of stakeholder consultation activities, including meeting minutes, survey results, written feedback, and any other relevant documents capturing the input and perspectives of stakeholders.\n \nEvidence Compilation Report: A report summarizing the evidence of stakeholder consultation, highlighting key themes, concerns raised, and the integration of stakeholder input into AI-related policies or regulatory frameworks.\n \nCommunication Plan: A plan outlining how the evidence of stakeholder consultation will be communicated to relevant stakeholders and the public to ensure transparency and accountability. | Responsibility Explanation: Consultation Record-Keeping System: A system for maintaining records of stakeholder consultations. \n\nConsultation Documentation Process: A process detailing how consultation feedback and decisions are documented.\n \nRecord-Keeping Training Program: Training materials for staff responsible for consultation record keeping. | Data Explanation: Consultation Process Documentation: A record outlining the process, timeline, and methods used for engaging with stakeholders. \n\nStakeholder Submission Summaries: Compilations of the feedback, comments, and recommendations received during the consultation phase. \n\nDelegated Acts Drafting Records: Documentation of the drafting process of delegated acts, reflecting the integration of stakeholder input. | Fairness Explanation: Consultation Documentation Logs: Logs that detail every consultation session and its relevance to fairness. \n\nInfluential Feedback Records: Records of stakeholder feedback that had a significant influence on fairness considerations. \n\nConsultation-to-Act Traceability Files: Files that trace how consultations have directly influenced the formulation of acts regarding AI fairness. | Safety & Performance  Explanation: Record-keeping protocols, consultation meeting minutes, lists of stakeholders consulted, summaries of discussions and input received, and archives of all consultation-related communications and documents. | Impact Explanation: Consultation Evidence Collection: A process for systematically collecting evidence of stakeholder consultations, including meeting minutes, feedback forms, and other relevant documents. \n\nConsultation Summary Reports: Documents summarizing the key findings and outcomes of stakeholder consultations."
    },
    {
        "control_name": "Public Database Listing",
        "category": "Stakeholder Feedback Mechanisms",
        "description": "Establish a standardized process to list high-risk AI systems and foundation models in a public-access database, capturing essential details and compliance status.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 49: Registration\nArticle 71: EU Database for High-Risk AI Systems Listed in Annex III, NIST 800-53: CM-8: System Component Inventory\nPM-1: Information Security Program Plan\nPM-15: Security and Privacy Groups and Associations\nPL-2: System Security and Privacy Plans\nCA-2: Control Assessments\nPM-9: Risk Management Strategy\nAU-6: Audit Record Review, Analysis, and Reporting\nRA-3: Risk Assessment\nPM-18: Privacy Program Plan, SCF: AAT-01.3: AI & Autonomous Technologies Value Sustainment - Ensure that the public database reflects the sustained value and compliance of AI systems.\nAAT-02.2: AI & Autonomous Technologies Internal Controls - Implement internal controls to accurately list AI systems in the public database.\nAAT-04.3: AI & Autonomous Technologies Targeted Application Scope - Include information in the public database about the application scope and intended use of listed AI systems.\nAAT-11: Robust Stakeholder Engagement for AI & Autonomous Technologies - Engage with stakeholders to ensure the accuracy and completeness of the AI systems' listing in the public database.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Publish and maintain a public database of high-risk AI systems, including essential details and compliance status.",
        "explainability": "The Public Database Listing control involves the establishment of a public database listing AI systems, providing accessible information to the public about deployed AI systems. The rationale for this control is to enhance transparency, allow public scrutiny, and facilitate awareness about the presence and characteristics of AI systems in various domains.",
        "evidence": "Rationale Explanation: Database Inclusion Criteria: A document specifying the criteria for including AI systems in the public database, outlining the information to be disclosed, and ensuring relevance and accuracy. \n\nPublic Database Interface: An accessible interface for the public to search and retrieve information from the database, providing details about deployed AI systems, their purposes, and potential impacts. \n\nTransparency Reports: Periodic reports summarizing the data in the public database, highlighting trends, and providing insights into the overall landscape of AI systems deployment. | Responsibility Explanation: Database Listing Procedure: A procedure for listing AI systems in the public database. \n\nAI System Information Template: A template capturing essential details and compliance status of AI systems. \n\nDatabase Management Training Modules: Training materials for database managers on listing procedures. | Data Explanation: Public Database Design Document: Outline of the structure, features, and accessibility options for the database. \n\nAI System Registration Records: Entries for each AI system, including system details, operational purposes, and compliance information. \n\nCompliance Status Reports: Regularly updated documents reflecting the compliance status of listed AI systems. | Fairness Explanation: Public Listing Protocols: Protocols outlining the process for listing AI systems in the public database with fairness considerations. \n\nDatabase Inclusion Criteria: A set of criteria that ensures AI systems are listed in the database based on their fairness compliance. \n\nCompliance Status Reports: Reports on the compliance status of listed AI systems with respect to fairness standards. | Safety & Performance  Explanation: Protocols for database entry, detailed database entries for each high-risk AI system, a system for updating compliance status, and user guides for public access and interpretation of the database contents. | Impact Explanation: Database Listing Documentation: A document outlining the criteria, information, and update processes for the public database listing AI systems. \n\nPublic Database Records: Records containing information about listed AI systems, accessible to the public."
    },
    {
        "control_name": "Legal Expertise in Notifying Authorities",
        "category": "Public Engagement, Disclosure, & Reporting",
        "description": "Ensure notifying authorities employ personnel with specialized legal expertise in fundamental rights.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 70: Designation of National Competent Authorities and Single Point of Contact, NIST 800-53: PS-7: External Personnel Security\nAT-3: Role-Based Training\nPM-13: Security and Privacy Workforce\nPL-4: Rules of Behavior\nPM-18: Privacy Program Plan\nPM-1: Information Security Program Plan\nAC-2: Account Management\nPM-15: Security and Privacy Groups and Associations, SCF: AAT-01.1: AI & Autonomous Technologies-Related Legal Requirements Definition - Define and assign legal requirements related to AI technologies, ensuring personnel involved are well-versed in fundamental rights.\nGOV-04: Assigned Cybersecurity & Data Protection Responsibilities - Assign responsibilities to personnel with legal expertise in fundamental rights for the notification process.\nGOV-04.1: Stakeholder Accountability Structure - Establish a stakeholder accountability structure that includes legal experts in fundamental rights.\nGOV-06: Contacts With Authorities - Ensure that personnel engaging with authorities have the necessary legal expertise in fundamental rights.\nGOV-13: State-Sponsored Espionage - Include legal expertise in fundamental rights as part of the strategy to address state-sponsored espionage and compliance issues.",
        "explainability": "The Legal Expertise in Notifying Authorities control involves ensuring that AI system deployers possess legal expertise when notifying authorities about incidents or issues related to AI system deployment. The rationale for this control is to guarantee accurate and compliant communication with regulatory bodies, fostering transparency and adherence to legal requirements.",
        "evidence": "Rationale Explanation: Legal Expertise Guidelines: A document outlining the specific legal expertise requirements for AI system deployers when notifying authorities, including knowledge of relevant laws, regulations, and communication protocols. \n\nIncident Notification Records: Documentation demonstrating instances where legal expertise was applied in the notification process, ensuring compliance with legal requirements and effective communication with authorities. \n\nContinuous Legal Training Plan: A plan specifying how AI system deployers will maintain and update their legal expertise, ensuring ongoing compliance with evolving legal frameworks. | Responsibility Explanation: Legal Expertise Hiring Plan: A plan for hiring personnel with legal expertise. \n\nFundamental Rights Training Program: A training program for notifying authority personnel on fundamental rights. \n\nLegal Expertise Assessment Criteria: Criteria for assessing the legal expertise of personnel. | Data Explanation: Legal Expertise Requirement Policy: A policy document that outlines the need for legal expertise in fundamental rights within the authority. \n\nStaff Training Records: Documentation proving that personnel have received training in fundamental rights relevant to AI systems. \n\nExpert Consultation Reports: Records of consultations with legal experts on matters related to fundamental rights within AI applications. | Fairness Explanation: Legal Expertise Fairness Training Records: Records of specialized training in fundamental rights for notifying authority personnel. \n\nFairness Legal Framework Overviews: Documents providing an overview of the legal frameworks related to AI fairness that personnel should be familiar with. \n\nFundamental Rights Case Studies: Case studies used for training purposes that illustrate the application of fundamental rights in AI oversight. | Safety & Performance  Explanation: Job descriptions detailing the legal expertise requirements, records of personnel qualifications and training in fundamental rights law, and documentation of legal assessments performed in response to AI system notifications. | Impact Explanation: Legal Notification Protocol: A document outlining the procedures for legal notifications to authorities, including the involvement of legal experts. \n\nLegal Notification Records: Documents confirming the involvement of legal experts in the notification process."
    },
    {
        "control_name": "Clear AI System Information",
        "category": "Transparency & Communication",
        "description": "Require AI systems to inform users of their interaction with the AI system in a clear manner, outlining specific details on AI functionalities, human oversight, decision-making processes, and rights.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: AC-8: System Use Notification\nPL-4: Rules of Behavior\nPM-18: Privacy Program Plan\nPM-20: Dissemination of Privacy Program Information\nPM-21: Accounting of Disclosures\nPM-23: Data Governance Board\nPL-2: System Security and Privacy Plans\nRA-3: Risk Assessment\nAU-6: Audit Record Review, Analysis, and Reporting\nSA-11: Developer Testing and Evaluation\nAC-2: Account Management, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensure AI systems are trustworthy and transparent, providing users with clear information about AI interactions.\nAAT-03: AI & Autonomous Technologies Context Definition - Define the context in which AI systems operate, including clear communication about functionalities and user rights.\nAAT-04.4: AI & Autonomous Technologies Cost/Benefit Mapping - Map and communicate the specific details of AI functionalities, including the benefits and potential costs to users.\nAAT-14: AI & Autonomous Technologies Requirements Definitions - Require that AI systems have defined requirements for user interaction transparency and communication of decision-making processes.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Publish clear and accessible information about AI systems' functionalities, oversight, and decision-making processes.",
        "explainability": "The Clear AI System Information control involves providing transparent and easily understandable information about AI systems to users, stakeholders, and the public. The rationale for this control is to ensure clarity, promote informed decision making, and foster trust in AI systems.",
        "evidence": "Rationale Explanation: Transparency Guidelines: A document outlining the guidelines for providing clear and understandable information about AI systems, including key features, functionalities, limitations, and potential impacts. \n\nUser-Facing Documentation: Informational materials, such as user manuals, guides, and FAQs, designed to convey clear details about AI systems in a user-friendly manner. \n\nAccessibility Features: Implementation of features that enhance the accessibility of information, ensuring that it is easily understandable by a diverse audience, including those with varying levels of technical expertise. | Responsibility Explanation: User Information Protocol: A protocol for providing clear information to users. \n\nAI System Information Guide: A guide outlining the specifics to be communicated to users. \n\nUser Interface Design Standards: Standards for designing user interfaces that clearly communicate AI system information.\n | Data Explanation: User Information Guidelines: Guidelines detailing the type of information that should be provided to users regarding AI system interaction. \n\nFunctionality Descriptions: Clear and detailed descriptions of the AI system's functionalities. \n\nHuman Oversight Protocols: Documentation that explains how human oversight is integrated into the AI system's operations. Rights \n\nCommunication Strategy: A strategy to inform users of their rights in relation to the decisions made by the AI system. | Fairness Explanation: User Information Clarity Guidelines: Guidelines ensuring that AI systems provide clear information to users about interactions. \n\nDecision-Making Process Maps: Visual maps that detail the AI system’s decision-making process for user understanding. \n\nHuman Oversight Clarity Records: Records that specify the human oversight involved in AI systems to ensure transparency and fairness. | Safety & Performance  Explanation: Information disclosure guidelines for AI systems, user interface elements that clearly present AI information, documentation provided to users outlining AI system details, and records of user acknowledgments of having received and understood the AI system information. | Impact Explanation: Information Transparency Guidelines: A document outlining the guidelines and standards for providing clear information about AI systems. \n\nInformation Disclosures: Documents and interfaces presenting clear information about AI systems, their capabilities, and potential impact."
    },
    {
        "control_name": "AI Licensing Compliance Framework",
        "category": "Responsible AI Licensing",
        "description": "Create a comprehensive framework to manage compliance with licensing requirements for AI software and components, ensuring responsible use and distribution.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 13: Transparency and Provision of Information to Deployers, NIST 800-53: SA-4: Acquisition Process\nPL-2: System Security and Privacy Plans\nCM-2: Baseline Configuration\nCM-8: System Component Inventory\nSA-10: Developer Configuration Management\nPM-9: Risk Management Strategy\nSR-2: Supply Chain Risk Management Plan\nSR-4: Provenance\nPM-15: Security and Privacy Groups and Associations\nAU-6: Audit Record Review, Analysis, and Reporting, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Establish governance structures that include compliance management for AI software and component licensing.\nAAT-02.2: AI & Autonomous Technologies Internal Controls - Implement internal controls to ensure compliance with AI licensing requirements and responsible distribution.\nAAT-14: AI & Autonomous Technologies Requirements Definitions - Define and enforce requirements for AI software licensing, ensuring compliance and responsible use.\nAAT-14.2: AI & Autonomous Technologies Viability Decisions - Make viability decisions based on compliance with AI licensing frameworks and responsible usage guidelines.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Develop and publish a comprehensive framework for managing compliance with AI licensing requirements.",
        "explainability": "The AI Licensing Compliance Framework control involves establishing a framework to ensure compliance with licensing requirements for AI systems. The rationale for this control is to uphold legal and regulatory standards, protect intellectual property, and facilitate lawful use of AI technologies.",
        "evidence": "Rationale Explanation: Licensing Compliance Guidelines: A document outlining the guidelines and requirements for AI system licensing compliance, including legal obligations, permissible use cases, and conditions for licensing. \n\nCompliance Records: Documentation demonstrating adherence to licensing requirements, including records of licensing agreements, usage tracking, and any relevant communications with licensing authorities. \n\nTraining and Awareness Plan: A plan detailing how AI system users and stakeholders will be trained and made aware of licensing compliance requirements, ensuring ongoing awareness and adherence. | Responsibility Explanation: Licensing Compliance Framework: A framework outlining compliance with AI licensing requirements. \n\nCompliance Checklist for AI Licensing: A checklist ensuring compliance with AI software and component licensing. \n\nLicensing Training Program for Compliance Officers: Training materials for compliance officers on managing AI licensing compliance. | Data Explanation: Compliance Framework Document: A detailed document that defines the compliance requirements for AI software licensing. \n\nLicense Tracking System: A system for tracking the status of AI software licenses and any associated compliance metrics. \n\nDistribution Control Procedures: Procedures that govern the distribution of AI software to prevent unauthorized use. | Fairness Explanation: Licensing Framework Compliance Manuals: Manuals that detail compliance requirements for AI licensing with a focus on fairness. \n\nResponsible Use Guidelines: Guidelines that ensure AI software and components are used responsibly in a manner that upholds fairness.\n \nDistribution Compliance Checklists: Checklists for verifying that the distribution of AI components complies with licensing and fairness requirements. | Safety & Performance  Explanation: Licensing compliance policies, detailed checklists for license terms verification, records of licenses for all AI software and components in use, and documentation of compliance audits and corrective actions taken in cases of noncompliance. | Impact Explanation: Licensing Compliance Framework Document: A document outlining the procedures, criteria, and responsibilities for ensuring compliance with AI licensing requirements. \n\nLicensing Compliance Records: Documents confirming adherence to the licensing compliance framework."
    },
    {
        "control_name": "Worker Consultation Prior to AI Deployment",
        "category": "Human-in-the-Loop Mechanisms",
        "description": "Require deployers to consult with workers' representatives and inform all affected employees about the system's implications and functionalities before deploying a high-risk AI system.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: PL-4: Rules of Behavior\nAT-2: Literacy Training and Awareness\nAC-1: Access Control Policy and Procedures\nPM-13: Security and Privacy Workforce\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-15: Security and Privacy Groups and Associations\nPM-18: Privacy Program Plan, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensure AI systems are developed and deployed in a manner that is trustworthy and transparent, particularly in communicating with workers about the system’s implications.\nAAT-11.2: AI & Autonomous Technologies End User Feedback - Involve end users, including workers, in providing feedback prior to the deployment of high-risk AI systems.\nHRS-03: Roles & Responsibilities - Clearly define roles and responsibilities, including the involvement of workers' representatives in the consultation process prior to AI deployment.\nHRS-03.2: Competency Requirements for Security-Related Positions - Ensure that those responsible for deploying AI systems are competent in understanding the implications for workers and conducting consultations.\nHRS-05.1: Rules of Behavior - Establish rules of behavior or policies that mandate consultation with employees and their representatives before deploying AI systems.",
        "explainability": "The Worker Consultation Prior to AI Deployment control involves seeking input and feedback from workers before the deployment of AI systems in the workplace. The rationale for this control is to ensure the inclusion of workers' perspectives, address potential concerns, and enhance collaboration between AI technology and human workers.",
        "evidence": "Rationale Explanation: Consultation Plan: A document outlining the plan for worker consultation prior to AI deployment, specifying the methods, channels, and timelines for gathering input and feedback from workers. \n\nFeedback Compilation Report: Documentation summarizing the input and feedback received from workers during the consultation process, highlighting key themes, concerns, and suggestions.\n \nCommunication Strategy: A strategy detailing how the organization will communicate the outcomes of worker consultation, address concerns, and facilitate transparent communication between workers and AI deployment teams. | Responsibility Explanation: Worker Consultation Process Guidelines: Guidelines outlining the process for consulting with workers’ representatives. \n\nEmployee Information Session Plan: A plan for organizing information sessions for employees about the AI system. \n\nWorker Feedback Collection Mechanism: A mechanism for collecting feedback from workers during consultations. | Data Explanation: Consultation Policy and Procedure: Official policy documents that outline the process for worker consultation before AI system deployment. \n\nEmployee Information Sessions: Records and materials from informational sessions held for employees regarding the AI system. \n\nFeedback Documentation: Documentation of feedback and concerns raised by workers and their representatives during the consultation process. | Fairness Explanation: Worker Consultation Reports: Detailed reports of consultations with worker representatives regarding the AI system. \n\nEmployee Information Packets: Informational materials distributed to employees detailing the AI system's functionalities and fairness implications.\n \nConsultation Feedback Analysis: Analysis of feedback received from workers to ensure their perspectives are incorporated into fairness considerations. | Safety & Performance  Explanation: The deliverables would include a Risk Assessment Report, which outlines all potential safety hazards and performance bottlenecks identified during the predeployment phase, along with a Performance Benchmarking Report that measures the AI system's accuracy and reliability against industry standards. Additionally, a Worker Impact Statement may be provided, detailing how the system's deployment will affect workers' daily operations and safety. | Impact Explanation: Consultation Process Documentation: Detailed records of the consultation process with workers' representatives, including dates, participants, and topics discussed. \n\nEmployee Information Packs: Comprehensive information packages provided to employees, outlining the AI system's functionalities, potential impacts, and any changes to work processes. \n\nMeeting Minutes and Feedback Records: Documentation of meetings and feedback sessions with employees, capturing their responses and concerns."
    },
    {
        "control_name": "Worker Representation & AI Information Dissemination",
        "category": "Human-in-the-Loop Mechanisms",
        "description": "Implement measures to ensure worker representation in decisions related to high-risk AI systems. Additionally, ensure regular dissemination of information about such systems to all relevant employees.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: PL-4: Rules of Behavior\nAT-2: Literacy Training and Awareness\nPM-13: Security and Privacy Workforce\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-15: Security and Privacy Groups and Associations\nAC-1: Access Control Policy and Procedures\nPM-18: Privacy Program Plan\nSA-11: Developer Testing and Evaluation, SCF: AAT-11: Robust Stakeholder Engagement for AI & Autonomous Technologies - Engage a wide range of stakeholders, including workers, in AI system development and deployment processes.\nAAT-11.3: AI & Autonomous Technologies End User Feedback - Implement mechanisms for regular information dissemination to employees and gather their feedback on AI system usage and impacts.\nHRS-03: Roles & Responsibilities - Clearly define roles and responsibilities, ensuring worker representation in AI-related decision-making.\nHRS-03.1: User Awareness - Develop awareness programs for employees regarding high-risk AI systems, their functionalities, and impacts.\nHRS-05: Terms of Employment - Include terms in employment policies that ensure workers are informed and involved in AI system deployment and usage.",
        "explainability": "The Worker Representation & AI Information Dissemination control involves establishing mechanisms for worker representation in AI-related decision-making processes and ensuring effective dissemination of information about AI systems to workers. The rationale for this control is to empower workers, address concerns, and foster a collaborative and informed work environment.",
        "evidence": "Rationale Explanation: Worker Representation Guidelines: A document outlining guidelines for worker representation in AI-related decision-making processes, specifying the methods for selecting representatives and ensuring diverse and inclusive participation. \n\nInformation Dissemination Protocols: Protocols detailing how information about AI systems will be disseminated to workers, including communication channels, content, and frequency of updates. \n\nWorker Representation Forums: Establishing forums or committees for worker representation, providing a platform for workers to express concerns, seek clarification, and participate in discussions related to AI systems. | Responsibility Explanation: Worker Representation Policy: A policy ensuring worker representation in AI-related decisions. \n\nAI System Information Dissemination Plan: A plan for the regular dissemination of information on AI systems to employees. \n\nEmployee Communication Channels: Established channels for communicating AI system information to employees. | Data Explanation: Worker Representation Policy: A policy that specifies the mechanisms for worker involvement in AI-related decision making. \n\nAI System Information Dissemination Plan: A plan that outlines how and when information about AI systems will be shared with employees. \n\nDecision-Making Records: Documentation of decisions made regarding high-risk AI systems, including input from worker representatives. | Fairness Explanation: Representation Policy Documents: Documents outlining policies for worker representation in AI decision making.\n\nAI System Information Bulletins: Regular bulletins that update employees on AI systems and their potential impact. \n\nDissemination Fairness Logs: Logs that record the dissemination of AI system information to employees, noting adherence to fairness. | Safety & Performance  Explanation: Deliverables would include a Worker Representation Charter, which outlines the roles and rights of worker representatives in AI oversight and an AI Transparency Log, which records all AI system updates and the dissemination of this information to employees. Also, a Performance and Safety Feedback Loop Report, which details the responses and adjustments made from worker feedback regarding AI system performance and safety. | Impact Explanation: Worker Representation Policies: Detailed guidelines on how worker representation is integrated into AI-related decision-making processes. \n\nInformation Dissemination Plans: Structured plans outlining the regular communication of AI system information to employees, including content, frequency, and channels used. \n\nMeeting and Communication Logs: Records of meetings and communications involving worker representatives and the dissemination of AI system information."
    },
    {
        "control_name": "Automated AI Decision Notifications",
        "category": "Human-in-the-Loop Mechanisms",
        "description": "Implement automated notification systems to inform natural persons when decisions affecting them are made by high-risk AI systems. Maintain logs of these notifications for audit and review purposes.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: AC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nAU-11: Audit Record Retention\nCM-8: System Component Inventory\nPM-18: Privacy Program Plan\nPL-4: Rules of Behavior\nAC-2: Account Management\nRA-3: Risk Assessment\nPM-9: Risk Management Strategy, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensure that AI systems are trustworthy in providing timely notifications to affected individuals.\nAAT-07.3: AI & Autonomous Technologies Continuous Improvements - Integrate automated notifications into AI systems as part of continuous improvement efforts.\nAAT-11.4: AI & Autonomous Technologies Incident & Error Reporting - Maintain logs of automated AI decision notifications for incident and error reporting, and for audit and review purposes.\nMON-01: Continuous Monitoring - Establish continuous monitoring mechanisms, including automated notifications for decisions made by high-risk AI systems.\nMON-01.6: Host-Based Devices - Implement notification systems on host-based devices to alert natural persons of decisions made by AI systems.",
        "explainability": "The Automated AI Decision Notifications control involves implementing automated notifications to inform individuals about AI-driven decisions that impact them. The rationale for this control is to enhance transparency, provide individuals with insights into the decision-making process, and offer avenues for recourse or clarification.",
        "evidence": "Rationale Explanation: Notification System Implementation Plan: A document outlining the plan for implementing automated AI decision notifications, including the triggers for notifications, content, and timing. \n\nNotification Content Standards: Standards specifying the information to be included in AI decision notifications, ensuring clarity and relevance to the individuals affected. \n\nRecourse Mechanism Documentation: Documentation detailing the mechanisms available for individuals to seek recourse, clarification, or appeal in response to AI-driven decisions. | Responsibility Explanation: Automated Notification System: A system for automatically notifying individuals of AI decisions. \n\nNotification Log Maintenance Procedure: A procedure for maintaining logs of notifications. \n\nNotification System Audit Plan: A plan for auditing the notification system. | Data Explanation: Notification System Design: Detailed plans and technical specifications for the automated notification systems. \n\nNotification Logs: A complete log of all notifications sent to individuals affected by AI system decisions. \n\nAudit Trail Procedures: Established procedures for maintaining and reviewing logs to ensure proper documentation and compliance. | Fairness Explanation: Notification System Design Documents: Documents that describe the design and operation of the automated notification system for AI decisions. \n\nNotification Logs: Detailed logs that record every notification sent to individuals affected by AI decisions. \n\nNotification Fairness Audit Reports: Reports from audits conducted to assess the fairness of the automated notification process. | Safety & Performance  Explanation: Key deliverables would include an Automated Notification System Design Document, detailing the architecture and workflow of the notification system and an AI Decision Log, which chronologically records every decision made by the AI and the corresponding notifications sent out. | Impact Explanation: Notification System Protocols: Detailed guidelines on the operation of the automated notification system, including triggers for notifications and the type of information to be provided. \n\nNotification Logs: Comprehensive logs maintaining records of all notifications sent to individuals, including timestamps and the nature of the AI decision. \n\nSystem Performance Reports: Regular reports on the performance and reliability of the notification system, including any issues or failures and corrective actions taken."
    },
    {
        "control_name": "User Notification",
        "category": "Human-in-the-Loop Mechanisms",
        "description": "Implement automated notification mechanisms to inform affected individuals when they are being evaluated or impacted by decisions made by high-risk AI systems.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: AC-8: System Use Notification\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-18: Privacy Program Plan\nPM-20: Dissemination of Privacy Program Information\nPM-22: Personally Identifiable Information Quality Management\nPM-31: Continuous Monitoring Strategy\nPL-4: Rules of Behavior\nAC-2: Account Management\nAU-11: Audit Record Retention\nPM-9: Risk Management Strategy\nRA-3: Risk Assessment\nPM-15: Security and Privacy Groups and Associations, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensure AI systems are designed to be trustworthy, including providing timely and clear notifications to affected individuals.\nAAT-07.3: AI & Autonomous Technologies Continuous Improvements - Continuously improve AI systems to effectively notify users about evaluations and impacts.\nAAT-11.4: AI & Autonomous Technologies Incident & Error Reporting - Keep logs of user notifications for audit, incident, and error reporting purposes in AI systems.\nMON-01: Continuous Monitoring - Implement mechanisms for continuous monitoring that include automated user notifications for decisions made by high-risk AI systems.\nMON-01.11: Automated Response to Suspicious Events - Adapt systems to automatically notify individuals when they are evaluated or impacted by AI decisions.",
        "explainability": "The User Notification control involves the provision of notifications to users regarding the use of AI systems and any relevant information that impacts their experience or decisions. The rationale for this control is to enhance user awareness, foster transparency, and provide users with insights into the functioning of AI systems.",
        "evidence": "Rationale Explanation: Notification Policy: A document outlining the policy for user notifications, specifying the scenarios and events that trigger notifications, the content to be included, and the timing of notifications. \n\nUser-Facing Notification Interface: Implementation of a user-facing interface that displays notifications in a clear and understandable manner, ensuring that users can easily access and comprehend the information provided. \n\nContinuous User Education Plan: A plan detailing how users will be educated and informed about the purpose and significance of notifications, promoting awareness and understanding. | Responsibility Explanation: User Notification Mechanism: A mechanism for automatically notifying users affected by AI decisions. \n\nNotification Content Guidelines: Guidelines on the content of notifications to users. \n\nUser Feedback Collection System: A system for collecting feedback from users on notifications. | Data Explanation: Notification Mechanism Documentation: Technical and process documentation for the automated notification systems. \n\nUser Notification Records: Logs that record every instance where a user is notified of being evaluated or affected by an AI decision. \n\nNotification Compliance Protocols: Protocols to ensure that the notification process adheres to legal and ethical standards. | Fairness Explanation: User Notification Protocols: Detailed protocols outlining when and how users are notified about AI system interactions. \n\nUser Impact Alert Summaries: Summaries that capture the content and frequency of alerts sent to users. \n\nNotification Fairness Compliance Certificates: Certificates indicating that the AI system's user notification process meets fairness requirements. | Safety & Performance  Explanation: Artifacts would include a comprehensive User Notification System Specification Document, which provides a detailed description of the system's technical design, protocols for notification delivery, and privacy safeguards. Additionally, a Detailed Notification Audit Trail would be maintained, containing timestamps, user identifiers, and the nature of the AI decision or evaluation, ensuring traceability and accountability. | Impact Explanation: User Notification Protocols: Detailed guidelines and procedures for the automated notification system, specifying when and how users are notified about AI evaluations and decisions. \n\nNotification Delivery Records: Logs and records of all notifications sent to users, including details on the timing and content of each notification. \n\nSystem Operation Reports: Regular reports documenting the operation, effectiveness, and any challenges encountered by the notification system."
    },
    {
        "control_name": "Human Oversight Mechanism",
        "category": "Human-in-the-Loop Mechanisms",
        "description": "Design high-risk AI systems in a manner that allows for effective and proportionate human oversight. This might include alert systems, decision review mechanisms, or manual override capabilities.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: SI-4: System Monitoring\nAC-7: Unsuccessful Logon Attempts\nSA-11: Developer Testing and Evaluation\nPL-2: System Security and Privacy Plans\nPM-13: Security and Privacy Workforce\nAU-6: Audit Record Review, Analysis, and Reporting\nAC-2: Account Management\nCP-2: Contingency Plan\nSA-4: Acquisition Process, SCF: AAT-01.2: Trustworthy AI & Autonomous Technologies - Ensure that high-risk AI systems are designed with trustworthiness in mind, incorporating mechanisms for effective human oversight.\nAAT-07.2: AI & Autonomous Technologies Likelihood & Impact Risk Analysis - Perform risk analysis on AI systems, factoring in the effectiveness and proportionality of human oversight mechanisms.\nAAT-07.3: AI & Autonomous Technologies Continuous Improvements - Continuously refine AI systems to improve and maintain effective human oversight capabilities.\nAAT-10.3: AI TEVV Trustworthiness Demonstration - Demonstrate the trustworthiness of AI systems by showing their capability for human oversight, such as alert systems and manual overrides.\nAAT-16.6: AI & Autonomous Technologies Performance Changes - Monitor and evaluate the performance of AI systems, especially how changes impact human oversight functions.",
        "explainability": "The Human Oversight Mechanism control involves the establishment of mechanisms to ensure human oversight in AI decision-making processes. The rationale for this control is to address ethical concerns, provide accountability, and enable human intervention when needed in critical decisions made by AI systems.",
        "evidence": "Rationale Explanation: Oversight Framework: A document outlining the framework for human oversight in AI decision making, specifying the criteria for human intervention, the types of decisions subject to oversight, and the roles and responsibilities of human overseers. \n\nIncident Response Plan: Documentation detailing the plan for addressing incidents or situations where human intervention is required, including communication protocols, escalation procedures, and postincident analysis. \n\nTraining Program: Implementation of a training program to educate human overseers on the functioning of AI systems, ethical considerations, and the criteria for intervention, ensuring they are well-prepared for their roles. | Responsibility Explanation: Oversight Mechanism Design Document: A document outlining the design of the human oversight mechanism. \n\nOversight Procedure Manual: A manual detailing the procedures for human oversight. \n\nOversight Team Training Program: Training materials for the oversight team on managing and utilizing the oversight mechanism. | Data Explanation: Oversight Design Specifications: Detailed designs of the oversight mechanisms to be integrated into the AI system. \n\nHuman-AI Interaction Protocols: Defined protocols for how humans will interact with the AI system to monitor and review decisions. \n\nOverride System Documentation: Technical documentation for the systems that allow for manual intervention or override of AI decisions. | Fairness Explanation: Oversight Mechanism Design Framework: A framework outlining the design of human oversight mechanisms within AI systems to ensure fairness. \n\nManual Override Protocols: Protocols detailing when and how human operators can override AI decisions. \n\nHuman Oversight Fairness Training Manuals: Manuals that provide training for human operators on maintaining fairness when overseeing AI systems. | Safety & Performance  Explanation: Artifacts would encompass a Human Oversight Framework Document, outlining the structure and protocols for human interaction with the AI system and an Oversight Intervention Log, recording every instance of human intervention, the reasons, and the outcomes. Moreover, an Alert System Efficacy Report would provide analysis on the alert system's performance in notifying human overseers. | Impact Explanation: Oversight Mechanism Design Documents: Detailed descriptions of the human oversight mechanisms integrated into AI systems, including their functionalities and operational protocols. \n\nSystem Oversight Logs: Records of human interactions with the AI system, such as overrides, decision reviews, or alerts responses. \n\nTraining Manuals for Human Operators: Comprehensive guides for personnel responsible for overseeing AI system operations, detailing procedures and best practices."
    },
    {
        "control_name": "Workforce AI Transition Management",
        "category": "AI Impact on Workforce",
        "description": "Manage the transition and transformation of the workforce as AI systems are integrated, addressing training needs and potential job role evolutions.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 13: Transparency and Provision of Information to Deployers, NIST 800-53: AT-2: Literacy Training and Awareness\nAT-3: Role-Based Training\nPM-13: Security and Privacy Workforce\nPL-4: Rules of Behavior\nPM-1: Information Security Program Plan\nAC-2: Account Management\nPM-18: Privacy Program Plan, SCF: AAT-05: AI & Autonomous Technologies Training - Provide specific training related to AI systems to ensure the workforce is prepared for the transition and transformation.\nHRS-03.2: Competency Requirements for Security-Related Positions - Address competency and training needs for workforce as they transition to new roles involving AI systems.\nHRS-13: Identify Critical Skills & Gaps - Identify the evolving skill requirements and gaps in the workforce due to the integration of AI systems.\nHRS-13.1: Remediate Identified Skills Deficiencies - Implement training programs to address the skill deficiencies identified in the workforce due to AI integration.\nHRS-13.4: Perform Succession Planning - Plan for workforce succession and role evolutions as AI systems are integrated into the workplace.",
        "explainability": "The Workforce AI Transition Management control involves the development and implementation of a strategy to manage the transition of the workforce in response to the introduction of AI systems. The rationale for this control is to ensure a smooth transition, address potential impacts on employees, and provide support for upskilling and reskilling.",
        "evidence": "Rationale Explanation: Transition Plan: A document outlining the plan for managing the workforce transition in response to AI system implementation, specifying timelines, communication strategies, and support mechanisms for employees. \n\nSkill Assessment Framework: Development of a framework for assessing the skills of the existing workforce, identifying gaps, and determining the skills required for the successful integration of AI systems. \n\nTraining and Support Programs: Implementation of training and support programs to facilitate upskilling and reskilling of employees, ensuring they acquire the necessary skills for working alongside AI technologies. | Responsibility Explanation: Workforce Transition Plan: A plan detailing the steps for managing workforce transition due to AI integration. \n\nAI Training Program for Employees: A training program to equip employees with skills for evolving job roles. \n\nRole Evolution Tracking System: A system for tracking the evolution of job roles as AI is integrated. | Data Explanation: Transition Management Plan: A comprehensive plan that outlines strategies for workforce transformation including training programs and role adjustments. \n\nTraining Needs Analysis: An analysis document that identifies the skills gap and training requirements for employees affected by AI integration. \n\nRole Evolution Framework: A framework that outlines how existing job roles may evolve with the integration of AI and the support mechanisms in place. | Fairness Explanation: Transition Management Plans: Plans that outline how the workforce will be transitioned to work with AI systems, with fairness as a key concern. \n\nTraining Needs Analysis Reports: Reports analyzing the training needs of employees as AI systems are integrated. \n\nJob Evolution Fairness Frameworks: Frameworks that address how job roles may evolve due to AI integration and ensure these changes are managed fairly. | Safety & Performance  Explanation: Deliverables would include a Workforce Transition Plan, detailing timelines, training schedules, and role transformations and a Training Effectiveness Report, evaluating the success of training programs in equipping staff with the necessary skills. Additionally, a Job Evolution Roadmap would be provided, outlining anticipated changes to job roles and progression paths. | Impact Explanation: Transition Management Plans: Comprehensive plans detailing the steps for workforce transition, including timelines, training programs, and support mechanisms. \n\nTraining Program Documentation: Detailed outlines of training programs designed to upskill employees in response to AI integration. \n\nRole Evolution Analyses: Analyses of how specific job roles are expected to evolve with AI integration, including new competencies and responsibilities."
    },
    {
        "control_name": "Human-AI Collaboration Framework",
        "category": "Human-AI Collaboration Policies",
        "description": "Develop policies that govern the interaction between humans and AI systems, ensuring effective collaboration and safety in shared environments.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: PL-4: Rules of Behavior\nAT-2: Literacy Training and Awareness\nSA-11: Developer Testing and Evaluation\nPL-2: System Security and Privacy Plans\nPM-13: Security and Privacy Workforce\nAC-1: Access Control Policy and Procedures\nPM-18: Privacy Program Plan\nIA-1: Identification and Authentication Policy and Procedures\nRA-3: Risk Assessment\nSC-8: Transmission Confidentiality and Integrity\nSI-1: System and Information Integrity Policy and Procedures, SCF: AAT-01: Artificial Intelligence (AI) & Autonomous Technologies Governance - Establish governance structures that include policies for human-AI collaboration.\nAAT-01.2: Trustworthy AI & Autonomous Technologies - Develop AI systems to be trustworthy, ensuring their collaboration with humans is safe and effective.\nAAT-07: AI & Autonomous Technologies Risk Management Decisions - Incorporate human-AI collaboration considerations in risk management decisions for AI systems.\nHRS-03: Roles & Responsibilities - Define roles and responsibilities in policies governing human-AI interaction to ensure effective collaboration.\nHRS-05: Terms of Employment - Include in the terms of employment the expectations and guidelines for human-AI collaboration in the workplace.",
        "explainability": "The Human-AI Collaboration Framework control involves the establishment of a framework that outlines the principles and processes for effective collaboration between humans and AI systems. The rationale for this control is to promote synergy, define roles, and ensure that the strengths of both humans and AI are leveraged for optimal outcomes.",
        "evidence": "Rationale Explanation: Collaboration Principles Document: A document outlining the principles that govern the collaboration between humans and AI systems, emphasizing transparency, communication, and shared decision making.\n\nRole Definition Guidelines: Guidelines specifying the roles and responsibilities of humans and AI systems in various collaborative scenarios, ensuring clarity and avoiding ambiguity. \n\nContinuous Improvement Plan: Implementation of a plan for continuously assessing and improving the collaboration framework based on feedback, performance evaluations, and evolving technologies. | Responsibility Explanation: Collaboration Policy Document: A document outlining the policies for human-AI collaboration. \n\nSafety and Collaboration Guidelines: Guidelines ensuring safety and effectiveness in human-AI collaboration.\n \nCollaboration Framework Training Materials: Training materials for employees on the human-AI collaboration framework. | Data Explanation: Collaboration Policy Document: A document outlining the rules and guidelines for human-AI interactions to foster effective and safe collaboration. \n\nSafety Protocol for AI Interactions: Specific protocols designed to ensure the safety of humans when in a shared environment with AI systems. \n\nJoint Task Guidelines: Guidelines for tasks that are to be undertaken collaboratively by humans and AI systems detailing roles and responsibilities. | Fairness Explanation: Collaboration Policy Documents: Documents detailing the collaborative policies between humans and AI systems with fairness guidelines. \n\nSafety and Fairness Protocols: Protocols that ensure the human-AI interaction is safe and fair.\n \nFairness in Collaboration Reports: Reports evaluating the effectiveness of the collaboration framework in upholding fairness. | Safety & Performance  Explanation: Deliverables would include a Human-AI Collaboration Policy Document, which outlines the rules and ethical considerations of human-AI interaction and a Collaboration Protocol Manual that provides operational procedures for humans working alongside AI systems. A Training and Procedure Compliance Log would also be maintained to document adherence to the established guidelines. | Impact Explanation: Collaboration Framework Policies: Detailed guidelines outlining how humans and AI systems should interact, including role definitions and collaboration protocols. \n\nSafety and Best Practice Manuals: Documentation of safety measures and best practices for human-AI collaboration. \n\nTraining Modules for Human-AI Interaction: Educational materials and training modules designed to prepare employees for effective and safe collaboration with AI systems."
    },
    {
        "control_name": "API Access Control & Authentication",
        "category": "AI-Specific User Authentication",
        "description": "Implement stringent access controls and authentication mechanisms for AI system APIs.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity, NIST 800-53: AC-2: Account Management\nAC-3: Access Enforcement\nIA-2: Identification and Authentication (Organizational Users)\nIA-3: Device Identification and Authentication\nAC-17: Remote Access\nIA-5: Authenticator Management\nAC-6: Least Privilege\nAC-4: Information Flow Enforcement\nSA-11: Developer Testing and Evaluation, SCF: IAC-03: Identification & Authentication for Non-Organizational Users - Enforce stringent access controls and authentication for non-organizational users accessing ML APIs.\nIAC-06: Multi-Factor Authentication (MFA) - Implement MFA for securing access to ML APIs.\nIAC-06.2: Network Access to Non-Privileged Accounts - Control network access to non-privileged accounts, including those accessing ML APIs.\nIAC-10: Authenticator Management - Manage authenticators rigorously for API access, ensuring robust authentication mechanisms are in place.\nIAC-10.5: Protection of Authenticators - Protect the integrity and confidentiality of authenticators used for ML API access.",
        "explainability": "The API Access Control & Authentication control involves implementing measures to control and authenticate access to application programming interfaces (APIs) in an AI system. The rationale for this control is to ensure the security and integrity of the system by regulating who can access the API and verifying the authenticity of users or systems interacting with it.",
        "evidence": "Rationale Explanation: Access Control Policy: A document outlining the policy for controlling access to APIs, specifying the criteria for granting or denying access, and defining roles and permissions. \n\nAuthentication Mechanism Documentation: Documentation detailing the mechanisms employed to authenticate users or systems interacting with the API, ensuring secure and authorized access. \n\nLogging and Monitoring Plan: Implementation of a plan for logging and monitoring API access, recording relevant information for security analysis, and detecting and responding to unauthorized access attempts. | Responsibility Explanation: API Access Control Policy: A policy outlining access controls for machine learning APIs. \n\nAuthentication Mechanism Implementation Guide: A guide for implementing robust authentication mechanisms. \n\nAPI Security Training Program: A training program for IT staff on API access control and authentication. | Data Explanation: Access Control Policy: A comprehensive policy that outlines access permissions and levels for machine learning API endpoints. \n\nAuthentication Mechanism Specifications: Detailed technical specifications for the authentication mechanisms employed by machine learning APIs. \n\nAPI Usage Logs: Logs that record all access and transactions made through the machine learning API for monitoring and auditing purposes. | Fairness Explanation: Access Control Policy Documents: Documents outlining the access control policies for machine learning APIs with fairness safeguards. \n\nAuthentication Procedure Manuals: Manuals detailing authentication procedures to ensure fair and secure API access. \n\nAPI Fairness Access Logs: Logs recording all access attempts to machine learning APIs, including fairness-related access controls. | Safety & Performance  Explanation: Deliverables would include an API Access Policy, which specifies the criteria for granting access and an Authentication Protocols Document, detailing the security measures in place for verifying identities. An Access Control Log would record all access requests and responses, providing an audit trail for security checks. | Impact Explanation: Access Control Policy Documents: Detailed policies outlining the access control and authentication measures for machine learning APIs, including user roles and permissions. \n\nAuthentication Mechanism Descriptions: Documentation of the authentication protocols and mechanisms in place, such as two-factor authentication, API keys, and token-based access. \n\nAccess Logs: Detailed logs recording all access attempts and activities on the machine learning APIs, including successful and failed authentication events."
    },
    {
        "control_name": "Customer/Partner API Access Review",
        "category": "AI-Specific User Authentication",
        "description": "Periodically review and audit which customers or partners have access to model or service APIs.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity, NIST 800-53: AC-2: Account Management\nAC-4: Information Flow Enforcement\nAU-6: Audit Record Review, Analysis, and Reporting\nAU-12: Audit Record Generation\nCM-8: System Component Inventory\nCA-7: Continuous Monitoring\nAC-6: Least Privilege\nPM-15: Security and Privacy Groups and Associations\nAC-17: Remote Access, SCF: MON-01.12: Alert Threshold Tuning - Tune alert thresholds in monitoring systems to flag unusual or unauthorized API access by customers or partners.\nIAC-07: User Provisioning & De-Provisioning - Regularly review and manage the provisioning of API access to customers or partners.\nIAC-07.2: Termination of Employment - Include procedures for reviewing API access as part of the termination process for customer or partner relationships.\nIAC-09: Identifier Management (User Names) - Manage and review identifiers used by customers or partners for API access.\nIAC-15.1: Automated System Account Management - Implement automated systems to manage and periodically review customer/partner API account access.",
        "explainability": "The Customer/Partner API Access Review control involves conducting regular reviews of customer and partner application programming interface (API) access to ensure compliance with security policies and prevent unauthorized access. The rationale for this control is to maintain the integrity of the AI system, safeguard sensitive data, and address any potential risk associated with external API interactions.",
        "evidence": "Rationale Explanation: Access Review Policy: A document outlining the policy and procedures for conducting regular reviews of customer and partner API access, specifying the frequency, criteria for review, and the roles involved.\n \nAccess Review Reports: Generation of reports summarizing the findings of the API access reviews, including details on access permissions, user activities, and any identified anomalies.\n \nRemediation Plan: Development of a plan for addressing and remedying any issues or risk identified during the API access reviews, ensuring timely mitigation of potential security threats. | Responsibility Explanation: API Access Review Schedule: A schedule for periodic reviews of API access by customers or partners. \n\nAccess Review Report Template: A template for documenting the findings of API access reviews. \n\nCustomer/Partner Communication Protocol: A protocol for communicating with customers or partners about API access changes or issues. | Data Explanation: API Access Review Schedule: A document that specifies the frequency and scope of the access reviews and audits. \n\nAccess Review Reports: Detailed reports generated from each periodic review, documenting the entities with access and the level of access granted. \n\nPartner Access Agreements: Agreements or contracts that specify the terms of API access for partners or customers. | Fairness Explanation: API Access Review Schedules: Schedules outlining when and how API access reviews are conducted, with an emphasis on fairness. \n\nPartner Access Fairness Evaluation Reports: Reports evaluating the fairness of API access given to customers or partners. \n\nAPI Access Review Guidelines: Guidelines ensuring that API access reviews are conducted fairly and transparently. | Safety & Performance  Explanation: Artifacts should include a Customer/Partner Access Review Schedule outlining how often access reviews are conducted and an API Access Audit Report, which documents findings from each review period. Additionally, an Access Revocation Log would track the removal of access privileges from entities no longer authorized. | Impact Explanation: API Access Review Reports: Detailed reports from each review period, documenting which customers or partners have access to APIs and any changes since the last review. \n\nAccess Control Lists: Updated lists of customers or partners with authorized access to the APIs, including the scope and limitations of their access rights. \n\nAudit Logs: Records of all access to the APIs by customers or partners, including timestamps and activities performed."
    },
    {
        "control_name": "Breach Reporting Mechanism",
        "category": "Breach Response Protocols",
        "description": "Establish a transparent and easily accessible mechanism for breach reporting and provide protection to reporters.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 87: Reporting of Breaches and Protection of Reporting Persons, NIST 800-53: IR-6: Incident Reporting\nAC-2: Account Management\nAU-6: Audit Record Review, Analysis, and Reporting\nAC-8: System Use Notification\nPL-4: Rules of Behavior\nPM-16: Threat Awareness Program\nAU-11: Audit Record Retention\nPS-7: External Personnel Security\nPM-13: Security and Privacy Workforce, SCF: IRO-10: Incident Response Operations - Develop and implement operations for handling breaches, including a mechanism for reporting.\nIRO-10.1: Automated Reporting - Set up automated systems for the efficient reporting of breaches.\nIRO-10.2: Cyber Incident Reporting for Sensitive Data - Ensure that the breach reporting mechanism is especially sensitive to incidents involving data breaches.\nIRO-11: Incident Reporting Assistance - Provide assistance and support for breach reporting, ensuring the process is transparent and accessible.\nIRO-11.1: Automation Support of Availability of Information/Support - Use automation to support the availability and accessibility of breach reporting mechanisms, enhancing protection for reporters.",
        "explainability": "The Breach Reporting Mechanism control involves the establishment of a mechanism for promptly reporting and addressing security breaches or incidents related to AI systems. The rationale for this control is to ensure timely detection, response, and resolution of security incidents, minimizing potential damage and maintaining the trust and integrity of the AI system.",
        "evidence": "Rationale Explanation: Breach Reporting Policy: A document outlining the policy for reporting security breaches or incidents, specifying the criteria for identifying a breach, the timeline for reporting, and the roles and responsibilities of individuals involved.\n \nIncident Response Plan: Documentation detailing the plan for responding to security breaches, including communication protocols, escalation procedures, and postincident analysis. \n\nReporting Interface: Implementation of an interface or system for users and stakeholders to report security incidents, ensuring a streamlined and efficient reporting process. | Responsibility Explanation: Breach Reporting System: A system for reporting breaches transparently and easily. \n\nReporter Protection Policy: A policy providing protection to individuals reporting breaches. \n\nBreach Response Plan: A plan detailing the steps to be taken following a breach report. | Data Explanation: Breach Reporting Protocol: A document that outlines the steps for reporting a breach and the protections afforded to reporters. \n\nBreach Report Forms: Standardized forms or digital interfaces for submitting breach reports. \n\nReporter Protection Policies: Policies that provide assurances and protection for those who report breaches. | Fairness Explanation: Breach Reporting Procedures: Procedures that detail how to report breaches fairly and protectively. \n\nReporter Protection Policies: Policies designed to protect the identities and rights of those reporting breaches. \n\nBreach Report Fairness Review Standards: Standards that ensure breach reports are evaluated fairly and without prejudice. | Safety & Performance  Explanation: Deliverables would include a Breach Reporting Protocol Document, which details the steps for reporting breaches and a Reporter Protection Policy, outlining measures to ensure the anonymity and safety of reporters. An Incident Response Plan would also be integral, specifying the procedures for addressing reported breaches. | Impact Explanation: Breach Reporting Policy Documents: Comprehensive policies detailing the breach reporting process, including how to report, who to contact, and the protections in place for reporters. \n\nBreach Reporting System Descriptions: Descriptions of the tools or platforms used for reporting breaches, ensuring they are user-friendly and accessible. \n\nProtection Protocols for Reporters: Guidelines ensuring that individuals who report breaches are protected from retaliation or negative consequences."
    },
    {
        "control_name": "Breach Detection Timeliness",
        "category": "Breach Response Protocols",
        "description": "Ensure actions are taken promptly upon breach detection that are subject to laws and regulations in which the entity operates.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 87: Reporting of Breaches and Protection of Reporting Persons, NIST 800-53: IR-4: Incident Handling\nSI-4: System Monitoring\nRA-5: Vulnerability Monitoring and Scanning\nIR-5: Incident Monitoring\nAC-2: Account Management\nCP-9: System Backup\nIR-6: Incident Reporting\nAU-6: Audit Record Review, Analysis, and Reporting\nCM-6: Configuration Settings, SCF: MON-01.11: Automated Response to Suspicious Events - Utilize automated response systems to immediately act upon breach detection.\nIRO-02: Incident Handling - Establish procedures to ensure prompt action is taken when a breach is detected.\nIRO-02.1: Automated Incident Handling Processes - Implement automated processes to ensure quick response to detected breaches.\nIRO-02.3: Dynamic Reconfiguration - Configure systems to dynamically respond to and manage detected breaches promptly.\nIRO-02.6: Automatic Disabling of System - Set up mechanisms for the automatic disabling or containment of affected systems upon breach detection.",
        "explainability": "The Breach Detection Timeliness control involves defining and implementing measures to ensure timely detection of security breaches or incidents related to AI systems. The rationale for this control is to minimize the impact of security breaches by detecting and responding to incidents promptly, reducing potential damage and safeguarding the integrity of the AI system.",
        "evidence": "Rationale Explanation: Detection Timeliness Criteria: Documentation specifying the criteria and benchmarks for determining the timeliness of breach detection, including acceptable time frames for different types of incidents. \n\nMonitoring and Alerting System: Implementation of a system for continuous monitoring of AI system activities and the establishment of alerts that trigger in response to potential security breaches, enabling quick detection. \n\nReporting Mechanism: Communication channels and protocols for reporting detected security breaches to relevant stakeholders, ensuring a swift and coordinated response. | Responsibility Explanation: Breach Detection Protocol: A protocol outlining the steps for timely detection and response to breaches. \n\nIncident Response Plan: A comprehensive plan detailing actions to be taken upon breach detection. \n\nBreach Response Training Program: Training materials for personnel on rapid breach detection and response. | Data Explanation: Breach Response Plan: A documented plan that specifies the immediate actions to be taken when a breach is detected.\n\nIncident Response Team Assignments: Clear designation of roles and responsibilities for team members responding to a breach. \n\nBreach Detection and Response Training Materials: Training resources to prepare relevant staff for rapid breach detection and response. | Fairness Explanation: Breach Response Timeliness Protocols: Protocols outlining the timely response to breaches, with an emphasis on fairness in the response. \n\nTimeliness Compliance Logs: Logs that record the timing and responsiveness of actions taken following breach detection. \n\nBreach Response Fairness Checklists: Checklists to ensure that breach responses are timely and uphold fairness. | Safety & Performance  Explanation: Artifacts would include a Breach Detection Protocol, which outlines the systematic approach to identifying breaches and an Incident Response Time Log documenting the timeline of each detected breach and the subsequent actions taken. A Continuous Improvement Plan for Detection Systems would also be an essential deliverable, which shows plans for upgrading detection methods. | Impact Explanation: Breach Response Protocols: Detailed guidelines and procedures for immediate actions following breach detection, including communication plans and steps for containment. \n\nIncident Response Logs: Records of all detected breaches and the timelines of the responses, documenting the promptness of actions taken. \n\nPostincident Reports: Detailed analyses of each incident, including response times and effectiveness of actions taken."
    },
    {
        "control_name": "AI Incident Public Disclosure Procedure",
        "category": "AI Incident Public Disclosure",
        "description": "Formulate procedures for the timely and responsible public disclosure of AI incidents, ensuring transparency and maintaining public trust.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 50: Transparency Obligations for Providers and Users of Certain AI Systems and GPAI Models, NIST 800-53: IR-1: Incident Response Policy and Procedure\nIR-4: Incident Handling\nAU-6: Audit Record Review, Analysis, and Reporting\nPL-4: Rules of Behavior\nAC-8: System Use Notification\nPM-15: Security and Privacy Groups and Associations\nPM-18: Privacy Program Plan\nRA-3: Risk Assessment\nPM-13: Security and Privacy Workforce\nPL-2: System Security and Privacy Plans, SCF: CPL-05: Legal Assessment of Investigative Inquires - Include legal assessments in the public disclosure process to ensure compliance and responsibility.\nCPL-05.2: Investigation Access Restrictions - Manage access to investigation details in a manner that supports responsible public disclosure while maintaining necessary confidentiality.\nIRO-04: Incident Response Plan (IRP) - Develop a plan that includes procedures for the public disclosure of AI incidents.\nIRO-10: Incident Stakeholder Reporting - Establish reporting mechanisms for stakeholders, which include public disclosure guidelines for AI incidents.\nIRO-16: Public Relations & Reputation Repair - Prepare public relations strategies that encompass responsible public disclosure of AI incidents.",
        "explainability": "The AI Incident Public Disclosure Procedure control involves establishing a procedure for publicly disclosing information about security incidents or breaches related to AI systems. The rationale for this control is to ensure transparency, accountability, and timely communication with stakeholders, including customers, partners, and the public, in the event of a security incident.",
        "evidence": "Rationale Explanation: Disclosure Procedure Document: A document outlining the procedure for publicly disclosing AI-related security incidents, specifying the criteria for disclosure, the timeline for communication, and the information to be shared. \n\nCommunication Plan: Development of a plan detailing the communication channels, stakeholders, and messaging strategy for public disclosure, ensuring a consistent and coordinated approach. \n\nIncident Report: Preparation of a comprehensive incident report that provides details on the nature of the incident, the impact, and the steps taken for resolution, serving as a basis for public disclosure. | Responsibility Explanation: Public Disclosure Protocol: A protocol for the public disclosure of AI incidents. \n\nIncident Communication Guide: A guide for communicating AI incidents to the public responsibly. \n\nMedia Training for Incident Disclosure: Training materials for staff on handling media and public disclosures. | Data Explanation: Public Disclosure Procedure Documentation: Documentation specifying the procedures for public disclosure of AI incidents, emphasizing data-related aspects and transparency. \n\nIncident Records: Records of AI incidents and their handling, including details on public disclosures. | Fairness Explanation: Public Disclosure Protocols: Protocols detailing the fair and responsible disclosure of AI incidents to the public. \n\nIncident Transparency Reports: Reports that document AI incidents and the fairness of the disclosure process. \n\nDisclosure Procedure Fairness Guidelines: Guidelines ensuring that public disclosures of AI incidents are conducted fairly. | Safety & Performance  Explanation: Artifacts should include a Public Disclosure Policy detailing how information about incidents will be communicated to the public and an Incident Reporting Template, which is designed to ensure that all necessary information is conveyed clearly and consistently. A Public Relations Management Plan would also be integral that outlines strategies for maintaining public trust during and after incident disclosure. | Impact Explanation: Public Disclosure Policies: Detailed guidelines and protocols for public disclosure of AI incidents, including criteria for what should be disclosed and timelines. \n\nDisclosure Templates and Checklists: Standardized templates and checklists for preparing public disclosures, ensuring consistency and comprehensiveness. \n\nRecord of Public Disclosures: A log or record of all AI incidents that have been publicly disclosed, including dates and details of the disclosure."
    },
    {
        "control_name": "Judicial Review for Exceptional AI Deployment",
        "category": "Change Management",
        "description": "Establish a procedure requiring judicial review when deploying high-risk AI systems under unique or exceptional circumstances.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 26: Obligations of Deployers of High-Risk AI Systems, NIST 800-53: PM-9: Risk Management Strategy\nPL-2: System Security and Privacy Plans\nSA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nAC-25: Reference Monitor\nPM-1: Information Security Program Plan\nRA-3: Risk Assessment\nSA-4: Acquisition Process\nPM-15: Security and Privacy Groups and Associations, SCF: CHG-03: Security Impact Analysis for Changes - Conduct a security impact analysis for deploying high-risk AI systems, especially those requiring judicial review under unique circumstances.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Develop and publish documentation outlining procedures for judicial review in cases of exceptional AI deployment.\nGOV-03: Periodic Review & Update of Cybersecurity & Data Protection Program - Regularly update and review the governance program to include judicial review processes for high-risk AI deployments.\nGOV-13: State-Sponsored Espionage - Consider judicial review requirements in the context of high-risk AI systems deployment that may have national security implications.\nGOV-15.4: Authorize Systems, Applications & Services - Include a judicial review step as part of the authorization process for deploying exceptional or high-risk AI systems.",
        "explainability": "The Judicial Review for Exceptional AI Deployment control involves establishing a process for judicial review in cases of exceptional AI deployment that may raise legal or ethical concerns. The rationale for this control is to provide a mechanism for independent oversight and legal scrutiny to ensure that AI systems are deployed in exceptional situations with due consideration for legal and ethical implications.",
        "evidence": "Rationale Explanation: Judicial Review Framework: Documentation outlining the framework for judicial review in cases of exceptional AI deployment, specifying the criteria for triggering a review, the judicial authorities involved, and the procedures to be followed. \n\nCase Documentation: Compilation of comprehensive documentation for exceptional AI deployments, including details on the decision-making process, risk assessments, and justifications, to facilitate the judicial review process. \n\nLegal Compliance Report: Preparation of a report assessing the legal compliance of exceptional AI deployments, outlining any identified legal or ethical concerns, and recommending corrective actions if necessary. | Responsibility Explanation: Judicial Review Procedure Document: A document outlining the procedure for judicial review of exceptional AI deployments. \n\nLegal Compliance Checklist: A checklist to ensure legal compliance in the review process. \n\nLegal Advisory Sessions: Sessions with legal advisors to guide the judicial review process. | Data Explanation: Judicial Review Policy: A formal policy that establishes the need and process for judicial review under exceptional deployment circumstances. \n\nReview Request Documentation: Templates and documents used to file for a judicial review. \n\nLegal Compliance Checklist: A checklist to ensure that all necessary legal considerations are met before proceeding with the deployment. | Fairness Explanation: Judicial Review Policy Documents: Documents that describe the policy for judicial review in exceptional AI deployments, emphasizing fairness. \n\nExceptional Deployment Fairness Checklists: Checklists that ensure deployments under exceptional circumstances meet fairness criteria. \n\nJudicial Oversight Fairness Reports: Reports from judicial reviews that focus on the fairness of exceptional AI deployments. | Safety & Performance  Explanation: Artifacts would include a Judicial Review Procedure Manual, which outlines the steps and requirements for initiating a review and an Exceptional Deployment Case File containing all documentation to be assessed by the judiciary. An AI System Risk Assessment Report may also be prepared detailing potential risk and mitigations presented during the review. | Impact Explanation: Judicial Review Reports: Detailed documents outlining the judicial review process, findings, and recommendations for the AI system. These reports should include legal analyses, ethical considerations, and potential societal impacts.  \n\nAI Risk Assessment Documentation: Comprehensive records detailing the risk assessment of the AI system, focusing on its potential societal and ethical implications.  \n\nStakeholder Feedback Compilation: A collection of feedback from relevant stakeholders, including industry experts, legal professionals, and potentially affected communities."
    },
    {
        "control_name": "Alignment with Union Harmonization Law for High-Risk AI Systems",
        "category": "Change Management",
        "description": "Ensure that the requirements of high-risk AI systems are integrated and aligned with the specifications outlined in EU harmonization law, specifically Annex II, Section A.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 8: Compliance with the Requirements\nAnnex I: List of Union Harmonisation Legislation, NIST 800-53: PL-2: System Security and Privacy Plans\nSA-11: Developer Testing and Evaluation\nRA-3: Risk Assessment\nPM-9: Risk Management Strategy\nPM-1: Information Security Program Plan\nSA-4: Acquisition Process\nCA-2: Control Assessments\nCM-2: Baseline Configuration\nPM-15: Security and Privacy Groups and Associations, SCF: CHG-02: Configuration Change Control - Manage changes to AI systems to ensure continuous alignment with Union harmonisation law.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensure compliance with statutory and regulatory requirements, including Union harmonisation law for high-risk AI systems.\nCPL-01.2: Compliance Scope - Define the scope of compliance to include the specifications of Union harmonisation law, particularly those in Annex II, Section A.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversee and manage cybersecurity and data privacy controls to align with Union harmonisation law requirements for high-risk AI systems.\nGOV-01: Cybersecurity & Data Protection Governance Program - Integrate and align the governance program with Union harmonisation law for high-risk AI systems.",
        "explainability": "The Alignment with Union Harmonization Law for High-Risk AI Systems control involves ensuring that high-risk AI systems adhere to EU harmonization laws and regulations. The rationale for this control is to promote consistency, compliance, and legal conformity with EU regulations for high-risk AI deployments, fostering a unified legal framework.",
        "evidence": "Rationale Explanation: Legal Compliance Matrix: Documentation detailing the alignment of high-risk AI systems with specific EU harmonization laws, providing a matrix that outlines the relevant legal requirements and the corresponding compliance status. \n\nLegal Alignment Statement: A formal statement attesting to the alignment of high-risk AI systems with EU harmonization laws, signed by responsible parties, demonstrating commitment to legal compliance. \n\nPeriodic Legal Audits: Implementation of a schedule for conducting periodic legal audits to assess and verify the ongoing alignment of high-risk AI systems with EU harmonization laws. | Responsibility Explanation: Harmonization Compliance Plan: A plan for ensuring AI system compliance with EU harmonization law. \n\nAlignment Assessment Report: A report assessing the alignment of AI systems with the specified law. \n\nLegal Training on Harmonization Requirements: Training materials for compliance teams on EU harmonization law. | Data Explanation: Compliance Alignment Document: A comprehensive analysis that aligns AI system requirements with the EU harmonization law's standards. \n\nAnnex II Implementation Guide: A guide for implementing the specific requirements of Annex II, Section A, in the context of high-risk AI systems. \n\nRegulatory Compliance Reports: Reports that document the AI systems' compliance with the EU harmonization law. | Fairness Explanation: Harmonization Compliance Reports: Detailed reports documenting AI systems' alignment with EU harmonization law. \n\nAnnex II Section A Adherence Checklists: Checklists specifically designed to verify compliance with the fairness-related specifications of EU law. \n\nCompliance Alignment Protocols: Protocols outlining steps for integrating and maintaining AI system compliance with fairness aspects of EU law. | Safety & Performance  Explanation: Deliverables would include a Compliance Mapping Document crossreferencing the AI system's features with the requirements in Annex II, Section A, and a Union Harmonization Compliance Report documenting the system’s adherence to the specified legal standards. An AI System Legal Alignment Checklist might also be created, providing a quick reference for compliance checks. | Impact Explanation: Compliance Mapping Documents: Detailed mappings showing how the AI system aligns with each aspect of EU harmonization law, particularly Annex II, Section A.  \n\nLegal Analysis Reports: Comprehensive reports analyzing the AI system’s features against the EU law requirements, highlighting areas of compliance and noncompliance.\n\nImpact Assessment Summaries: Summaries that assess the broader societal and individual impacts of the AI system in the context of compliance with EU law."
    },
    {
        "control_name": "Rights Breach Guidance Adoption",
        "category": "Legal, Regulatory, & AI-Prohibited Use Cases",
        "description": "Ensure that businesses adopt and adhere to the guidance from the Commission regarding obligations concerning breaches of fundamental rights. This guidance should be regularly reviewed and updated.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 5: Prohibited Artificial Intelligence Practices\nArticle 87: Reporting of Breaches and Protection of Reporting Persons, NIST 800-53: PL-2: System Security and Privacy Plans\nPM-9: Risk Management Strategy\nPM-1: Information Security Program Plan\nPM-18: Privacy Program Plan\nRA-3: Risk Assessment\nAU-6: Audit Record Review, Analysis, and Reporting\nAC-1: Access Control Policy and Procedures\nSA-11: Developer Testing and Evaluation, SCF: CPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversee and manage adherence to Commission guidance on fundamental rights breaches.\nGOV-02: Publishing Cybersecurity & Data Protection Documentation - Publish and distribute documentation that includes Commission guidance on fundamental rights breaches.\nGOV-03: Periodic Review & Update of Cybersecurity & Data Protection Program - Regularly review and update the cybersecurity and data protection program to align with the latest Commission guidance on rights breaches.\nPRI-01: Data Privacy Program - Integrate Commission guidance on fundamental rights breaches into the organization's data privacy program.\nPRI-02: Data Privacy Notice - Ensure data privacy notices reflect compliance with Commission guidance on fundamental rights breaches.",
        "explainability": "The Rights Breach Guidance Adoption control involves adopting guidance for handling and responding to rights breaches related to AI systems. The rationale for this control is to provide clear and standardized procedures for identifying, assessing, and addressing rights breaches, ensuring that individuals' rights are respected and protected in the context of AI system operations.",
        "evidence": "Rationale Explanation: Rights Breach Guidance Document: A document outlining the guidance for handling rights breaches related to AI systems, specifying the steps to be taken, the responsible parties, and the timelines for response and resolution. \n\nTraining Program: Development of a training program to educate relevant stakeholders, including AI system operators, support personnel, and customer service representatives, on the rights breach guidance and procedures. \n\nReporting Mechanism: Establishment of a reporting mechanism for individuals to report potential rights breaches, ensuring a transparent and accessible process. | Responsibility Explanation: Rights Breach Guidance Adoption Plan: A plan for adopting and adhering to the EU Commission's guidance. \n\nFundamental Rights Compliance Checklist: A checklist to ensure adherence to the guidance on rights breaches. \n\nRights Breach Training Program: Training materials on understanding and adhering to the rights breach guidance. | Data Explanation: Guidance Adoption Policy: A policy that mandates the incorporation of the EU Commission's guidance into business practices. \n\nTraining Programs on Rights Breach: Training materials and sessions designed to educate employees about fundamental rights and the guidance provided by the EU Commission. \n\nGuidance Review Records: Records of periodic reviews of the EU Commission’s guidance and any updates made to company policies accordingly. | Fairness Explanation: Rights Breach Response Manuals: Manuals that provide guidance on responding to breaches of fundamental rights in AI system operations. \n\nGuidance Review Summaries: Summaries of the latest guidance from the Commission on fundamental rights obligations and their application for fairness. \n\nRights Adherence Training Modules: Training modules developed to educate businesses on adhering to guidance concerning fairness and fundamental rights. | Safety & Performance  Explanation: Artifacts would include a Fundamental Rights Alignment Document detailing the AI system's compliance with the EU Commission's guidance and a Guidance Review Log chronicling each review and update of the guidance documentation. A Rights Breach Response Plan, outlining procedures for addressing any breaches, would also be necessary. | Impact Explanation: Adoption Compliance Reports: Documents detailing how the business has adopted and is adhering to the EU Commission's guidance, including steps taken to integrate the guidance into business practices.  \n\nFundamental Rights Impact Assessments: Reports assessing the impact of the business's AI systems on fundamental rights, considering the latest guidance from the EU Commission.  \n\nGuidance Review and Update Logs: Records of how and when the guidance was reviewed and updated within the business, indicating responsiveness to evolving standards."
    },
    {
        "control_name": "Collaborative AI Intellectual Property Rights Management",
        "category": "Intellectual Property Protections",
        "description": "Establish a collaborative framework to manage and protect the intellectual property (IP) rights associated with AI systems, ensuring compliance with legal standards.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 25: Responsibilities Along the AI Value Chain, NIST 800-53: SA-14: System and Services Acquisition Policy and Procedures\nPM-11: Mission/Business Process Definition\nPL-8: Security and Privacy Architectures\nSA-5: System Documentation\nRA-3: Risk Assessment\nPM-9: Risk Management Strategy\nAU-6: Audit Record Review, Analysis, and Reporting\nAC-1: Access Control Policy and Procedures\nPM-16: Threat Awareness Program, SCF: AST-22: Microphones & Web Cameras - Secure intellectual property in AI systems, including proprietary technologies in microphones and web cameras.\nAST-23: Multi-Function Devices (MFD) - Protect intellectual property in AI systems, especially in multi-function devices used for AI development and deployment.\nCPL-01: Statutory, Regulatory & Contractual Compliance - Ensure compliance with laws and regulations governing intellectual property rights in AI systems.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversee and manage the protection of intellectual property rights in AI systems.\nTPM-05: Third-Party Contract Requirements - Include intellectual property protection clauses in contracts with third parties involved in AI system development or use.",
        "explainability": "The Collaborative AI Intellectual Property Rights Management control involves implementing a collaborative framework for managing IP rights related to AI. The rationale for this control is to establish transparent processes for identifying, protecting, and sharing IP in collaborative AI projects, fostering innovation while ensuring legal compliance.",
        "evidence": "Rationale Explanation: Intellectual Property Management Framework: Documentation outlining the collaborative framework for managing IP rights in AI projects, specifying the procedures for identification, protection, and sharing of IP. \n\nCollaborative Agreement Templates: Development of templates for collaborative agreements that explicitly define the IP rights, responsibilities, and terms for collaborators engaged in AI projects. \n\nTraining and Awareness Program: Implementation of a program to educate collaborators on IP rights, legal implications, and the collaborative framework, promoting understanding and compliance. | Responsibility Explanation: Intellectual Property Management Framework: A framework for managing AI IP rights collaboratively. \n\nCompliance with Legal Standards Guide: A guide for ensuring IP management complies with legal standards. \n\nIP Rights Training Program for Staff: Training materials for staff on managing AI IP rights. | Data Explanation: IPR Management Policy: A policy outlining the approach to managing and safeguarding the IPR of AI systems. \n\nCollaboration Agreements: Legal agreements that define the terms of cooperation between parties for IPR management of AI systems. \n\nCompliance Documentation: Documentation proving adherence to IPR laws and standards in the context of AI. | Fairness Explanation: IP Management Frameworks: Frameworks that outline the collaborative management of AI-related IP.\n\nIP Rights Collaboration Agreements: Agreements that define the terms of collaboration between entities for the protection of AI IP.\n \nCompliance with IP Legal Standards Logs: Logs documenting the compliance of AI systems with IP legal standards, emphasizing fairness. | Safety & Performance  Explanation: Artifacts should include an Intellectual Property Management Plan outlining the strategies for managing AI-related IP rights and an IP Compliance Report documenting adherence to legal standards. A Stakeholder Agreement on IP Rights detailing the shared and individual rights of all parties involved in the AI system’s development should also be included. | Impact Explanation: IPR Management Framework Documentation: A comprehensive document detailing the collaborative framework for managing AI-related IP rights, including roles, processes, and compliance measures.  \n\nLegal Compliance Reports: Reports evaluating the framework's alignment with current IPR laws and regulations.\n  \nImpact Analysis on Innovation and Competitiveness: Assessments detailing how the IPR management impacts innovation in AI development and the competitive landscape."
    },
    {
        "control_name": "AI Export Control Compliance Procedures",
        "category": "AI Export Controls",
        "description": "Establish procedures to ensure that the development, distribution, and sale of AI technologies comply with international export control laws and regulations.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 8: Compliance with the Requirements, NIST 800-53: SA-4: Acquisition Process\nPM-9: Risk Management Strategy\nRA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nCM-2: Baseline Configuration\nPM-15: Security and Privacy Groups and Associations\nAU-6: Audit Record Review, Analysis, and Reporting\nPL-2: System Security and Privacy Plans\nPM-1: Information Security Program Plan\nPM-18: Privacy Program Plan, SCF: CPL-01: Statutory, Regulatory & Contractual Compliance - Ensure compliance with international export control laws and regulations for AI technologies.\nCPL-01.1: Non-Compliance Oversight - Monitor for non-compliance with export control laws in the development, distribution, and sale of AI technologies.\nCPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversee cybersecurity and data privacy controls to ensure they align with export control requirements for AI technologies.\nTPM-01: Third-Party Management - Manage third-party involvement in AI technologies to ensure compliance with export control laws.\nTPM-03: Supply Chain Protection - Protect the supply chain of AI technologies to adhere to export control regulations.",
        "explainability": "The AI Export Control Compliance Procedures control involves establishing procedures to ensure compliance with export control regulations for AI technologies. The rationale for this control is to mitigate the risk of unauthorized transfer of sensitive AI technologies across borders, ensuring adherence to export control laws and national security measures.",
        "evidence": "Rationale Explanation: Export Control Procedures Manual: Documentation outlining the procedures for AI export control compliance, including steps for assessing the export classification of AI technologies, obtaining necessary licenses, and conducting due diligence on end users.\n\nCompliance Documentation: Compilation of documentation demonstrating compliance with export control regulations, including records of export classification assessments, license applications, and due diligence reports. \n\nTraining and Awareness Program: Implementation of a program to educate relevant personnel on export control procedures, classification criteria, and the importance of compliance to prevent unauthorized exports. | Responsibility Explanation: Export Control Compliance Procedure: A procedure for ensuring compliance with export control laws. \n\nExport Control Training Program: A training program for personnel involved in the export of AI technologies. \n\nExport Control Compliance Checklist: A checklist to ensure all aspects of export control are covered. | Data Explanation: Export Compliance Policy: A policy document that details the export control regulations applicable to AI technologies and the procedures for compliance. \n\nExport License Applications: Documentation and records of export licenses applied for in accordance with international laws. \n\nExport Control Training Modules: Training programs designed to educate employees on the legal requirements and compliance procedures for AI technology export. | Fairness Explanation:  Export Control Compliance Checklists: Checklists for ensuring AI technologies adhere to export control laws, with a focus on fairness. \n\nDistribution Compliance Reports: Reports on the compliance of AI technology distribution with international laws. \n\nSale Regulation Fairness Guidelines: Guidelines to ensure that the sale of AI technologies complies with fairness principles in international trade. | Safety & Performance  Explanation: Deliverables would include an Export Control Compliance Manual, which details the procedures for exporting AI technologies and an Export License Tracking Log documenting all licenses applied for and received. An International Trade Compliance Report would also be essential that summarizes compliance with export controls over a given period. | Impact Explanation: Export Compliance Policy Documents: Detailed policies outlining the procedures for ensuring AI technologies comply with export control laws.  \n\nCompliance Training Records: Records of training provided to employees on export control laws and procedures.  \n\nExport Compliance Audit Reports: Regular audit reports assessing adherence to export control laws in the development, distribution, and sale of AI technologies."
    },
    {
        "control_name": "Protection of Vulnerable Populations in AI Deployment",
        "category": "Environmental & Societal Impact Assessment",
        "description": "Develop and implement measures and mechanisms within the AI systems to ensure that vulnerable populations are safeguarded from unfairness, discrimination or unethical considerations.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 27: Fundamental Rights Impact Assessment for High-Risk AI Systems, NIST 800-53: RA-3: Risk Assessment\nPM-9: Risk Management Strategy\nSA-11: Developer Testing and Evaluation\nSI-22: Information Diversity\nPL-2: System Security and Privacy Plans\nPM-18: Privacy Program Plan\nPM-22: Personally Identifiable Information Quality Management\nAU-6: Audit Record Review, Analysis, and Reporting\nSA-8: Security and Privacy Engineering Principles\nPM-16: Threat Awareness Program, SCF: AAT-06: AI & Autonomous Technologies Fairness & Bias - Address fairness and bias in AI systems to protect vulnerable populations.\nAAT-17.3: Previously Unknown AI & Autonomous Technologies Threats & Risks - Identify and mitigate unknown threats and risk in AI systems, especially those impacting vulnerable groups.\nPRI-01.6: Security of Personal Data - Ensure the security of personal data in AI systems, with a focus on data pertaining to vulnerable populations.\nRSK-04: Risk Assessment - Conduct risk assessments on AI systems to identify potential adverse impacts on vulnerable populations.\nRSK-06: Risk Remediation - Remediate risk identified in AI systems that could adversely impact vulnerable groups or children.",
        "explainability": "The Protection of Vulnerable Populations in AI Deployment control involves implementing measures to safeguard vulnerable populations from potential harms associated with AI deployment. The rationale for this control is to address ethical considerations, ensure fairness, and prevent discrimination or harm to vulnerable individuals or groups in the deployment of AI systems.",
        "evidence": "Rationale Explanation: Vulnerable Populations Protection Policy: Documentation outlining the policy for protecting vulnerable populations in AI deployment, specifying the criteria for identifying vulnerability, the measures to be taken, and the responsible parties. \n\nImpact Assessment Templates: Development of templates for conducting impact assessments specifically focused on evaluating the potential effects of AI deployment on vulnerable populations, including considerations for fairness, bias, and ethical implications. \n\nTraining and Sensitization Program: Implementation of a program to educate relevant stakeholders, including AI developers, operators, and decision-makers, on the protection policy, impact assessment process, and the importance of considering vulnerable populations. | Responsibility Explanation: Vulnerable Population Protection Plan: A plan for protecting vulnerable populations in AI deployment. \n\nImpact Mitigation Strategies: Strategies for mitigating potential adverse impacts on vulnerable groups.\n \nVulnerability Assessment Protocol: A protocol for assessing vulnerabilities and potential impacts. | Data Explanation: Vulnerability Impact Assessment Framework: A framework for assessing the potential impact of AI systems on vulnerable groups. \n\nChild Protection Protocols: Specific protocols aimed at protecting children from adverse effects of AI systems. \n\nMitigation Strategy Documents: Strategies and plans developed to mitigate risk identified during the vulnerability assessments. | Fairness Explanation: Vulnerable Group Protection Plans: Plans outlining measures to protect vulnerable populations from unfair AI outcomes. \n\nImpact Mitigation Strategies: Strategies specifically designed to mitigate any potential unfair impacts on vulnerable groups. \n\nRisk Management Fairness Documentation: Documentation of the risk management processes, focusing on the protection of fairness for vulnerable populations. | Safety & Performance  Explanation: Artifacts should include a Vulnerable Populations Protection Plan, which lays out the strategies for safeguarding these groups and an Adverse Impact Mitigation Report detailing the steps taken to reduce negative outcomes. An Impact Assessment Toolkit tailored to identify risk to vulnerable groups, including children, should also be developed. | Impact Explanation: Vulnerability Impact Assessment Reports: Detailed analyses of how AI systems may affect vulnerable populations, including risk factors and mitigation strategies. \n \nRisk Management Process Documentation: Documentation of the processes and criteria used to identify and mitigate risk to vulnerable groups.  \n\nMitigation Strategy Records: Records of strategies and actions taken to reduce adverse impacts on vulnerable populations."
    },
    {
        "control_name": "High-Risk AI System Impact Assessment",
        "category": "AI Impact Assessments",
        "description": "Implement, conduct and document a completed fundamental rights impact assessment, evidence of a mitigation plan, and registration of high-risk AI systems in a public database.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 27: Fundamental Rights Impact Assessment for High-Risk AI Systems, NIST 800-53: RA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nRA-8: Privacy Impact Assessments\nPM-18: Privacy Program Plan\nCM-8: System Component Inventory\nPL-2: System Security and Privacy Plans\nSA-11: Developer Testing and Evaluation\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-9: Risk Management Strategy\nSA-4: Acquisition Process, SCF: AAT-01.3: AI & Autonomous Technologies Value Sustainment - Sustain the value of AI technologies while addressing the impacts on fundamental rights and mitigating risk.\nGOV-08: Defining Business Context & Mission - Define the business context and mission in the context of deploying high-risk AI systems, ensuring alignment with fundamental rights impact assessments.\nDCH-25: Transfer of Sensitive and/or Regulated Data - Manage the transfer and registration of high-risk AI systems in a public database, ensuring sensitive data handling complies with fundamental rights considerations.\nRSK-04: Risk Assessment - Conduct a comprehensive risk assessment for high-risk AI systems, including the assessment of fundamental rights impacts.\nRSK-08: Business Impact Analysis (BIA) - Perform a business impact analysis that includes the evaluation of fundamental rights and other key impacts of high-risk AI systems.",
        "explainability": "The High-Risk AI System Impact Assessment control involves conducting comprehensive assessments to evaluate the potential impacts of high-risk AI systems on individuals and society. The rationale for this control is to proactively identify and mitigate any negative consequences, ethical concerns, or unintended effects associated with the deployment of high-risk AI systems.",
        "evidence": "Rationale Explanation: Impact Assessment Framework: Documentation outlining the framework for conducting impact assessments on high-risk AI systems, detailing the criteria for evaluation, the methodologies to be employed, and the responsible parties involved. \n\nImpact Assessment Reports: Compilation of impact assessment reports for each high-risk AI system, providing a detailed analysis of potential impacts on individuals, communities, and broader societal factors. \n\nStakeholder Consultation Summary: Summary documentation capturing the results of stakeholder consultations conducted during the impact assessment process, ensuring diverse perspectives are considered. | Responsibility Explanation: Impact Assessment Report: A report documenting the fundamental rights impact assessment. \n\nMitigation Plan Document: A document outlining the plan to mitigate identified risk. \n\nSystem Registration in Public Database: Ensuring high-risk AI systems are registered in the public database. | Data Explanation: Fundamental Rights Impact Assessment Report: A report detailing the outcomes of the impact assessment on fundamental rights conducted for the AI system. \n\nRisk Mitigation Plan: A document outlining the strategies and measures to mitigate any risk identified during the impact assessment. \n\nAI System Registration Proof: Confirmation of the registration of the high-risk AI system in the designated public database. | Fairness Explanation: Fundamental Rights Impact Assessment Reports: Comprehensive reports assessing the potential impact of high-risk AI systems on fundamental rights, with a focus on fairness. \n\nRights Mitigation Plan Documentation: Detailed documentation of plans to mitigate any negative impacts on fundamental rights.\n \nHigh-Risk System Registration Certificates: Certificates that validate the registration of high-risk AI systems in public databases, ensuring transparency and adherence to fairness. | Safety & Performance  Explanation: The key deliverables would be a Fundamental Rights Impact Assessment Report, which documents the assessment results and potential impacts on rights. A Mitigation Strategy Document is also required, specifying actionable plans to address the risk identified. Lastly, a Registration Confirmation Receipt from a public database where the high-risk AI system is listed must be provided. | Impact Explanation: Fundamental Rights Impact Assessment Reports: Comprehensive reports detailing the assessment of potential impacts of the AI system on fundamental rights.  \n\nRisk Mitigation Plans: Documented strategies outlining steps to mitigate identified risk associated with the AI system.  \n\nRegistration Documents: Proof of registration of the high-risk AI system in a relevant public database, including details of the system and its intended use."
    },
    {
        "control_name": "Predeployment Impact Assessment",
        "category": "AI Impact Assessments",
        "description": "Prior to deployment, high-risk AI systems must undergo a comprehensive assessment to evaluate potential impacts on users, processes, and the environment ensuring responsible and accountable deployment practices.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 27: Fundamental Rights Impact Assessment for High-Risk AI Systems, NIST 800-53: RA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nRA-5: Vulnerability Monitoring and Scanning\nRA-8: Privacy Impact Assessments\nRA-9: Criticality Analysis\nSA-4: Acquisition Process\nPM-9: Risk Management Strategy\nCM-2: Baseline Configuration\nPL-2: System Security and Privacy Plans\nAU-6: Audit Record Review, Analysis, and Reporting\nPM-18: Privacy Program Plan, SCF: AAT-01.3: AI & Autonomous Technologies Value Sustainment - Ensure that AI systems sustain their value while evaluating their potential impacts on users and the environment before deployment.\nAAT-07.2: AI & Autonomous Technologies Likelihood & Impact Risk Analysis - Analyze the likelihood and impact of risk associated with AI systems on users and the environment before deployment.\nAAT-17.2: AI & Autonomous Technologies Environmental Impact & Sustainability - Assess the environmental impact and sustainability of high-risk AI systems before their deployment.\nRSK-04: Risk Assessment - Perform a detailed risk assessment for high-risk AI systems before deployment, focusing on potential impacts on users, processes, and the environment.\nRSK-08: Business Impact Analysis (BIA) - Conduct a business impact analysis to understand the implications of deploying high-risk AI systems on various business processes and the environment.",
        "explainability": "The Predeployment Impact Assessment control involves conducting assessments to evaluate the potential impacts of AI systems before their deployment. The rationale for this control is to proactively identify and address any ethical, social, or legal concerns associated with AI system deployment, ensuring responsible and accountable deployment practices.",
        "evidence": "Rationale Explanation: Predeployment Impact Assessment Guidelines: Documentation outlining guidelines for conducting pre-deployment predeployment impact assessments, specifying the criteria for evaluation, the methodologies to be employed, and the responsible parties involved.\n \nImpact Assessment Reports: Compilation of predeployment impact assessment reports for each AI system, providing a detailed analysis of potential impacts on individuals, communities, and broader societal factors. \n\nRisk Mitigation Plan: Development of a plan outlining the measures to be taken to mitigate identified risk and address concerns raised during the predeployment impact assessment. | Responsibility Explanation: Predeployment Assessment Procedure: A procedure for conducting predeployment impact assessments. \n\nImpact Evaluation Report Template: A template for documenting the findings of the impact assessment. \n\nRisk Assessment Training Modules: Training materials for teams conducting risk assessments. | Data Explanation: Predeployment Assessment Report: A detailed report that documents the findings of the predeployment impact evaluation. \n\nUser Impact Analysis: An analysis that specifically addresses how the AI system may affect end users. \n\nEnvironmental Impact Study: A study to assess the potential environmental effects of deploying the AI system. | Fairness Explanation: Impact Assessment Reports: Detailed reports on the predeployment evaluation of AI systems, specifically focusing on fairness impacts. \n\nUser and Environmental Impact Analysis: Analysis documents that assess the potential effects of AI systems on users and the environment with a fairness lens. \n\nProcess Change Fairness Documentation: Documentation on how the deployment of the AI system could change existing processes, with an emphasis on maintaining fairness. | Safety & Performance  Explanation: Deliverables would include a Predeployment Impact Assessment Report, which thoroughly analyzes the AI system's potential effects and a Risk Mitigation Plan that outlines strategies to address any identified risk. Environmental Impact Statements could also be necessary to assess the system's footprint and compliance with environmental standards. | Impact Explanation: Predeployment Impact Reports: Detailed reports assessing the potential impacts of the AI system on users, processes, and the environment. These reports should include analysis of risk and proposed mitigation strategies.  \n\nUser and Process Impact Analyses: Specific analyses focusing on how the AI system will affect end users and existing processes, including potential benefits and drawbacks.  \n\nEnvironmental Impact Assessments: Detailed evaluations of the AI system's potential environmental impacts, both direct and indirect."
    },
    {
        "control_name": "Risk Management",
        "category": "Risk Analysis Records",
        "description": "Develop, implement and document regular detailed records of risk analysis, especially those related to health, safety, rights, democracy, and the environment.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 9: Risk Management System, NIST 800-53: RA-1: Risk Assessment Policy and Procedures\nRA-2: Security Categorization\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nRA-6: Technical Surveillance Countermeasures Survey\nRA-8: Privacy Impact Assessments\nPM-9: Risk Management Strategy\nPM-10: Security Authorization Process\nPL-2: System Security and Privacy Plans\nPL-8: Security and Privacy Architectures\nPL-9: Central Management\nSI-5: Security Alerts, Advisories, and Directives, SCF: DCH-06.2: Sensitive Data Inventories - Maintain inventories of sensitive data, including information related to health, safety, and environmental risk.\nPRI-05.5: Inventory of Personal Data - Keep detailed records of personal data processing that may impact health, safety, and rights.\nRSK-04.1: Risk Register - Maintain a risk register that includes detailed records of risk analyses, focusing on health, safety, rights, democracy, and environmental aspects.\nRSK-06: Risk Remediation - Document the processes and outcomes of risk remediation efforts, especially in areas concerning health, safety, rights, and the environment.\nRSK-11: Risk Monitoring - Continuously monitor and record risk, emphasizing ongoing documentation related to health, safety, rights, democracy, and environmental risk.",
        "explainability": "The Risk Management control involves implementing a systematic process to identify, assess, and mitigate risk associated with AI deployment. The rationale for this control is to ensure that potential risk, whether ethical, technical, or legal, are proactively managed to minimize adverse impacts and foster responsible AI practices.",
        "evidence": "Rationale Explanation: Risk Management Framework: Documentation outlining the framework for risk management in AI deployment, specifying the processes for risk identification, assessment, and mitigation, as well as the responsible parties involved. \n\nRisk Register: Compilation of a risk register that systematically records identified risk, their potential impacts, and the planned mitigation strategies for each risk. \n\nTraining Program: Implementation of a training program to educate relevant stakeholders, including AI developers, operators, and decision-makers, on the risk management framework and processes. | Responsibility Explanation: Risk Analysis Records: Detailed records of all risk analyses conducted for AI systems. \n\nHealth, Safety, and Rights Impact Reports: Reports documenting the impact of AI systems on health, safety, and rights. \n\nEnvironmental Impact Assessment Documentation: Documentation of AI systems' impact on the environment. | Data Explanation: Risk Analysis Reports: Comprehensive reports documenting the identified risk and their potential impacts in areas such as health, safety, and rights. \n\nRisk Management Framework: A structured approach outlining how risk are identified, assessed, and managed. \n\nEnvironmental Impact Records: Specific records detailing any environmental risk and the measures taken to mitigate it. | Fairness Explanation: Risk Management Logs: Logs that record all identified risk, including those related to fairness, along with actions taken.\n \nHealth and Safety Fairness Records: Specific records that document the fairness considerations in health and safety risk analyses.\n \nDemocracy and Rights Risk Reports: Reports that assess risk to democracy and fundamental rights from the deployment of AI systems. | Safety & Performance  Explanation: Artifacts would include a Comprehensive Risk Register documenting all identified risk and their management strategies and a set of Risk Analysis Reports, which provide in-depth details on specific risk related to health, safety, rights, democracy, and the environment. Additionally, a Mitigation Actions Log would track the implementation and effectiveness of risk response strategies. | Impact Explanation: Comprehensive Risk Analysis Reports: In-depth reports documenting the analysis of risk related to health, safety, rights, democracy, and the environment. These reports should detail the potential risk and their mitigation strategies.  \n\nRisk Mitigation Documentation: Documentation outlining the steps taken to mitigate identified risk, including implementation plans and effectiveness assessments.  \n\nHistorical Risk Records: Records of past risk analyses and mitigation efforts, providing a historical perspective on risk management practices and effectiveness."
    },
    {
        "control_name": "Continuous AI Risk Management Framework",
        "category": "AI Risk Process Management",
        "description": "Develop a continuous and iterative risk management framework for high-risk AI systems, emphasizing regular reviews, updates, and stakeholder engagement.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 9: Risk Management System, NIST 800-53: PM-9: Risk Management Strategy\nPM-31: Continuous Monitoring Strategy\nRA-3: Risk Assessment\nCA-2: Control Assessments\nCA-7: Continuous Monitoring\nCA-9: Internal System Connections\nPL-9: Central Management\nSA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nCM-17: Protecting Controlled Unclassified Information on External Systems, SCF: GOV-04.1: Stakeholder Accountability Structure - Integrate stakeholder engagement into the risk management framework for AI systems, ensuring continuous involvement and feedback.\nPRM-01: Cybersecurity & Data Privacy Portfolio Management - Develop a portfolio management approach that includes continuous risk management processes for AI systems, with regular reviews and updates.\nRSK-01: Risk Management Program - Establish a risk management program that includes continuous and iterative processes for managing risk in high-risk AI systems.\nRSK-01.2: Risk Management Resourcing - Allocate necessary resources for the continuous risk management of AI systems, ensuring effective execution.\nRSK-11: Risk Monitoring - Implement continuous monitoring within the risk management framework to regularly review and update the risk status of AI systems.",
        "explainability": "The Continuous AI Risk Management Framework control involves establishing and maintaining a dynamic framework for ongoing identification, assessment, and mitigation of risk associated with AI deployment. The rationale for this control is to ensure that risk management is a continuous and iterative process that adapts to evolving circumstances, allowing for timely response to emerging risk.",
        "evidence": "Rationale Explanation: Continuous AI Risk Management Framework Documentation: Documentation outlining a framework for continuous AI risk management, detailing the processes for ongoing risk identification, assessment, and mitigation, as well as the mechanisms for periodic review and updates. \n\nContinuous Risk Monitoring Reports: Compilation of reports summarizing the results of continuous risk monitoring activities, highlighting identified risk, its status, and any adjustments made to the risk management strategies. \n\nTraining and Awareness Program: Implementation of a program to educate relevant stakeholders, including AI developers, operators, and decision-makers, on the continuous AI risk management framework and the importance of staying vigilant to evolving risk. | Responsibility Explanation: Risk Management Framework Document: A document outlining the continuous risk management framework.\n \nReview and Update Schedule: A schedule for regular reviews and updates of the risk management process. \n\nStakeholder Engagement Plan: A plan for engaging stakeholders in the risk management process. | Data Explanation: Continuous Risk Management Policy: A policy that dictates an ongoing and cyclical approach to risk management for AI systems. \n\nRisk Review Schedules: Timetables that establish regular intervals for risk assessments and updates. \n\nStakeholder Feedback System: A formal system for collecting and integrating feedback from various stakeholders into the risk management process. | Fairness Explanation: Risk Management Framework Documents: Frameworks outlining the continuous and iterative process for managing AI risk, including fairness.\n \nReview and Update Schedules: Schedules that dictate when AI risk management processes are reviewed and updated, with a focus on fairness. \n\nStakeholder Engagement Records: Records of stakeholder engagement in the risk management process, emphasizing the role of fairness. | Safety & Performance  Explanation: Deliverables would include a Risk Management Framework Document, which lays out the procedures and schedules for regular risk assessments and updates and a Stakeholder Engagement Report, documenting how stakeholder feedback is collected and used. A Change Management Log would also be necessary to record all updates and modifications to the AI system. | Impact Explanation: Risk Management Framework Documentation: Detailed documents outlining the continuous risk management process, including protocols for regular reviews and updates.  \n\nReview and Update Records: Records of each review and update cycle, highlighting changes made and the rationale behind them.  \n\nStakeholder Engagement Summaries: Summaries of stakeholder meetings and feedback sessions indicating how stakeholder input is integrated into the risk management process."
    },
    {
        "control_name": "Collaborative AI Risk Management",
        "category": "Stakeholder-Involved Risk Handling",
        "description": "Ensure AI system providers actively collaborate with external risk assessment experts and maintain comprehensive documentation of the risk management process.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 9: Risk Management System, NIST 800-53: PM-10: Authorization Process\nPM-15: Security and Privacy Groups and Associations\nPM-16: Threat Awareness Program\nRA-1: Risk Assessment Policy and Procedures\nRA-2: Security Categorization\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nRA-8: Privacy Impact Assessments\nSA-9: External System Services\nSA-11: Developer Testing and Evaluation\nPL-2: System Security and Privacy Plans\nPL-4: Rules of Behavior\nAU-6: Audit Record Review, Analysis, and Reporting, SCF:  PRM-04: Cybersecurity & Data Privacy In Project Management - Include collaborative risk management practices in project management processes for AI systems, ensuring involvement of external experts.\nRSK-01: Risk Management Program - Develop a risk management program that encourages collaboration with external risk assessment experts in AI system development.\nRSK-03: Risk Identification - Identify risk in collaboration with external experts to ensure a comprehensive risk profile for AI systems.\nRSK-04: Risk Assessment - Conduct risk assessments for AI systems in partnership with external risk assessment experts.\nTPM-01: Third-Party Management - Manage relationships with third-party risk assessment experts as part of the AI system risk management process.",
        "explainability": "The Collaborative AI Risk Management control involves fostering collaboration and coordination among relevant stakeholders in the identification, assessment, and mitigation of risk associated with AI deployment. The rationale for this control is to ensure that diverse perspectives are considered, and collective expertise is leveraged to address multifaceted risk effectively.",
        "evidence": "Rationale Explanation: Collaborative AI Risk Management Guidelines: Documentation outlining guidelines for collaborative risk management in AI deployment, specifying the roles and responsibilities of different stakeholders, communication mechanisms, and collaborative decision-making processes. \n\nRisk Collaboration Platform: Establishment of a collaborative platform or framework that facilitates communication, information sharing, and joint decision-making among stakeholders involved in AI risk management. \n\nCollaborative Risk Mitigation Plans: Development of collaborative risk mitigation plans that involve inputs and contributions from various stakeholders, ensuring a comprehensive and collective approach to addressing identified risk. | Responsibility Explanation: Collaborative Risk Management Protocol: A protocol for collaboration between AI providers and risk experts.\n \nRisk Management Collaboration Records: Records of all collaborative activities and decisions in risk management. \n\nExternal Expert Engagement Plan: A plan for engaging external risk assessment experts. | Data Explanation: Risk Management Plan: A detailed document outlining the risk management process, including data sources, data handling procedures, and potential risk associated with the AI system. \n\nData Usage Logs: Records of all data used by the AI system, including sources, preprocessing steps, and any data modifications. \n\nRisk Assessment Reports: Reports generated in collaboration with external risk assessment experts, detailing data-related risk and mitigation strategies. | Fairness Explanation: Collaboration Agreements with Risk Experts: Agreements that establish the terms of collaboration with external risk experts, including fairness considerations. \n\nRisk Management Collaboration Logs: Logs documenting the collaborative risk management process and fairness considerations. \n\nExternal Expertise Fairness Evaluation Reports: Reports evaluating the fairness contributions of external experts to the risk management process. | Safety & Performance  Explanation: Artifacts would include a Collaboration Protocol Document outlining the terms and structure of engagement with external experts and a Risk Management Collaboration Log, which records all instances of external consultations and their outcomes. A Comprehensive Risk Documentation File would be maintained detailing the entire risk management process. | Impact Explanation: Collaboration Agreements and Protocols: Documentation of agreements and protocols established for collaboration with external risk assessment experts. \n \nComprehensive Risk Management Documentation: Detailed records of the risk management process, including inputs from external experts and the outcomes of collaborative assessments.  \n\nExpert Feedback and Recommendations Reports: Reports containing feedback, insights, and recommendations from external risk assessment experts."
    },
    {
        "control_name": "Collaborative Risk Mitigation",
        "category": "Regulatory Partnership in Risk Reduction",
        "description": "Implement regular meetings for AI system deployers to actively cooperate with national supervisory authorities in addressing and mitigating identified risk.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 9: Risk Management System, NIST 800-53: PM-15: Security and Privacy Groups and Associations\nPM-16: Threat Awareness Program\nPM-32: Purposing\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nCA-2: Control Assessments\nCA-7: Continuous Monitoring\nCA-8: Penetration Testing\nCA-9: Internal System Connections\nPL-2: System Security and Privacy Plans\nPL-4: Rules of Behavior\nSA-12: Supply Chain Protection\nIR-4: Incident Handling, SCF: AAT-11.2: AI & Autonomous Technologies Ongoing Assessments - Continually assess AI systems for risk and collaborate with supervisory authorities to mitigate them.\nGOV-06: Contacts With Authorities - Establish formal communication channels between AI system deployers and national supervisory authorities for risk mitigation.\nGOV-07: Contacts With Groups & Associations - Facilitate collaborations between AI system deployers and regulatory groups to address and mitigate risk.\nRSK-06: Risk Remediation - Develop processes for AI system deployers to work with national supervisory authorities on risk remediation efforts.\nTPM-11: Third-Party Incident Response & Recovery Capabilities - Incorporate mechanisms for third-party deployers to engage with authorities in risk mitigation strategies.",
        "explainability": "The Collaborative Risk Mitigation control involves engaging relevant stakeholders in a collective effort to address identified risk associated with AI deployment. The rationale for this control is to ensure that diverse perspectives are considered and collaborative risk mitigation plans are developed to effectively address and minimize potential adverse impacts.",
        "evidence": "Rationale Explanation: Collaborative Risk Mitigation Guidelines: Documentation outlining guidelines for collaborative risk mitigation in AI deployment, specifying the processes for collective decision making, stakeholder involvement, and the development of collaborative risk mitigation plans. \n\nCollaborative Risk Mitigation Plans: Development of comprehensive risk mitigation plans that involve inputs and contributions from various stakeholders, ensuring a collaborative and inclusive approach to addressing identified risk.\n\nCommunication Protocols: Establishment of clear communication protocols to facilitate effective information sharing and coordination among stakeholders during the implementation of collaborative risk mitigation plans. | Responsibility Explanation: Risk Mitigation Cooperation Framework: A framework for cooperation in risk mitigation between AI deployers and supervisory authorities. \n\nMitigation Action Plan: A plan detailing actions for mitigating identified risk. \n\nCooperation and Communication Records: Records of all cooperative activities and communications with supervisory authorities. | Data Explanation: Risk Mitigation Plan: A comprehensive plan outlining the strategies and actions to address and mitigate identified risk associated with the AI system. \n\nData Governance Framework: Documentation specifying how data is managed, including data access, sharing, and updates, in collaboration with national supervisory authorities. \n\nRisk Mitigation Reports: Periodic reports detailing the progress and effectiveness of risk mitigation efforts. | Fairness Explanation: Risk Mitigation Cooperation Frameworks: Frameworks that guide the cooperation between deployers and supervisory authorities for risk mitigation, with a focus on fairness. \n\nAuthority Collaboration Records: Records of interactions and cooperative efforts with authorities to mitigate risk fairly. \n\nMitigation Action Fairness Reports: Reports on the actions taken to mitigate risk, analyzing their fairness. | Safety & Performance  Explanation: Deliverables would include a Cooperative Risk Mitigation Strategy, which outlines the joint efforts between deployers and authorities and a Supervisory Authority Interaction Record documenting all communications and interventions. An Authority-Verified Risk Mitigation Report may also be prepared evidencing the effectiveness of the mitigative actions taken. | Impact Explanation: Cooperation Framework Documentation: Detailed documentation outlining the mechanisms and procedures for collaboration with national supervisory authorities.  \n\nRisk Mitigation Action Plans: Documented plans developed in cooperation with supervisory authorities, outlining specific actions to mitigate identified risk.\n  \nCompliance and Progress Reports: Regular reports submitted to supervisory authorities, detailing progress in risk mitigation and compliance efforts."
    },
    {
        "control_name": "AI System Malfunction Tracking",
        "category": "AI-Specific Risk Assessment",
        "description": "Document and analyze all AI system malfunctions, rights breaches, and significant damages and ensure that follow up was conducted for all AI system malfunctions.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 87: Reporting of Breaches and Protection of Reporting Persons, NIST 800-53: AU-6: Audit Record Review, Analysis, and Reporting\nAU-11: Audit Record Retention\nCM-8: System Component Inventory\nIR-4: Incident Handling\nIR-5: Incident Monitoring\nIR-6: Incident Reporting\nIR-8: Incident Response Plan\nPE-6: Monitoring Physical Access\nPM-16: Threat Awareness Program\nRA-5: Vulnerability Monitoring and Scanning\nSI-4: System Monitoring\nSI-12: Information Management and Retention, SCF: AAT-11.4: AI & Autonomous Technologies Incident & Error Reporting - Develop and maintain a system for reporting and documenting AI system malfunctions and errors.\nMON-01: Continuous Monitoring - Implement continuous monitoring to document all AI system malfunctions, rights breaches, and significant damages.\nMON-03: Content of Event Logs - Ensure that event logs comprehensively capture details of AI system malfunctions, rights breaches, and damages.\nMON-06: Monitoring Reporting - Regularly report on monitoring findings, including malfunctions, rights breaches, and significant damages in AI systems.\nIRO-02: Incident Handling - Handle and document incidents related to AI system malfunctions and rights breaches as part of the incident response process.",
        "explainability": "The AI System Malfunction Tracking control involves the systematic tracking and documentation of malfunctions or anomalies in AI systems. The rationale for this control is to establish a process for identifying, recording, and analyzing instances of AI system malfunctions to ensure timely resolution, continuous improvement, and accountability.",
        "evidence": "Rationale Explanation: Malfunction Tracking System: Implementation of a system or framework for tracking AI system malfunctions, detailing the process for reporting, recording, and analyzing malfunctions. \n\nMalfunction Reports: Compilation of reports summarizing instances of AI system malfunctions, including details such as the nature of the malfunction, its impact, and the actions taken for resolution.\n \nImprovement Recommendations: Documentation of recommendations for improvements to prevent similar malfunctions in the future, based on the analysis of malfunction reports. | Responsibility Explanation: Malfunction Tracking System: A system for tracking and documenting AI system malfunctions. \n\nRights Breach Incident Log: A log of all incidents involving breaches of rights due to AI malfunctions. \n\nDamage Assessment Reports: Reports assessing the significance and impact of damages caused by AI system malfunctions. | Data Explanation: Malfunction Reports: Records of AI system malfunctions, including details on the nature of the malfunction and its impact and any associated rights breaches or damages. \n\nData Breach Logs: Documentation of any incidents where AI system operations resulted in data breaches or privacy violations. \n\nDamage Assessment Reports: Reports assessing the extent of damages caused by AI system malfunctions. | Fairness Explanation: Malfunction Tracking Protocols: Protocols that outline the procedures for tracking malfunctions, ensuring they are fair and thorough. \n\nRights Breach Incident Logs: Logs that record incidents where AI systems have breached rights, with an emphasis on fairness. \n\nDamage Recording Standards: Standards that dictate how significant damages from AI malfunctions are recorded, ensuring a fair representation of events. | Safety & Performance  Explanation: Artifacts would consist of a Malfunction and Incident Log that chronicles all relevant events, a Rights Breach Register that specifically records instances where rights have been compromised, and a Damage Report that quantifies and describes significant damages caused by the AI system. | Impact Explanation: Malfunction and Incident Logs: Comprehensive logs recording details of all AI system malfunctions, rights breaches, and significant damages, including timestamps, severity, and impact analysis.  \n\nResponse and Resolution Records: Documentation of the responses to each incident, including steps taken to resolve issues and prevent recurrence.\n  \nImpact Analysis Reports: Reports analyzing the broader impact of malfunctions and breaches, including on affected individuals and systems."
    },
    {
        "control_name": "Regular Threat Modeling",
        "category": "AI-Specific Risk Assessment",
        "description": "Develop, implement and conduct regular threat modeling exercises to identify potential vulnerabilities in AI systems and data.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nArticle 53: Obligations for Providers of General Purpose AI Models\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: RA-3: Risk Assessment\nRA-9: Risk Management Strategy\nRA-9: Criticality Analysis\nSA-15: Development Process, Standards, and Tools\nSA-17: Developer Security and Privacy Architecture and Design\nCA-2: Control Assessments\nCA-8: Penetration Testing\nSI-2: Flaw Remediation\nSI-4: System Monitoring\nSI-5: Security Alerts, Advisories, and Directives\nPM-12: Insider Threat Program, SCF: AAT-07.2: AI & Autonomous Technologies Likelihood & Impact Risk Analysis - Perform likelihood and impact risk analysis as part of threat modeling exercises for ML systems.\nAAT-10.2: AI TEVV Tools - Utilize tools for threat modeling and evaluation in ML systems to detect and analyze vulnerabilities.\nMON-01.7: File Integrity Monitoring (FIM) - Apply file integrity monitoring in the context of threat modeling to identify changes that might indicate vulnerabilities in ML systems.\nRSK-04: Risk Assessment - Conduct regular risk assessments that include threat modeling for machine learning (ML) systems to identify potential vulnerabilities.\nTDA-06.2: Threat Modeling - Specifically incorporate threat modeling exercises in the development and assessment of ML systems to identify and address vulnerabilities.",
        "explainability": "The Regular Threat Modeling control involves the systematic and periodic analysis of potential threats to an AI system. The rationale for this control is to proactively identify and assess potential security and operational risk, enabling the implementation of effective countermeasures and ensuring the ongoing resilience of the AI system.",
        "evidence": "Rationale Explanation: Threat Modeling Documentation: Creation of documentation outlining the processes and methodologies for conducting regular threat modeling exercises, detailing the identification, analysis, and mitigation of potential threats. \n\nThreat Assessment Reports: Compilation of reports summarizing the results of threat modeling exercises, including identified threats, their potential impacts, and recommended countermeasures. \n\nMitigation Implementation Plan: Development of a plan outlining the implementation of countermeasures to address identified threats, based on the analysis from threat modeling exercises. | Responsibility Explanation: Threat Modeling Exercise Plan: A plan for regular threat modeling exercises. \n\nVulnerability Assessment Reports: Reports documenting identified vulnerabilities in machine learning systems.\n\nCybersecurity Training Modules: Training materials for staff on conducting threat modeling exercises. | Data Explanation: Threat Modeling Reports: Documentation of threat modeling exercises, including identified vulnerabilities related to machine learning systems and data. \n\nData Vulnerability Assessment: Assessment of data-related vulnerabilities and potential data breaches. | Fairness Explanation: Threat Modeling Reports: Reports that document identified threats to fairness and proposed mitigations. \n\nVulnerability Fairness Analysis: Analysis of vulnerabilities with specific considerations of their impact on fairness.\n \nThreat Resolution Records: Records of how identified threats have been addressed, with a focus on maintaining or enhancing fairness. | Safety & Performance  Explanation: Expanded deliverables would encompass an in-depth Threat Modeling Framework Implementation Guide, a series of Detailed Threat Reports that dissect each identified threat and its potential impact, and a Comprehensive Vulnerability Response Strategy. Each exercise would result in an Updated Threat Model Matrix, mapping out new and existing threats, and their mitigation statuses. A Historical Threat Analysis Document might also be created to track threat evolution over time. | Impact Explanation: Threat Modeling Reports: Detailed reports generated from each threat modeling exercise, outlining identified vulnerabilities, potential attack vectors, and risk levels.  \n\nVulnerability Mitigation Plans: Documented strategies and actions planned or taken to address identified vulnerabilities in machine learning systems and data.  \n\nExercise Schedules and Protocols: Documentation of the schedules, protocols, and methodologies used for regular threat modeling exercises."
    },
    {
        "control_name": "Quality-Based AI Development",
        "category": "AI Quality & Safety Standards",
        "description": "Develop high-risk AI systems that adhere to established quality criteria for datasets, allowing for adaptability based on the system's market segment or application.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 17: Quality Management System\nArticle 57: AI Regulatory Sandboxes\nRecital 139: Emphasizes supervision of AI systems in the AI regulatory sandbox (i.e., development, training, testing and validation) before a system is placed in service., NIST 800-53: SA-4: Acquisition Process\nSA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nSA-17: Developer Security and Privacy Architecture and Design\nSI-2: Flaw Remediation\nSI-3: Malicious Code Protection\nSI-8: Software, Firmware, and Information Integrity\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nPL-8: Security and Privacy Architectures\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning, SCF: SEA-02: Alignment With Enterprise Architecture - Ensure that AI development is aligned with enterprise architecture, adhering to quality criteria and adaptability standards for different market segments.\nSEA-07.1: Technology Lifecycle Management - Manage the lifecycle of AI systems, focusing on maintaining quality standards throughout development and deployment.\nTDA-01.2: Integrity Mechanisms for Software / Firmware Updates - Incorporate integrity mechanisms in AI development to maintain data quality and system adaptability.\nTDA-06: Secure Coding - Apply secure coding practices in AI development, adhering to quality criteria for datasets and system adaptability.\nTDA-09.6: Secure Settings By Default - Ensure AI systems are developed with secure settings by default, reflecting quality standards suitable for their intended market segment or application.",
        "explainability": "The Quality-Based AI Development control involves implementing a development process that prioritizes and ensures the quality of AI systems throughout their life cycle. The rationale for this control is to establish a systematic approach to development that emphasizes reliability, accuracy, and performance, ultimately delivering AI solutions that meet high-quality standards.",
        "evidence": "Rationale Explanation: Quality Assurance Plan: Development and documentation of a plan outlining the quality assurance processes and methodologies to be applied throughout the AI system development life cycle.\n\nQuality Metrics Framework: Establishment of a framework defining key quality metrics and benchmarks that will be used to assess the performance, reliability, and accuracy of the AI system during development. \n\nContinuous Quality Monitoring: Implementation of mechanisms for continuous monitoring and assessment of the AI system's quality throughout its development stages. | Responsibility Explanation: Quality Criteria Development Guide: A guide outlining quality criteria for AI system datasets.\n\nMarket Segment Adaptability Plan: A plan for adapting AI systems to different market segments. \n\nQuality Assurance Protocol: A protocol for ensuring AI systems meet established quality criteria. | Data Explanation: Data Quality Criteria Documentation: Detailed documentation specifying the quality criteria for datasets used in AI system development. \n\nMarket Segment or Application Adaptability Plan: Documentation outlining how data quality criteria can be adapted based on the AI system's market segment or application. | Fairness Explanation: Quality Criteria Documentation: Documentation that outlines the quality criteria for datasets and development processes with fairness considerations. \n\nAdaptability Fairness Guidelines: Guidelines that ensure AI systems are adaptable in a fair manner to different market segments. \n\nDevelopment Quality Assurance Records: Records of quality assurance practices and how they adhere to fairness standards. | Safety & Performance  Explanation: Artifacts would include a Quality Management System (QMS) Document tailored for AI development outlining the quality criteria and processes. A Dataset Quality Assurance Report would detail the measures taken to ensure datasets meet established criteria, and a Market Segment Adaptability Analysis would show how the system's quality criteria are customized for various applications. | Impact Explanation: Quality Criteria Documentation: Comprehensive documentation of the established quality criteria for datasets, including standards for accuracy, completeness, and representativeness.  \n\nDevelopment Process Records: Detailed records of the AI system development process, demonstrating adherence to quality criteria.  \n\nMarket Segment-Specific Adaptations: Documentation showing how the AI system’s development adapts to the specific needs and characteristics of its intended market segment or application."
    },
    {
        "control_name": "Safety & Quality Assurance for All AI Systems",
        "category": "Comprehensive AI Safety Protocols",
        "description": "Ensure the enterprise adheres to safety and quality assurance practices, regardless of an AI system's risk classification. Ensure alignment with the appropriate guidance (e.g., Directive 2001/95/EC).",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 17: Quality Management System\nArticle 57: AI Regulatory Sandboxes\nRecital 139: Emphasizes supervision of AI systems in the AI regulatory sandbox (i.e., development, training, testing and validation) before a system is placed in service., NIST 800-53: SA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nSA-17: Developer Security and Privacy Architecture and Design\nSA-4: Acquisition Process\nSI-2: Flaw Remediation\nSI-3: Malicious Code Protection\nSI-7: Software, Firmware, and Information Integrity\nPL-8: Security and Privacy Architectures\nPL-2: System Security and Privacy Plans\nCM-3: Configuration Change Control\nCM-4: Impact Analysis\nRA-5: Vulnerability Monitoring and Scanning, SCF: SEA-01: Secure Engineering Principles - Apply secure engineering principles in the design and development of AI systems, ensuring safety and quality assurance in line with Directive 2001/95/EC.\nSEA-07: Predictable Failure Analysis - Conduct failure analysis of AI systems to ensure predictable responses, as part of safety and quality assurance practices.\nSEA-10: Memory Protection - Implement memory protection techniques in AI systems to enhance safety and quality assurance.\nTDA-02: Minimum Viable Product (MVP) Security Requirements - Ensure MVPs for AI systems meet safety and quality assurance standards, in accordance with Directive 2001/95/EC.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Continuously test AI systems for cybersecurity and data privacy throughout development, as part of safety and quality assurance measures.",
        "explainability": "The Safety & Quality Assurance for All AI Systems control involves the establishment of a comprehensive safety and quality assurance framework that ensures the reliability, security, and performance of all AI systems. The rationale for this control is to mitigate risk, adhere to safety standards, and prioritize the quality of AI solutions across diverse applications and domains.",
        "evidence": "Rationale Explanation: Safety and Quality Assurance Framework: Development and documentation of a framework outlining safety and quality assurance processes, methodologies, and standards to be applied universally across all AI systems. \n\nAssurance Checklists: Creation of checklists specifying safety and quality assurance criteria that must be met during the development, deployment, and operation of AI systems.\n\nUniversal Safety Standards Compliance Report: Compilation of reports summarizing the adherence of AI systems to universal safety standards, highlighting any deviations or areas for improvement. | Responsibility Explanation: Safety and Quality Assurance Policy: A policy outlining safety and quality assurance practices for AI systems. \n\nCompliance Checklist with Directive 2001/95/EC: A checklist ensuring alignment with the guidelines of Directive 2001/95/EC. \n\nSafety Training Program for Staff: A training program for staff on safety and quality assurance practices. | Data Explanation: Safety and Quality Assurance Documentation: Documentation outlining the safety and quality assurance practices followed for AI systems, including data-related aspects. \n\nCompliance Reports: Reports demonstrating alignment with Directive 2001/95/EC guidelines, particularly in data handling and quality assurance. | Fairness Explanation: Safety and Quality Assurance Policies: Policies that integrate safety and quality assurance with fairness for all AI systems. \n\nDirective Compliance Checklists: Checklists that ensure compliance with Directive 2001/95/EC, including fairness aspects.\n \n Quality Assurance Fairness Audit Reports: Audit reports that evaluate safety and quality assurance practices for their adherence to fairness. | Safety & Performance  Explanation: Deliverables would include a Safety and Quality Assurance Policy that aligns with Directive 2001/95/EC, a Compliance Audit Report detailing findings related to the directive's guidelines, and a Quality Control Checklist used across the AI system's development and deployment phases. A Safety Case File, documenting the safety assessments and compliance evidence for each AI system, would also be vital. | Impact Explanation: Safety and Quality Assurance Policies: Documented policies and procedures that outline the safety and quality assurance practices in line with Directive 2001/95/EC.  \n\nCompliance Audit Reports: Reports from audits conducted to assess compliance with the safety and quality assurance practices stipulated in the directive. \n \nAI System Safety Certifications: Certifications or proof of compliance demonstrating that the AI systems meet the safety and quality standards outlined in the directive."
    },
    {
        "control_name": "Safety Assurance for All AI Systems",
        "category": "AI Safety Verification",
        "description": "Ensure the enterprise implements regular safety and compliance checks, regardless of an AI system's risk classification, to ensure they meet minimum safety standards when released to the market.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 17: Quality Management System\nArticle 57: AI Regulatory Sandboxes\nRecital 139: Emphasizes supervision of AI systems in the AI regulatory sandbox (i.e., development, training, testing and validation) before a system is placed in service., NIST 800-53: SA-11: Developer Testing and Evaluation\nSA-4: Acquisition Process\nSA-10: Developer Configuration Management\nSA-17: Developer Security and Privacy Architecture and Design\nCM-4: Impact Analysis\nCM-6: Configuration Settings\nPL-2: System Security and Privacy Plans\nPL-8: Security and Privacy Architectures\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSI-2: Flaw Remediation\nSI-7: Software, Firmware, and Information Integrity, SCF: SEA-01: Secure Engineering Principles - Apply secure engineering principles to all AI systems to ensure they meet minimum safety standards upon market release.\nSEA-07: Predictable Failure Analysis - Conduct failure analysis on AI systems to verify safety standards are met, irrespective of their risk classification.\nSEA-17: Secure Log-On Procedures - Implement secure log-on procedures as part of the safety assurance process for all AI systems.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Continuously test AI systems during development for compliance with safety standards.\nTDA-17: Unsupported Systems - Manage unsupported systems and components in AI development to ensure ongoing safety and compliance.",
        "explainability": "The Safety Assurance for All AI Systems control involves implementing a comprehensive safety assurance framework to ensure the reliability and security of all AI systems. The rationale for this control is to mitigate risk, adhere to safety standards, and prioritize the safety of AI solutions across diverse applications and domains.",
        "evidence": "Rationale Explanation: Safety Assurance Framework: Development and documentation of a framework outlining safety assurance processes, methodologies, and standards to be universally applied across all AI systems. \n\nSafety Compliance Reports: Compilation of reports summarizing the adherence of AI systems to safety standards, highlighting any deviations or areas for improvement. \n\nContinuous Safety Monitoring Mechanisms: Implementation of mechanisms for continuous monitoring and assessment of the safety of AI systems throughout their life cycle. | Responsibility Explanation: Safety Compliance Procedure: A procedure for conducting safety checks on AI systems. \n\nMinimum Safety Standards Checklist: A checklist of minimum safety standards for AI system release. \n\nCompliance Reporting System: A system for reporting compliance with safety standards. | Data Explanation: Safety and Compliance Documentation: Documentation specifying the safety and compliance checks conducted for AI systems, including data-related aspects. \n\nSafety Assessment Reports: Reports detailing the outcomes of safety assessments, including data-related findings. | Fairness Explanation: Safety Compliance Frameworks: Frameworks that outline safety compliance checks, including considerations for fairness across all AI systems. \n\nNon-High-Risk AI Safety Reports: Reports that document the safety checks performed on non-high-risk AI systems, with fairness assessments. \n\nMinimum Standards Compliance Records: Records demonstrating compliance with minimum safety standards and how these standards relate to fairness. | Safety & Performance  Explanation: Deliverables would include a Universal Safety Standards Checklist, which outlines the minimum safety criteria applicable to all AI systems and a Compliance Certification for each system released, affirming its adherence to these standards. A Premarket Safety Assessment Report would also be generated documenting the results of safety checks performed prior to release. | Impact Explanation: Safety Compliance Checklists: Detailed checklists used to conduct safety and compliance checks, ensuring all AI systems meet minimum safety standards.  \n\nSafety Assessment Reports: Reports documenting the outcomes of safety assessments, detailing any identified issues and their resolutions.  \n\nMarket Release Authorization Documents: Documents or certifications that authorize the release of AI systems to the market after confirming their safety compliance."
    },
    {
        "control_name": "Security by Design for High-Risk AI Systems",
        "category": "Proactive High-Risk AI Safeguarding",
        "description": "Conduct a comprehensive security assessment before deploying high-risk AI systems. This should include penetration testing, vulnerability scanning, and, where feasible, third-party security evaluations to validate system robustness.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 17: Quality Management System\nArticle 57: AI Regulatory Sandboxes\nRecital 139: Emphasizes supervision of AI systems in the AI regulatory sandbox (i.e., development, training, testing and validation) before a system is placed in service., NIST 800-53: CA-2: Control Assessments\nCA-8: Penetration Testing\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSA-11: Developer Testing and Evaluation\nSR-6: Supplier Assessments and Reviews\nSA-17: Developer Security and Privacy Architecture and Design\nSI-2: Flaw Remediation\nCM-2: Baseline Configuration\nCM-4: Impact Analysis\nCM-6: Configuration Settings\nPL-8: Security and Privacy Architectures, SCF: SEA-01: Secure Engineering Principles - Implement secure engineering principles during the design phase of high-risk AI systems, ensuring comprehensive security is built-in from the start.\nSEA-03: Defense-In-Depth (DiD) Architecture - Design high-risk AI systems with a defense-in-depth architecture, focusing on layered security and robustness.\nTDA-09.5: Application Penetration Testing - Conduct penetration testing on high-risk AI systems to identify and mitigate potential security vulnerabilities.\nVPM-06: Vulnerability Scanning - Regularly perform vulnerability scanning on high-risk AI systems, as part of the comprehensive security assessment.\nVPM-07: Penetration Testing - Engage in penetration testing as a key component of the security assessment for high-risk AI systems, identifying vulnerabilities and enhancing system security.",
        "explainability": "The Security by Design for High-Risk AI Systems control involves incorporating security measures as a fundamental aspect of the design and development process for high-risk AI systems. The rationale for this control is to proactively address security concerns, prevent vulnerabilities, and ensure robust security measures throughout the life cycle of high-risk AI systems.",
        "evidence": "Rationale Explanation: Security Design Guidelines: Development and documentation of guidelines outlining the incorporation of security measures at each stage of the design and development process for high-risk AI systems.\n\nSecurity Architecture Documentation: Creation of documentation detailing the security architecture of high-risk AI systems, including key security features, protocols, and measures implemented. \n\nSecurity Verification Plan: Development of a plan specifying the methods and processes for verifying the security measures implemented in high-risk AI systems. | Responsibility Explanation: Security Assessment Plan: A plan for conducting comprehensive security assessments. \n\nPenetration Testing and Vulnerability Scanning Reports: Reports documenting the findings of security assessments. \n\nExternal Security Evaluation Protocol: A protocol for involving third-party security evaluations. | Data Explanation: Security Assessment Plan: A plan outlining the comprehensive security assessment, including details on data-related aspects. \n\nPenetration Testing Reports: Reports detailing the results of penetration testing, including data-related vulnerabilities. \n\nVulnerability Scanning Reports: Reports from vulnerability scanning assessments, highlighting data-related vulnerabilities. \n\nThird-Party Security Evaluation Reports: Reports from third-party evaluations focusing on data security. | Fairness Explanation: Security Assessment Protocols: Protocols for conducting comprehensive security assessments that consider fairness. \n\nPenetration Testing Reports with Fairness Analysis: Reports from penetration tests that include an analysis of fairness implications. \n\nThird-Party Security Evaluation Summaries: Summaries of third-party security evaluations that validate system robustness and fairness. | Safety & Performance  Explanation: Deliverables would include a Security Assessment Report that details the findings from penetration tests and vulnerability scans, a Third-Party Evaluation Summary that compiles insights from external security experts, and a Security by Design Certification that verifies the system has been designed with integral security features. | Impact Explanation: Security Assessment Reports: Detailed reports from the comprehensive security assessments, including findings from penetration testing and vulnerability scanning.  \n\nThird-Party Evaluation Certificates: Certificates or reports from third-party security evaluations, validating the robustness and security of the AI systems.  \n\nSecurity Design Documentation: Documentation of the AI system's security features and protocols, demonstrating the incorporation of security considerations from the design phase."
    },
    {
        "control_name": "Prelaunch Security Assessment",
        "category": "Security Quality, Testing, & Assessment",
        "description": "Design high-risk AI systems with built-in automatic event logging capabilities. Adhere to state-of-the-art logging standards, capturing essential data for traceability and postmarket monitoring.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 17: Quality Management System\nArticle 57: AI Regulatory Sandboxes\nRecital 139: Emphasizes supervision of AI systems in the AI regulatory sandbox (i.e., development, training, testing and validation) before a system is placed in service., NIST 800-53: AU-2: Event Logging\nAU-3: Content of Audit Records\nAU-6: Audit Record Review, Analysis, and Reporting\nAU-11: Audit Record Retention\nAU-12: Audit Record Generation\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nPL-4: Rules of Behavior\nSA-10: Developer Configuration Management\nSA-11: Developer Testing and Evaluation\nSI-4: System Monitoring\nSI-7: Software, Firmware, and Information Integrity, SCF: CFG-02.2: Automated Central Management & Verification - Utilize automated central management for event logging in high-risk AI systems, aligning with best practices.\nMON-03: Content of Event Logs - Ensure that event logs in high-risk AI systems capture essential data for traceability and post-market monitoring.\nSEA-01: Secure Engineering Principles - Incorporate automatic event logging capabilities in the design of high-risk AI systems, ensuring adherence to state-of-the-art standards.\nSEA-05: Information In Shared Resources - Implement information logging in shared resources as part of the security design for high-risk AI systems.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include rigorous testing of security features, including event logging, in the pre-launch assessment of high-risk AI systems.",
        "explainability": "The Prelaunch Security Assessment control involves conducting a thorough security assessment before the launch of an AI system to identify and address potential security vulnerabilities. The rationale for this control is to ensure that the AI system meets high-security standards, minimizing the risk of security breaches upon deployment.",
        "evidence": "Rationale Explanation: Prelaunch Security Assessment Plan: Development and documentation of a plan outlining the methodologies, tools, and processes for conducting the prelaunch security assessment. \n\nSecurity Assessment Report: Compilation of a comprehensive report summarizing the findings, vulnerabilities, and recommended mitigations identified during the prelaunch security assessment. \n\nVerification of Mitigations: Documentation confirming the implementation and effectiveness of the recommended security mitigations identified in the prelaunch security assessment. | Responsibility Explanation: Security Design and Logging Standards: Standards for security design and logging in AI systems. \n\nEvent Logging System Implementation Guide: A guide for implementing event logging systems. \n\nCompliance Checklist for Security Assessment: A checklist for ensuring compliance with security assessment standards. | Data Explanation: Event Logging Design Specifications: Documentation specifying the design of automatic event logging capabilities, including data elements to capture. \n\nEvent Logs: Records of events captured by the system, including data elements for traceability and monitoring. | Fairness Explanation: Security Assessment Protocols with Fairness Considerations: Protocols that outline security assessments, including how event logging contributes to fairness. \n\nEvent Logging Standards Documentation: Documentation detailing the logging standards followed, emphasizing the capture of data relevant to fairness. \n\nTraceability and Monitoring Fairness Reports: Reports that evaluate the system’s traceability and monitoring capabilities from a fairness perspective. | Safety & Performance  Explanation: Deliverables would include a Logging Capability Design Document outlining the event logging architecture and how it adheres to state-of-the-art standards and an Event Log Report, which samples and demonstrates the data captured during system operations. Additionally, a Prelaunch Logging Test Summary would confirm the system's logging capabilities before market release. | Impact Explanation: Event Logging System Documentation: Documentation detailing the event logging capabilities of the AI system, including standards and protocols used for data capture and storage.\n  \nCompliance Verification Reports: Reports verifying compliance with state-of-the-art logging standards and practices.  \n\nPrelaunch Testing Results: Results from prelaunch testing exercises that validate the effectiveness and reliability of the event logging system."
    },
    {
        "control_name": "Prerelease Testing of High-Risk AI Systems",
        "category": "Security Quality, Testing, & Assessment",
        "description": "Establish a mechanism for the national supervisory authority to organize and oversee the testing of high-risk AI systems upon request. This involves coordinating with AI providers, setting testing parameters, and ensuring comprehensive evaluation.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 17: Quality Management System\nArticle 57: AI Regulatory Sandboxes\nRecital 139: Emphasizes supervision of AI systems in the AI regulatory sandbox (i.e., development, training, testing and validation) before a system is placed in service., NIST 800-53: CA-2: Control Assessments\nCA-3: Information Exchange\nCA-6: Security Authorization\nCM-3: Configuration Change Control\nCM-4: Impact Analysis\nPM-15: Security and Privacy Groups and Associations\nPM-16: Threat Awareness Program\nRA-3: Risk Assessment\nSA-11: Developer Testing and Evaluation\nSR-6: Supplier Assessments and Reviews\nSA-15: Development Process, Standards, and Tools\nSI-2: Flaw Remediation, SCF: GOV-06: Contacts With Authorities - Set up communication and coordination mechanisms with the national supervisory authority for overseeing high-risk AI system testing.\nIAC-15: Account Management - Manage accounts involved in AI system testing, ensuring secure access and oversight by the national supervisory authority.\nSEA-11: Honeypots - Utilize honeypots and similar strategies during testing to evaluate AI systems' security responses and vulnerabilities.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Include comprehensive testing phases as part of the AI system development, especially for high-risk AI systems, under the guidance of national supervisory authorities.\nTDA-09.5: Application Penetration Testing - Conduct application penetration testing as part of pre-release testing, overseen by national supervisory authorities.",
        "explainability": "The Prerelease Testing of High-Risk AI Systems control involves conducting comprehensive testing activities before the release of high-risk AI systems to ensure their reliability, accuracy, and safety. The rationale for this control is to identify and address potential issues, vulnerabilities, and performance concerns, minimizing the risk of adverse impacts upon deployment.",
        "evidence": "Rationale Explanation: Prerelease Testing Plan: Development and documentation of a plan outlining the testing methodologies, scenarios, and criteria for assessing the reliability, accuracy, and safety of high-risk AI systems before release. \n\nTest Reports: Compilation of reports summarizing the results, findings, and any identified issues during the prerelease testing of high-risk AI systems. \n\nVerification of Issue Resolutions: Documentation confirming the resolution and mitigation of issues identified during prerelease testing. | Responsibility Explanation: Testing Coordination Procedure: A procedure for coordinating prerelease testing of high-risk AI systems. \n\nTesting Parameter Guidelines: Guidelines for setting testing parameters. \n\nComprehensive Evaluation Report Template: A template for documenting the evaluation of AI systems. | Data Explanation: Testing Oversight Mechanism Documentation: Documentation outlining the mechanism for national supervisory authority oversight of high-risk AI system testing, including data-related aspects. \n\nTesting Parameters: Specifications for testing parameters, including data-related criteria. \n\nTesting Reports: Reports generated as a result of the oversight and testing process, focusing on data-related findings. | Fairness Explanation: Testing Mechanism Framework: A framework that outlines the process for supervisory authority-organized testing, focusing on fairness evaluation. \n\nTesting Coordination Reports: Reports detailing the coordination between providers and authorities, with emphasis on fairness in testing parameters. \n\nEvaluation Fairness Checklists: Checklists used during testing to ensure a comprehensive evaluation of fairness aspects. | Safety & Performance  Explanation: Deliverables would include a Supervisory Authority Testing Protocol, which defines the methodologies and standards for testing and a Testing Coordination Plan that outlines the roles and responsibilities of the AI providers in the testing process. A Testing Report would be generated upon completion that documents the outcomes and any recommendations. | Impact Explanation: Testing Coordination Plans: Detailed plans outlining the coordination process between the national supervisory authority and AI providers for testing high-risk AI systems.  \n\nTest Parameter Documentation: Documentation specifying the testing parameters, including objectives, scope, methodologies, and success criteria.  \n\nEvaluation Reports: Comprehensive reports from the supervisory authority detailing the testing outcomes, highlighting areas of compliance, concerns, and recommendations for improvement."
    },
    {
        "control_name": "Model Obfuscation for Code Protection",
        "category": "AI-Specific Patch & Configuration Management",
        "description": "Obfuscate model code to prevent reverse engineering by attackers.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 26: Obligations of Deployers of High-Risk AI Systems\nArticle 55: Obligations for Providers of General-Purpose AI Models with Systemic Risk\nRecital 70: Emphasizes anonymization and encryption\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit.\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks., NIST 800-53: SA-15: Development Process, Standards, and Tools\nSC-28: Protection of Information at Rest\nSC-38: Operations Security\nSC-18: Mobile Code\nSC-39: Process Isolation\nSC-40: Wireless Link Protection\nSC-41: Port and I/O Device Access\nSI-3: Malicious Code Protection\nSI-7: Software, Firmware, and Information Integrity, SCF: CFG-02: System Hardening Through Baseline Configurations - Implement system hardening techniques, including model code obfuscation, to prevent reverse engineering.\nCFG-03: Least Functionality - Ensure that AI models operate with the least functionality necessary, with obfuscation techniques applied to protect code.\nSEA-13: Heterogeneity - Utilize heterogeneity in coding and design, including obfuscation, to protect AI models from reverse engineering attacks.\nSEA-14: Concealment & Misdirection - Apply concealment and misdirection techniques, such as code obfuscation, to secure AI models against unauthorized access and reverse engineering.\nTDA-06: Secure Coding - Apply secure coding practices that include obfuscation of model code to enhance protection against reverse engineering.",
        "explainability": "The Model Obfuscation for Code Protection control involves implementing techniques to obfuscate the code of AI models, enhancing security and protecting intellectual property. The rationale for this control is to prevent unauthorized access, reverse engineering, or tampering with the underlying code of AI models.",
        "evidence": "Rationale Explanation: Model Obfuscation Techniques: Documentation outlining the specific techniques and methods employed for obfuscating the code of AI models, ensuring protection against reverse engineering. \n\nObfuscated Model Code: Generation of obfuscated versions of AI model code that demonstrate the application of obfuscation techniques to protect intellectual property. \n\nCode Protection Verification Plan: Development of a plan specifying the methods and processes for verifying the effectiveness of model obfuscation in protecting the code. | Responsibility Explanation: Code Obfuscation Techniques Guide: A guide detailing techniques for model code obfuscation. \n\nObfuscation Implementation Record: A record of implementing code obfuscation in AI models. \n\nSecurity Training on Code Protection: Training materials for developers on code obfuscation techniques. | Data Explanation: Code Obfuscation Plan: Documentation outlining the strategy and techniques used for obfuscating model code, including data protection considerations. \n\nObfuscated Model Code: The actual obfuscated model code. | Fairness Explanation: Code Obfuscation Guidelines with Fairness Implications: Guidelines that dictate how model code should be obfuscated to prevent tampering while considering fairness. \n\nObfuscation Technique Fairness Analysis: Analysis of obfuscation techniques for their impact on the fairness of the model's operation. \n\nReverse Engineering Prevention Reports: Reports on the measures taken to prevent reverse engineering and their relevance to maintaining fairness. | Safety & Performance  Explanation: Deliverables would include an Obfuscation Techniques Document detailing the methods used to obfuscate the code and an Obfuscated Model Codebase, which is the result of applying these techniques. There would also be an Obfuscation Efficacy Report evaluating the strength of the obfuscation against reverse engineering attempts. | Impact Explanation: Obfuscation Technique Documentation: Detailed documentation of the obfuscation techniques used, including methodologies, rationale, and expected outcomes.  \n\nSecurity Testing Reports: Reports from security testing exercises demonstrating the effectiveness of the obfuscation in preventing reverse engineering.  \n\nCode Integrity Verification Records: Records verifying the integrity of the obfuscated model code, ensuring it functions as intended without compromising performance or security."
    },
    {
        "control_name": "Model Information Security",
        "category": "AI Model Protection",
        "description": "Secure model information to prevent potential cyberattacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 26: Obligations of Deployers of High-Risk AI Systems\nArticle 55: Obligations for Providers of General-Purpose AI Models with Systemic Risk\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit.\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks., NIST 800-53: SC-28: Protection of Information at Rest\nSC-13: Cryptographic Protection\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nSC-7: Boundary Protection\nSC-8: Transmission Confidentiality and Integrity\nSC-12: Cryptographic Key Establishment and Management\nSA-17: Developer Security and Privacy Architecture and Design\nSI-7: Software, Firmware, and Information Integrity\nSI-3: Malicious Code Protection\nCM-6: Configuration Settings\nRA-5: Vulnerability Monitoring and Scanning, SCF: CFG-08: Sensitive / Regulated Data Access Enforcement - Enforce strict access controls for sensitive AI model information to protect against unauthorized access and cyber-attacks.\nCRY-05: Encrypting Data At Rest - Encrypt AI model information when stored to ensure its security against cyber-attacks.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles specifically to protect AI model information against cyber-attacks.\nSEA-04: Process Isolation - Use process isolation techniques in AI systems to secure model information and prevent unauthorized access.\nTDA-04: Documentation Requirements - Maintain thorough documentation of AI model design, ensuring information security protocols are in place.",
        "explainability": "The Model Information Security control involves implementing measures to secure the information related to AI models, including architecture, parameters, and training data. The rationale for this control is to protect sensitive information, prevent unauthorized access, and ensure the confidentiality and integrity of AI model details.",
        "evidence": "Rationale Explanation: Model Information Security Policies: Development and documentation of policies outlining the security measures and protocols for safeguarding information related to AI models, including architecture, parameters, and training data. \n\nSecure Model Information Repository: Establishment of a secure repository or storage system for storing AI model information with access controls, encryption, and audit trails. \n\nInformation Security Training Program: Implementation of a training program to educate personnel on the importance of model information security and the proper handling of sensitive AI model details. | Responsibility Explanation: Model Information Security Protocol: A protocol for securing model information.\n \nCybersecurity Measures Implementation Record: A record of implementing cybersecurity measures for model protection. \n\nInformation Security Training Modules: Training materials for IT staff on securing model information. | Data Explanation: Model Information Security Plan: Documentation outlining the strategies and measures implemented to secure model information, including data-related security controls. \n\nSecurity Logs: Records of security-related events and activities related to model information. | Fairness Explanation: Information Security Protocols with Fairness Considerations: Protocols that outline measures for securing model information, emphasizing the preservation of fairness. \n\nCybersecurity Fairness Measures Documentation: Documentation of cybersecurity measures taken to protect model information and their implications for fairness. \n\nModel Security Breach Fairness Reports: Reports detailing any security breaches of model information and the subsequent fairness impact analysis. | Safety & Performance  Explanation: Deliverables would include a Model Information Security Plan, which documents the security measures in place and an Information Security Audit Report, which provides an analysis of the model's security posture. A Cybersecurity Incident Response Plan would also be essential to outline actions to take in case of a security breach. | Impact Explanation: Information Security Policy Documents: Detailed documents outlining the policies and procedures in place to secure model information.  \n\nSecurity Implementation Reports: Reports detailing the specific security measures implemented to protect model information, including encryption, access controls, and monitoring systems.  \n\nIncident Response Plans: Documented plans outlining the procedures for responding to potential security incidents involving model information."
    },
    {
        "control_name": "AI System Integrity Maintenance",
        "category": "AI System Resilience",
        "description": "Maintain AI system integrity against adversarial attacks or operational changes.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 26: Obligations of Deployers of High-Risk AI Systems\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: SI-7: Software, Firmware, and Information Integrity\nSI-3: Malicious Code Protection\nSI-16: Memory Protection\nSI-20: Tainting\nSC-7: Boundary Protection\nSC-8: Transmission Confidentiality and Integrity\nSC-28: Protection of Information at Rest\nSC-39: Process Isolation\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nCM-6: Configuration Settings\nCM-7: Least Functionality\nSA-17: Developer Security and Privacy Architecture and Design, SCF: CFG-02: System Hardening Through Baseline Configurations - Apply system hardening techniques to AI systems to ensure their integrity is preserved against adversarial attacks and changes in their operational environment.\nMON-01.6: Host-Based Devices - Monitor AI systems using host-based devices to detect and respond to integrity threats or operational anomalies.\nSEA-01: Secure Engineering Principles - Implement secure engineering principles to maintain AI system integrity, particularly against adversarial attacks and operational changes.\nSEA-07: Predictable Failure Analysis - Use failure analysis methods to ensure AI system integrity under different scenarios, including adversarial attacks or operational changes.\nTDA-06.4: Supporting Toolchain - Utilize a robust toolchain in AI development that supports system integrity, especially in the face of adversarial challenges or changes in operation.",
        "explainability": "The AI System Integrity Maintenance control involves implementing processes and measures to maintain the integrity of the AI system throughout its life cycle. The rationale for this control is to ensure that the AI system remains secure, reliable, and free from unauthorized modifications or tampering.",
        "evidence": "Rationale Explanation: Integrity Maintenance Procedures: Development and documentation of procedures outlining the steps and measures taken to maintain the integrity of the AI system, including regular checks, updates, and verification processes.\n \nIntegrity Monitoring Reports: Compilation of regular reports summarizing the results of integrity monitoring activities, including any detected anomalies, deviations, or security incidents. \n\nVerification of System Integrity: Documentation confirming the successful verification of AI system integrity through scheduled checks and monitoring activities. | Responsibility Explanation: System Integrity Maintenance Plan: A plan for maintaining the integrity of AI systems. \n\nAdversarial Attack Response Procedure: A procedure for responding to adversarial attacks on AI systems. \n\nIntegrity Check Record: A record of regular integrity checks on AI systems. | Data Explanation: AI System Integrity Maintenance Plan: Documentation outlining strategies and measures for maintaining AI system integrity, including data-related safeguards. \n\nIntegrity Monitoring Reports: Reports documenting the results of integrity monitoring efforts, with a focus on data-related aspects. | Fairness Explanation: Integrity Maintenance Frameworks: Frameworks that outline the measures for maintaining AI system integrity, with a focus on fairness. \n\nAdversarial Attack Fairness Response Plans: Plans that describe how to respond to adversarial attacks, ensuring fairness is maintained. \n\nOperational Change Fairness Logs: Logs that record operational changes and their impact on the fairness of AI system outcomes. | Safety & Performance  Explanation: The detailed deliverables would include an Integrity Assurance Protocol Document, which provides a step-by-step approach to maintaining integrity and a series of Technical Safeguard Specifications, each outlining different countermeasures against specific types of adversarial attacks. A Dynamic Change Ledger would be kept, complete with records of all operational changes and their assessments, accompanied by a version-controlled Repository of System Configurations to track historical and current system states. | Impact Explanation: Integrity Maintenance Protocols: Detailed protocols outlining the measures taken to maintain AI system integrity, including security practices and operational safeguards.  \n\nSystem Monitoring Reports: Reports from continuous monitoring of the AI system, highlighting any detected threats or integrity issues and the responses to them.  \n\nChange Management Records: Documentation of any operational changes made to the AI system, including their impact on system integrity and measures taken to ensure continued security."
    },
    {
        "control_name": "Input Validation & Model Obfuscation",
        "category": "AI Input & Model Security Measures",
        "description": "Implement input validation to detect and prevent input manipulation attacks. Use model obfuscation techniques to prevent membership inference attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 26: Obligations of Deployers of High-Risk AI Systems\nArticle 55: Obligations for Providers of General-Purpose AI Models with Systemic Risk\nRecital 70: Emphasizes anonymization and encryption\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit.\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks., NIST 800-53: SI-10: Information Input Validation\nSA-15: Development Process, Standards, and Tools\nSC-28: Protection of Information at Rest\nSC-38: Operations Security\nSI-3: Malicious Code Protection\nSI-7: Software, Firmware, and Information Integrity\nAC-6: Least Privilege\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSC-13: Cryptographic Protection\nSC-18: Mobile Code\nSC-39: Process Isolation, SCF: CFG-03.2: Prevent Unauthorized Software Execution - Use input validation techniques in AI systems to prevent unauthorized software execution that could lead to manipulation attacks.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles to AI development, emphasizing input validation and model obfuscation for security.\nSEA-09: Information Output Filtering - Implement information output filtering as part of input validation for AI systems to detect and mitigate manipulation attacks.\nSEA-14: Concealment & Misdirection - Employ model obfuscation techniques as part of concealment and misdirection strategies to safeguard AI systems against membership inference attacks.\nTDA-18: Input Data Validation - Specifically focus on input data validation in AI system development to detect and prevent input manipulation attacks.",
        "explainability": "The Input Validation & Model Obfuscation control involves implementing measures to validate and secure input data and obfuscate the model to enhance security and protect against adversarial attacks. The rationale for this control is to ensure the robustness and integrity of the AI system by preventing malicious input and protecting the model's confidentiality.",
        "evidence": "Rationale Explanation: Input Validation Procedures: Development and documentation of procedures outlining the validation steps and measures taken to ensure the integrity and authenticity of input data, preventing malicious inputs and potential adversarial attacks. \n\nModel Obfuscation Techniques: Documentation outlining specific techniques and methods employed for obfuscating the AI model to enhance its security and protect against reverse engineering or tampering. \n\nValidation and Obfuscation Verification: Documentation confirming the successful verification of input validation procedures and model obfuscation techniques through testing and monitoring. | Responsibility Explanation: Input Validation Protocol: A protocol for validating inputs to AI systems. \n\nModel Obfuscation Implementation Plan: A plan for implementing model obfuscation techniques. \n\nSecurity Measures Evaluation Report: A report evaluating the security measures like input validation and model obfuscation. | Data Explanation: Input Validation Plan: Documentation specifying the strategies and techniques for input validation, with an emphasis on data-related aspects. \n\nModel Obfuscation Plan: Documentation outlining the model obfuscation techniques used to prevent membership inference attacks, including data protection considerations. | Fairness Explanation: Input Validation Protocols with Fairness Measures: Protocols that outline the validation of inputs to AI systems, ensuring they are fair and free from manipulation. \n\nModel Obfuscation Standards for Fairness: Standards that specify how models should be obfuscated to prevent inference attacks while maintaining fairness. \n\nInput Manipulation Fairness Incident Reports: Reports on any incidents of input manipulation, with analyses of their impact on fairness. | Safety & Performance  Explanation: Deliverables would include an Input Validation Procedure Guide outlining the methods used to check and sanitize inputs and an Obfuscation Technique Report detailing the specific approaches taken to obscure the AI model's data and operations. Additionally, an Input and Model Security Log would record all validation and obfuscation activities. | Impact Explanation: Input Validation Protocols: Documentation of the methods and protocols used for input validation, detailing how they detect and prevent manipulation attacks.  \n\nModel Obfuscation Techniques: Detailed descriptions of the obfuscation techniques implemented to protect against membership inference attacks, including their design and effectiveness.  \n\nSecurity Testing and Evaluation Reports: Reports from testing the effectiveness of input validation and model obfuscation, including any identified vulnerabilities and subsequent improvements."
    },
    {
        "control_name": "Data Validation & Use of Robust Models",
        "category": "AI Data Integrity Practices",
        "description": "Perform a thorough validation and verification of training data before using it to train the model. Use robust models designed to withstand manipulative attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Recital 67: Emphasizes high-quality data free of errors to the extent possible., NIST 800-53: SA-11: Developer Testing and Evaluation\nRA-3: Risk Assessment\nRA-5: Vulnerability Monitoring and Scanning\nSI-3: Malicious Code Protection\nSI-10: Information Input Validation\nSI-7: Software, Firmware, and Information Integrity\nCM-6: Configuration Settings\nSA-17: Developer Security and Privacy Architecture and Design\nSC-28: Protection of Information at Rest\nSA-4: Acquisition Process\nAC-4: Information Flow Enforcement\nSC-8: Transmission Confidentiality and Integrity, SCF: CFG-03.2: Prevent Unauthorized Software Execution - Implement measures in AI systems to prevent unauthorized data execution, enhancing data validation and model robustness.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles to ensure thorough validation of training data and resilience of AI models against manipulative attacks.\nTDA-06: Secure Coding - Incorporate secure coding practices in AI development to facilitate robust model design and effective data validation.\nTDA-06.2: Threat Modeling - Conduct threat modeling to assess how AI models handle and validate training data, and their resilience against attacks.\nTDA-09.2: Dynamic Code Analysis - Use dynamic code analysis techniques during AI development to validate data and assess the robustness of AI models.",
        "explainability": "The Data Validation & Use of Robust Models control involves implementing processes to validate data inputs and the use of robust models to enhance the reliability and accuracy of AI systems. The rationale for this control is to ensure the integrity of input data, mitigate biases, and enhance the robustness of AI models against adversarial attacks.",
        "evidence": "Rationale Explanation: Data Validation Procedures: Development and documentation of procedures outlining the validation steps and measures taken to ensure the integrity, accuracy, and fairness of input data, mitigating biases and preventing the use of unreliable data. \n\nRobust Model Utilization Guidelines: Documentation providing guidelines on the selection and utilization of robust models, considering factors such as resilience to adversarial attacks, fairness, and accuracy. \n\nValidation and Robust Model Use Verification: Documentation confirming the successful verification of data validation procedures and the implementation of guidelines for using robust models through testing and monitoring. | Responsibility Explanation: Data Validation Procedure: A procedure for validating and verifying training data. \n\nRobust Model Development Guide: A guide for developing robust AI models resistant to attacks. \n\nData and Model Security Training Program: A training program for staff on data validation and robust model development. | Data Explanation: Data Validation Plan: Documentation specifying the processes and criteria for validating training data, including data quality and security considerations.\n \nModel Robustness Assessment: Documentation outlining the measures and techniques used to ensure the model's robustness against manipulative attacks. | Fairness Explanation: Data Validation Protocols: Protocols that outline the process for data validation with fairness metrics. \n\nRobust Model Standards: Documentation of standards for model robustness that includes resistance to bias and manipulation.\n \nTraining Data Fairness Reports: Reports that document the fairness of the training data and the robustness of models. | Safety & Performance  Explanation: Deliverables would include a Data Validation Report, which details the methods and results of the training data verification process and a Robust Model Selection Criteria Document outlining the standards and requirements that models must meet to be considered robust against attacks. A Training Data Integrity Log would also be maintained. | Impact Explanation: Data Validation Reports: Comprehensive reports detailing the processes and outcomes of validating and verifying training data, including any discrepancies and corrective actions taken.  \n\nModel Robustness Documentation: Documentation on the design and features of the models, emphasizing their robustness against manipulative attacks.  \n\nModel Testing and Evaluation Reports: Reports from testing the models for robustness, including results from stress testing and attack simulations."
    },
    {
        "control_name": "Testing & Documentation on Errors & Limitations",
        "category": "AI Error Handling Protocols",
        "description": "Conduct testing, including adversarial and stress testing, to identify errors and limitations in the AI system. Document testing methodology, metrics, and performance outcomes.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 26: Obligations of Deployers of High-Risk AI Systems\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: SA-11: Developer Testing and Evaluation\nSR-6: Supplier Assessments and Reviews\nRA-5: Vulnerability Monitoring and Scanning\nCM-2: Baseline Configuration\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCA-2: Control Assessments\nCA-8: Penetration Testing\nCA-9: Internal System Connections\nCM-9: Configuration Management Plan\nCM-10: Software Usage Restrictions, SCF: CFG-01.1: Assignment of Responsibility - Assign specific responsibilities for conducting AI system testing and documenting errors and limitations.\nSEA-07: Predictable Failure Analysis - Utilize failure analysis techniques to predict and identify potential errors and limitations in AI systems.\nSEA-20: Clock Synchronization - Ensure proper clock synchronization in AI systems to accurately track and document errors and limitations.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Implement comprehensive testing, including adversarial and stress testing, for AI systems to identify errors and limitations.\nTDA-09.4: Malformed Input Testing - Conduct malformed input testing as part of identifying errors and limitations in AI systems.",
        "explainability": "The Testing & Documentation on Errors and Limitations control involves conducting thorough testing to identify errors and limitations in AI systems and documenting these findings. The rationale for this control is to enhance transparency, improve system reliability, and provide stakeholders with a clear understanding of the system's capabilities and constraints.",
        "evidence": "Rationale Explanation: Testing Procedures for Errors and Limitations: Development and documentation of procedures outlining the testing methods and processes used to identify errors and limitations in AI systems, ensuring comprehensive coverage of potential issues. \n\nError and Limitation Documentation: Compilation of documentation that transparently communicates identified errors and limitations in AI systems, including their impact, potential risk, and steps taken for mitigation. \n\nStakeholder Communication Plan: Documentation outlining a plan for communicating identified errors and limitations to relevant stakeholders, including timelines, channels, and responsible parties. | Responsibility Explanation: Testing Methodology Document: A document detailing the testing methodology used for AI systems. \n\nTest Performance Report: A report documenting metrics and outcomes of tests conducted. \n\nAdversarial and Stress Testing Guidelines: Guidelines for conducting adversarial and stress testing. | Data Explanation: Testing Plan: Documentation outlining the testing strategies, including adversarial and stress testing, with a focus on data-related aspects. \n\nTesting Reports: Reports documenting the testing methodology, metrics, and performance outcomes, emphasizing data-related findings. | Fairness Explanation: Testing Methodology Manuals: Manuals that detail the testing methodologies used, including considerations for fairness. \n\nError and Limitation Documentation: Comprehensive documentation of identified errors and limitations, with analyses on their fairness implications. \n\nPerformance Outcome Metrics with Fairness Analysis: Metrics that measure AI performance outcomes, including fairness assessments. | Safety & Performance  Explanation: Deliverables would include a Testing Methodology Guide, which outlines the approach and types of tests conducted, a Testing Metrics and Performance Outcomes Report, detailing the criteria for success and the results of the tests, and an Error and Limitation Documentation, which systematically records any issues uncovered during testing. | Impact Explanation: Testing Procedure Documentation: Documentation outlining the methodologies, tools, and procedures used in testing the AI system, including adversarial and stress tests.  \n\nTesting Outcome Reports: Detailed reports on the outcomes of the tests, highlighting identified errors, limitations, and performance metrics.  \n\nLimitation and Error Analysis Documentation: Comprehensive analysis of the identified limitations and errors, with insights on potential impacts and recommendations for improvements."
    },
    {
        "control_name": "Input Validation for Preventing Attacks",
        "category": "Adversarial Input Mitigation Techniques",
        "description": "Implement input validation to prevent attackers from providing malicious data.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 27: Emphasizes the need for technical robustness in development and ability to detect problems.\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit.\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks.\n, NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-10: Information Input Validation\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAC-7: Unsuccessful Login Attempts\nAC-17: Remote Access\nSC-7: Boundary Protection\nSC-8: Transmission Confidentiality and Integrity\nCM-6: Configuration Settings\nCM-7: Least Functionality\nAC-2: Account Management\nSA-4: Acquisition Process, SCF: CFG-03.2: Prevent Unauthorized Software Execution - Use input validation to prevent unauthorized or malicious data from executing within AI systems.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles to ensure effective input validation mechanisms in AI systems, preventing malicious data input.\nSEA-09: Information Output Filtering - Implement output filtering to validate data inputs and protect AI systems from malicious data attacks.\nTDA-06.4: Supporting Toolchain - Use a toolchain that supports robust input validation, enhancing the AI system's defense against malicious data inputs.\nTDA-18: Input Data Validation - Specifically focus on input data validation techniques in AI system development to prevent malicious data attacks.",
        "explainability": "The Input Validation for Preventing Attacks control involves implementing measures to validate input data, preventing malicious inputs, and enhancing the security of AI systems. The rationale for this control is to ensure the integrity and reliability of input data, mitigating the risk of attacks such as injection attacks and adversarial inputs.",
        "evidence": "Rationale Explanation: Input Validation Procedures: Development and documentation of procedures outlining the validation steps and measures taken to ensure the integrity, authenticity, and security of input data, preventing malicious inputs and potential attacks. \n\nAttack Prevention Documentation: Compilation of documentation that transparently communicates the techniques and methods employed for preventing attacks through input validation, including details on identified attack vectors and countermeasures. \n\nValidation Effectiveness Reports: Documentation confirming the successful verification of input validation procedures and the effectiveness of attack prevention measures through testing and monitoring. | Responsibility Explanation: Input Validation Protocol: A protocol for validating inputs to AI systems. \n\nSecurity Measures Implementation Record: A record of implementing security measures for input validation. \n\nCybersecurity Training on Input Validation: Training materials for staff on implementing and managing input validation. | Data Explanation: Input Validation Plan: Documentation specifying the strategies and techniques for input validation, with an emphasis on data-related aspects. | Fairness Explanation: Input Validation Guidelines with Fairness Measures: Guidelines that outline input validation procedures with fairness checks. \n\nMalicious Data Prevention Protocols: Protocols that describe the mechanisms to prevent the entry of malicious data, maintaining fairness in AI processing. \n\nValidation Process Fairness Logs: Logs that record each instance of input validation and its impact on maintaining fairness. | Safety & Performance  Explanation: Deliverables would include an Input Validation Protocol Document, which describes the technical safeguards and validation checks in place and an Input Validation Log recording all input checks and any detected attempts at malicious data entry. A Security Measures Effectiveness Report may also be provided that summarizes the performance of the input validation system. | Impact Explanation: Input Validation Protocol Documentation: Detailed documentation of the input validation protocols and mechanisms, describing how they prevent the injection of malicious data.  \n\nSecurity Testing Reports: Reports from security testing exercises that evaluate the effectiveness of the input validation mechanisms in detecting and blocking malicious inputs.  \n\nIncident Response and Mitigation Records: Records documenting any incidents of attempted attacks via malicious inputs and the responses to these incidents."
    },
    {
        "control_name": "Physical Adversarial Object Detection",
        "category": "AI Hardware Security",
        "description": "Ensure that machine learning models can detect and handle physical adversarial objects that might mislead them in real-world environments.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nAnnex XII: Transparency Information Referred to in Article 53(1b), NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-7: Software, Firmware, and Information Integrity\nSI-10: Information Input Validation\nCM-2: Baseline Configuration\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nAC-2: Account Management\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAC-7: Unsuccessful Login Attempts\nAC-17: Remote Access\nSC-7: Boundary Protection\nSC-8: Transmission Confidentiality and Integrity\nSC-28: Protection of Information at Rest, SCF: CFG-02.5: Configure Systems, Components or Services for High-Risk Areas - Configure ML models specifically to address the risk of physical adversarial objects in real-world scenarios.\nMON-01.6: Host-Based Devices - Use host-based monitoring tools to continuously evaluate ML model performance in detecting physical adversarial objects.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles to develop ML models capable of detecting physical adversarial objects.\nSEA-04: Process Isolation - Implement process isolation in ML models to enhance their ability to segregate and handle adversarial objects.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Conduct thorough testing of ML models during development to ensure they can identify and handle physical adversarial objects.",
        "explainability": "The Physical Adversarial Object Detection control involves implementing measures to detect physical adversarial objects that may be used to manipulate or deceive AI systems in the physical world. The rationale for this control is to enhance the robustness of AI systems against real-world attacks, ensuring their reliable performance and preventing potential harm.",
        "evidence": "Rationale Explanation: Detection Mechanism Documentation: Development and documentation of procedures outlining the mechanisms and technologies used for detecting physical adversarial objects, including details on the sensors, algorithms, and response protocols. \n\nAdversarial Object Incident Reports: Compilation of documentation that transparently communicates incidents involving physical adversarial objects, including their impact, response actions taken, and measures for improving detection and prevention. \n\nContinuous Improvement Plan: Documentation outlining a plan for continuously improving the detection mechanisms and response protocols based on lessons learned from adversarial object incidents. | Responsibility Explanation: Adversarial Object Detection Guidelines: Guidelines for detecting and handling physical adversarial objects. \n\nModel Testing and Evaluation Report: A report documenting the testing and evaluation of model's ability to detect adversarial objects. \n\nReal-World Testing Protocol: A protocol for testing machine learning models in real-world scenarios. | Data Explanation: Adversarial Object Dataset: A dataset containing examples of physical adversarial objects used for testing. \n\nAdversarial Object Detection Report: A report detailing the results of tests and assessments related to the detection and handling of physical adversarial objects. | Fairness Explanation: Adversarial Object Detection Standards: Standards that outline the detection capabilities models must have to fairly handle physical adversarial objects. \n\nReal-World Fairness Testing Reports: Reports on testing the model's detection capabilities in real-world scenarios, with fairness assessments.\n \nAdversarial Detection Methodology Documentation: Documentation of the methodologies used to test and improve adversarial object detection, emphasizing fairness. | Safety & Performance  Explanation: Deliverables would include a Physical Adversarial Object Detection Strategy, which outlines how the models are trained to detect anomalies and a Detection Test Report summarizing the models' response to various physical adversarial objects during testing. A Model Resilience Documentation, which records the model's robustness to such objects, would also be prepared. | Impact Explanation: Adversarial Detection Methodology Documentation: Documentation describing the methodologies and algorithms used for detecting physical adversarial objects.  \n\nTesting and Evaluation Reports: Reports from testing the machine learning models against various physical adversarial scenarios, detailing the detection accuracy and system responses.  \n\nImprovement and Update Records: Records of ongoing improvements and updates made to the machine learning models to enhance their ability to detect and handle adversarial objects."
    },
    {
        "control_name": "Input Validation for Model Interaction",
        "category": "AI-Specific Patch & Configuration Management",
        "description": "Implement stringent input validation checks when data is fed into the models to prevent malicious inputs or buffer overflow attacks.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 26: Obligations of Deployers of High-Risk AI Systems\nArticle 55: Obligations for Providers of General-Purpose AI Models with Systemic Risk\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit.\nRecital 115: Emphasizes cybersecurity protection related to malicious use or attacks., NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-10: Information Input Validation\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAC-7: Unsuccessful Login Attempts\nAC-17: Remote Access\nSC-7: Boundary Protection\nSC-8: Transmission Confidentiality and Integrity\nCM-6: Configuration Settings\nCM-7: Least Functionality\nAC-2: Account Management\nSA-4: Acquisition Process, SCF: CFG-02: System Hardening Through Baseline Configurations - Use system hardening techniques, including rigorous input validation, for AI models to safeguard against malicious inputs.\nCFG-03.2: Prevent Unauthorized Software Execution - Implement input validation in AI systems to prevent unauthorized or malicious data execution, protecting against buffer overflow attacks.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles to ensure effective input validation for data fed into AI models.\nTDA-18: Input Data Validation - Focus specifically on input data validation in AI system development to prevent malicious data inputs and buffer overflow attacks.",
        "explainability": "The Input Validation for Model Interaction control involves implementing measures to validate inputs during the interaction with an AI model, ensuring the integrity, authenticity, and security of the inputs. The rationale for this control is to prevent potential vulnerabilities and manipulations during the model interaction, enhancing the overall security and reliability of the AI system.",
        "evidence": "Rationale Explanation: Interaction Input Validation Procedures: Development and documentation of procedures outlining the validation steps and measures taken to ensure the integrity, authenticity, and security of inputs during the interaction with the AI model. \n\nSecurity Incident Reports: Compilation of documentation that transparently communicates incidents related to input validation during model interaction, including their impact, response actions taken, and measures for improving validation procedures. \n\nContinuous Enhancement Plan: Documentation outlining a plan for continuously enhancing the input validation procedures based on lessons learned from security incidents during model interaction. | Responsibility Explanation: Model Interaction Validation Procedure: A procedure for validating inputs during model interaction. \n\nSecurity Check Implementation Record: A record of implementing security checks for model interaction. \n\nDeveloper and Security Staff Training on Input Validation: Training materials for developers and security staff on input validation. | Data Explanation: Input Validation Plan: Documentation specifying the strategies and techniques for input validation, emphasizing data-related aspects. \n\nInput Validation Implementation: Documentation outlining the actual implementation of input validation checks in the model interaction process. | Fairness Explanation: Model Interaction Validation Guidelines: Guidelines for validating inputs during model interaction, with a focus on fairness. \n\nManipulation Prevention Protocols: Protocols specifically designed to prevent input manipulation that could lead to unfair outcomes. \n\nModel Interaction Fairness Logs: Logs that record all model interactions, highlighting any input validation issues and their fairness implications. | Safety & Performance  Explanation: Deliverables would include an Input Validation Protocol detailing the criteria and methods for data validation and a Model Interaction Security Report documenting the effectiveness of these checks in preventing unwanted or harmful inputs. Additionally, a System Vulnerability Analysis might be produced, assessing potential weak points in model interactions. | Impact Explanation: Input Validation Procedures: Detailed documentation of the input validation procedures, specifying the checks and balances in place to screen data inputs.  \n\nSecurity Testing Reports: Reports detailing the results of security testing exercises aimed at assessing the effectiveness of the input validation checks.  \n\nIncident and Response Logs: Logs documenting any incidents of malicious inputs or buffer overflow attempts, including the system's response and subsequent actions taken."
    },
    {
        "control_name": "Input Validation for Neural Networks",
        "category": "AI-Specific Patch & Configuration Management",
        "description": "Implement and develop measures to validate and sanitize neural network inputs to prevent misleading and/or manipulated outputs.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 15: Accuracy, robustness and cybersecurity\nArticle 26: Obligations of Deployers of High-Risk AI Systems\nRecital 76: Emphasizes the role of cybersecurity in ensuring AI systems are resilient against exploit., NIST 800-53: SA-11: Developer Testing and Evaluation\nSI-10: Information Input Validation\nAC-3: Access Enforcement\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAC-7: Unsuccessful Login Attempts\nAC-17: Remote Access\nSC-7: Boundary Protection\nSC-8: Transmission Confidentiality and Integrity\nCM-6: Configuration Settings\nCM-7: Least Functionality\nAC-2: Account Management\nSA-4: Acquisition Process, SCF: CFG-03.2: Prevent Unauthorized Software Execution - Ensure input validation in neural networks to prevent unauthorized data execution and protect against crafted inputs.\nMON-01.7: File Integrity Monitoring (FIM) - Monitor the integrity of inputs in neural networks to ensure their validation and sanitation are effective.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles to enforce rigorous input validation in neural networks.\nSEA-09: Information Output Filtering - Implement information output filtering for neural networks as part of input validation to prevent misleading results.\nTDA-18: Input Data Validation - Specifically focus on input data validation in neural network development to prevent misleading outputs from malicious inputs.",
        "explainability": "The Input Validation for Neural Networks control involves implementing measures to validate inputs specifically tailored for neural networks, ensuring the integrity, authenticity, and security of the inputs used in neural network models. The rationale for this control is to prevent potential vulnerabilities and manipulations in the input data, enhancing the overall security and reliability of neural network-based AI systems.",
        "evidence": "Rationale Explanation: Neural Network Input Validation Procedures: Development and documentation of procedures outlining the validation steps and measures taken to ensure the integrity, authenticity, and security of inputs specifically designed for neural networks.\n \nSecurity Incident Reports: Compilation of documentation that transparently communicates incidents related to input validation for neural networks, including their impact, response actions taken, and measures for improving validation procedures. \n\nContinuous Enhancement Plan: Documentation outlining a plan for continuously enhancing the input validation procedures for neural networks based on lessons learned from security incidents. | Responsibility Explanation: Neural Network Input Validation Guidelines: Guidelines for validating and sanitizing inputs for neural networks. \n\nInput Validation Process Documentation: Documentation of the process for input validation in neural networks. \n\n Data Scientist Training on Input Validation: Training materials for data scientists on neural network input validation. | Data Explanation: Input Validation Plan: Documentation specifying the strategies and techniques for input validation, with a focus on data-related aspects for neural network inputs. \n\n Input Validation Implementation: Documentation outlining the actual implementation of input validation and sanitization procedures for neural networks. | Fairness Explanation: Neural Network Input Standards: Standards that define the input validation requirements for neural networks to maintain fairness. \n\nInput Sanitization Protocols: Protocols that describe the process of sanitizing inputs to prevent misleading outputs, with fairness considerations. \n\nMisleading Input Detection Reports: Reports on the detection and handling of crafted inputs that could lead to unfair outcomes. | Safety & Performance  Explanation: Deliverables would include a Neural Network Input Validation Procedure Guide outlining the steps and techniques for input validation and sanitization and a Neural Network Input Sanitization Lo, documenting all instances of input checks and any actions taken to rectify problematic inputs. A Validation Techniques Effectiveness Report might also be prepared to evaluate the success of the input validation methods used. | Impact Explanation: Input Validation Strategy Documentation: Comprehensive documentation outlining the strategies and techniques used for validating and sanitizing inputs to neural networks.  \n\nTesting and Validation Reports: Detailed reports on the testing and validation exercises conducted to assess the effectiveness of input validation methods.  \n\nSanitization Protocol Records: Records of the specific sanitization protocols used, including any updates or changes made to improve input validation."
    },
    {
        "control_name": "Comprehensive Documentation",
        "category": "Documentation for System Design",
        "description": "Maintain comprehensive documentation detailing the development, design, and data of the security measures of the AI systems.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 17: Quality Management System\nArticle 57: AI Regulatory Sandboxes\nRecital 139: Emphasizes supervision of AI systems in the AI regulatory sandbox (i.e., development, training, testing and validation) before a system is placed in service., NIST 800-53: AC-20: Use of External Systems\nAC-23: Data Mining Protection\nAC-24: Access Control Decisions\nCM-3: Configuration Change Control\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nSA-11: Developer Testing and Evaluation\nPL-2: System Security and Privacy Plans\nPL-4: Rules of Behavior\nPL-8: Security and Privacy Architectures\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSC-32: System Partitioning\nSI-12: Cryptographic Key Establishment and Management\nSI-7: Software, Firmware, and Information Integrity, SCF: CFG-01.1: Assignment of Responsibility - Assign responsibility for the creation and maintenance of comprehensive documentation for AI systems.\nIAO-03: System Security & Privacy Plan (SSPP) - Develop a comprehensive System Security and Privacy Plan that includes documentation of AI system development and design.\nSEA-01: Secure Engineering Principles - Apply secure engineering principles, ensuring comprehensive documentation of AI system development and design.\nTDA-04: Documentation Requirements - Maintain detailed documentation for all aspects of AI system development, including design and training data.\nTDA-09: Cybersecurity & Data Privacy Testing Throughout Development - Document all testing and validation activities during the development of AI systems.",
        "explainability": "The Comprehensive Documentation control involves creating thorough documentation for AI systems, covering various aspects such as design, implementation, validation, and security measures. The rationale for this control is to provide a clear and accessible understanding of the AI system, facilitating transparency, accountability, and effective auditing.",
        "evidence": "Rationale Explanation: AI System Documentation: Development and maintenance of comprehensive documentation covering the design, architecture, implementation, data sources, validation procedures, security measures, and other relevant aspects of the AI system. \n\nAudit Trail Reports: Compilation of reports documenting the history of changes, updates, and activities related to the AI system, providing an audit trail for accountability and transparency. \n\nDocumentation Review Checklist: A checklist outlining the criteria for comprehensive documentation, serving as a guide for developers and auditors to ensure completeness and accuracy. | Responsibility Explanation: AI System Documentation Protocol: A protocol for documenting the development, design, and training data of AI systems. \n\nDevelopment and Design Documentation Record: A record of all documentation related to AI system development and design (e.g., model cards, datasheets of each AI dataset, transparency notes). \n\nTraining Data Documentation Guide: A guide for documenting the use and validation of training data.\n | Data Explanation: Documentation Guidelines: Documentation specifying the guidelines and standards for comprehensive documentation, emphasizing data-related aspects. \n\nComprehensive Documentation Records: Actual documentation records that detail the development, design, and training data of AI systems. | Fairness Explanation: Development and Design Documentation: Documents that detail the development and design processes of AI systems with an emphasis on fairness. \n\nTraining Data Documentation: Records of the training data used, including measures taken to ensure it is unbiased. \n\nSystem Fairness Overviews: Overviews that explain how fairness was integrated into the AI system at each stage of development. | Safety & Performance  Explanation: Deliverables would include a Development Process Documentation capturing all phases of the AI system's creation, a Design Specification Document detailing the system's architectural and operational design, and a Training Data Report documenting the data sources, processing methods, and usage in training the AI system. A Change Management Log, recording any updates or modifications, would also be key. | Impact Explanation: Development Process Documentation: Detailed records of the development process, including design decisions, methodologies, and development stages.  \n\nDesign Specifications: Complete documentation of the AI system's design specifications, outlining architectural choices, algorithms used, and system functionalities.  \n\nTraining Data Documentation: Comprehensive records of the training data used, including sources, data processing methods, and data quality assessments."
    },
    {
        "control_name": "AI Risk Awareness Training",
        "category": "Awareness & Role-Based Training Plans",
        "description": "Implement a comprehensive training program for natural persons overseeing high-risk AI systems. This program should explicitly cover potential risk, including automation bias (overreliance on AI decisions) and confirmation bias (favoring information that confirms preexisting beliefs) and needed skills to identify and assess risk associated with AI systems. The training should also provide strategies to recognize and mitigate these biases in real-time operations.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 9: Risk Management System, NIST 800-53: AT-1: Security Awareness and Training Policy and Procedures\nAT-2: Literacy Training and Awareness\nAT-3: Role-Based Training\nAT-4: Training Records\nPM-15: Security and Privacy Groups and Associations Responsibilities\nSA-11: Developer Testing and Evaluation, SCF: HRS-03.1: User Awareness - Ensure that all users, especially those overseeing AI systems, are aware of the inherent risk and biases associated with AI decision-making.\nHRS-03.2: Competency Requirements for Security-Related Positions - Establish competency requirements that include understanding and mitigating biases in AI systems for roles responsible for overseeing these systems.\nSAT-03: Role-Based Cybersecurity & Data Privacy Training - Implement role-based training focusing on AI risk, biases, and mitigation strategies for those overseeing high-risk AI systems.\nSAT-03.2: Suspicious Communications & Anomalous System Behavior - Include training on recognizing and responding to automation and confirmation biases as part of managing suspicious communications and anomalous behaviors in AI systems.\nSAT-03.5: Privileged Users - Provide specialized training for privileged users on the risk associated with high-risk AI systems, including awareness of biases.",
        "explainability": "The AI Risk Awareness Training control involves providing training programs to enhance awareness and understanding of AI-related risk among relevant stakeholders. The rationale for this control is to empower individuals with the knowledge and skills needed to identify, assess, and mitigate risk associated with AI systems.",
        "evidence": "Rationale Explanation: Training Modules: Development and delivery of comprehensive training modules covering various aspects of AI-related risk, including ethical considerations, security implications, potential biases, and the impact on individuals and society. \n\nTraining Attendance Records: Compilation of records documenting the attendance of individuals in AI Risk Awareness Training sessions, ensuring accountability and tracking the effectiveness of the training program. \n\nFeedback and Improvement Plan: Documentation outlining a plan for collecting feedback from participants, assessing the effectiveness of the training, and continuously improving the content and delivery methods. | Responsibility Explanation: AI Risk Awareness Training Program: A comprehensive program covering potential AI risk and bias mitigation strategies. \n\nTraining Content and Curriculum: Detailed content and curriculum focusing on AI risk, biases, and mitigation strategies. \n\nTraining Effectiveness Evaluation Reports: Reports assessing the effectiveness of the AI risk awareness training program. | Data Explanation: Training Program Content: Documentation outlining the content of the comprehensive AI risk awareness training program, including data-related aspects related to bias recognition and mitigation. \n\nTraining Records: Records of training sessions attended by natural persons overseeing high-risk AI systems. | Fairness Explanation: Training Program Curricula: Curricula that outline the content of risk awareness training programs, focusing on fairness. \n\nBias Mitigation Strategies: Strategies taught to employees for recognizing and mitigating biases during AI operations. \n\nTraining Participation Logs: Logs that record who has completed the training, ensuring all relevant personnel are trained in fairness. | Safety & Performance  Explanation: Deliverables would include a Comprehensive AI Risk Awareness Training Curriculum detailing the educational content and methodologies for addressing biases and a Training Completion Report for each participant documenting their participation and understanding of the material. Additionally, a Bias Mitigation Guide would be provided as a reference tool. | Impact Explanation: Training Program Curriculum: Detailed curriculum of the training program, covering topics on AI risk, particularly automation and confirmation biases.  \n\nTraining Completion Records: Records of individuals who have completed the training, including their assessments and feedback.  \n\nBias Mitigation Strategies Documentation: Documentation of the strategies and practices taught to recognize and mitigate biases in AI operations."
    },
    {
        "control_name": "Documentation of User Education",
        "category": "Awareness & Role-Based Training Plans",
        "description": "Implement the maintenance of records and evidence that demonstrates that individuals were educated on their rights, knowledge, and use of high-risk AI systems. This includes rights and knowledge in relation to responsible interaction, safety with the AI system, fostering transparency and informed decision making.",
        "risk_level": "High Risk",
        "framework": "EU AI Law: Article 11: Technical Documentation, NIST 800-53: AT-4: Training Records\nAC-4: Information Flow Enforcement\nAC-6: Least Privilege\nAC-19: Access Control for Mobile Devices\nAC-22: Publicly Accessible Content\nCM-3: Configuration Change Control\nCM-6: Configuration Settings\nCM-7: Least Functionality\nCM-8: System Component Inventory\nCM-9: Configuration Management Plan\nSA-11: Developer Testing and Evaluation\nPM-9: Risk Management Strategy\nPL-4: Rules of Behavior\nPL-8: Security and Privacy Architectures\nPL-9: Central Management\nSC-4: Information in Storage\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSC-32: System Partitioning\nSI-2: Flaw Remediation\nSI-7: Software, Firmware, and Information Integrity\nSI-11: Error Handling, SCF: CFG-01.1: Assignment of Responsibility - Assign responsibility for ensuring that documentation of user education on high-risk AI systems is maintained accurately and comprehensively.\nHRS-03.1: User Awareness - Document the processes and methods used to educate users about high-risk AI systems, focusing on their rights and potential impacts.\nIAO-02.4: Security Assessment Report (SAR) - Include user education documentation in the Security Assessment Report, highlighting education on AI system use and user rights.\nSAT-03: Role-Based Cybersecurity & Data Privacy Training - Implement and document role-based training that includes education on the use of high-risk AI systems and the rights of users.\nSAT-04: Cybersecurity & Data Privacy Training Records - Maintain comprehensive records of all training provided on the use and impacts of high-risk AI systems, including user education on their rights.",
        "explainability": "The Documentation of User Education control involves creating comprehensive documentation to educate users about the AI system, its capabilities, limitations, and potential risk. The rationale for this control is to empower users with the knowledge needed to interact responsibly and safely with the AI system, fostering transparency and informed decision making.",
        "evidence": "Rationale Explanation: User Education Manuals: Development of user-friendly manuals that provide detailed information on how to use the AI system and its features and guidelines for responsible and safe interaction. \n\nOnline Tutorials and FAQs: Creation of online tutorials and frequently asked questions (FAQs) to supplement user education, offering additional guidance and support for common queries and challenges. \n\nDocumentation Accessibility Plan: Documentation outlining a plan for ensuring the accessibility of user education materials, considering diverse user needs, such as language preferences and accessibility requirements. | Responsibility Explanation: User Education Documentation System: A system for documenting the education provided to users of high-risk AI systems. \n\nUser Rights and Impact Information Material: Information materials detailing users' rights and the potential impacts of AI systems. \n\nEducation Record Compliance Audit Reports: Reports auditing compliance with user education documentation requirements. | Data Explanation: User Education Records: Documentation and records of the user education process, including information about informing individuals and educating them about AI systems, their rights, and potential impacts. | Fairness Explanation: User Education Records: Records that document the education provided to users about their rights and the AI system. \n\nRights Explanation Documents: Documents that explain users' rights in relation to AI systems, with a focus on fairness. \n\nImpact Education Summaries: Summaries that outline the potential impacts of AI systems and how users can seek explanations. | Safety & Performance  Explanation: Deliverables would include User Education Records, which document the content and delivery of user education sessions and an Education Effectiveness Report evaluating how well the education sessions conveyed critical information about users' rights and AI impacts. A User Understanding Assessment Log might also be maintained that records individual users' comprehension and feedback. | Impact Explanation: User Education Records: Detailed records of the user education sessions, including content, dates, and attendance.  \n\nEducational Material Documentation: Copies of the educational materials provided to users, covering information about high-risk AI systems, user rights, and potential impacts.  \n\nUser Feedback and Understanding Assessments: Documentation of feedback from users on the education they received and assessments to gauge their understanding of the information provided."
    },
    {
        "control_name": "AI Data Protection & Intellectual Property Training",
        "category": "Awareness & Role-Based Training Plans",
        "description": "Implement data protection measures, provide continuous training on intellectual property (IP) rights, and embed legal safeguards in contracts to protect trade secrets.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: AT-4: Training Records\nAC-4: Information Flow Enforcement\nAC-8: System Use Notification\nAC-10: Concurrent Session Control\nCM-7: Least Functionality\nPM-7: Developer Testing and Evaluation\nPM-9: Risk Management Strategy\nPL-4: Rules of Behavior\nPL-8: Security and Privacy Architectures\nPL-9: Security Compliance Auditing\nSA-8: Security Training\nSC-4: Information in Storage\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSC-32: System Partitioning\nSI-7: Software, Firmware, and Information Integrity\nSI-11: Error Handling\nAU-6: Audit Record Review, Analysis, and Reporting\nAU-12: Audit Record Generation, SCF: CPL-02: Cybersecurity & Data Privacy Controls Oversight - Oversee the implementation and training of data protection and intellectual property safeguards in AI development.\nPRI-01.6: Security of Personal Data - Include training on the security of personal data as part of AI data protection and intellectual property training programs.\nSAT-03: Role-Based Cybersecurity & Data Privacy Training - Provide comprehensive training for employees on data protection and intellectual property rights in the context of AI systems.\nSAT-03.4: Vendor Cybersecurity & Data Privacy Training - Extend training programs to include vendors, focusing on AI data protection and the protection of intellectual property.\nTDA-03.1: Supplier Diversity - Ensure suppliers and partners are trained and compliant with data protection and intellectual property rights regarding AI technologies.",
        "explainability": "The AI Data Protection & Training control involves providing training programs focused on educating relevant stakeholders about data protection and IP considerations in the context of AI. The rationale for this control is to enhance awareness, compliance, and responsible handling of data and IP in AI development and deployment.",
        "evidence": "Rationale Explanation: Training Modules: Development and delivery of comprehensive training modules covering key aspects of AI data protection and IP, including legal frameworks, ethical considerations, and best practices for handling sensitive information. \n\nTraining Attendance Records: Compilation of records documenting the attendance of individuals in AI Data Protection & Training sessions, ensuring accountability and tracking the effectiveness of the training program. \n\nLegal Compliance Handbook: Documentation providing an overview of legal requirements related to data protection and IP in AI, offering practical guidance and resources for stakeholders. | Responsibility Explanation: Data Protection and IP Training Program: A training program on data protection and  IP rights in AI. \n\nLegal Safeguard Guidelines: Guidelines embedding legal safeguards in contracts for trade secret protection. \n\nData Protection Policy and Procedure Documents: Documents outlining data protection policies and procedures in AI usage. | Data Explanation: Data Protection Measures Documentation: Documentation outlining the data protection measures in place, emphasizing data-related aspects. \n\nIP Training Records: Records of continuous training sessions on IP rights. \n\nContractual Safeguards: Documentation outlining legal safeguards embedded in contracts for trade secret protection. | Fairness Explanation: Data Protection Training Modules: Training modules that focus on data protection in AI, including fairness in data handling. \n\nIP Rights Manuals: Manuals that provide continuous training on IP rights related to AI, with fairness considerations. \n\nLegal Safeguard Documentation: Documentation of legal safeguards in contracts to protect trade secrets and ensure fairness. | Safety & Performance  Explanation: Deliverables would include a Data Protection Policy outlining the measures taken to secure data and an IP Rights Training Manual detailing the training provided to staff on IP matters. A Legal Safeguards Documentation would also be prepared, capturing the contractual clauses protecting trade secrets and proprietary information. | Impact Explanation: Data Protection and IP Training Program Materials: Comprehensive materials and curriculum for training programs focused on data protection and IP rights.  \n\nTraining Completion and Compliance Records: Records of personnel who have completed the training, including their levels of compliance and understanding.\n \nLegal Safeguard Documentation: Documentation of legal safeguards embedded in contracts, focusing on the protection of trade secrets and compliance with data protection laws."
    },
    {
        "control_name": "AI Talent Development & Retention Programs",
        "category": "AI Workforce Development",
        "description": "Implement initiatives to cultivate AI skills within the organization, ensuring employees are equipped to work with AI technologies and strategies to retain top AI talent.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 13: Transparency and Provision of Information to Deployers, NIST 800-53: AT-1: Awareness and Training Policy and Procedures\nAT-2: Literacy Training and Awareness\nAT-3: Role-Based Training\nAT-4: Training Records\nPM-15: Security and Privacy Groups and Associations\nRA-11: Developer Testing and Evaluation\nPM-9: Risk Management Strategy\nPL-4: Rules of Behavior\nPL-8: Security and Privacy Architectures\nSC-28: Protection of Information at Rest\nSI-2: Flaw Remediation\nSI-7: Software, Firmware, and Information Integrity\nSI-11: Error Handling, SCF: HRS-05.4: Use of Critical Technologies - Include AI technologies as a critical area for employee development and retention strategies.\nHRS-13: Identify Critical Skills & Gaps - Identify AI skills and competencies needed within the organization and develop initiatives for skill cultivation.\nHRS-13.1: Remediate Identified Skills Deficiencies - Address AI skill gaps through targeted development programs and training initiatives.\nPRM-08: Manage Organizational Knowledge - Focus on AI talent as a key aspect of organizational knowledge management, implementing strategies for development and retention.\nSAT-03: Role-Based Cybersecurity & Data Privacy Training - Provide AI-focused training as part of the role-based training programs to enhance AI skills among employees.",
        "explainability": "The AI Talent Development & Retention Programs control involves implementing initiatives to attract, develop, and retain skilled professionals in the field of AI. The rationale for this control is to build a robust and capable workforce, fostering innovation and ensuring the long-term success of AI projects.",
        "evidence": "Rationale Explanation: Talent Development Plans: Creation of structured plans outlining the development pathways for AI professionals, including training programs, mentorship opportunities, and career progression tracks. \n\nRetention Incentive Programs: Implementation of programs offering incentives to retain top AI talent, such as competitive compensation, career advancement opportunities, and recognition for outstanding contributions. \n\nSkills and Competency Assessments: Conducting regular assessments to evaluate the skills and competencies of AI professionals, identifying areas for improvement and tailoring development programs accordingly. | Responsibility Explanation: AI Talent Development Program: A program for developing AI skills within the organization. \n\nTalent Retention Strategy Document: A document outlining strategies for retaining top AI talent. \n\nSkill Development and Retention Effectiveness Reports: Reports evaluating the effectiveness of AI talent development and retention programs. | Data Explanation: AI Talent Development Program: Documentation outlining the initiatives and strategies for developing AI skills within the organization, including data-related aspects. \n\nTalent Retention Strategies: Documentation outlining strategies for retaining top AI talent, with an emphasis on data-related considerations. | Fairness Explanation: Talent Development Strategies: Strategies outlining how the organization develops and retains AI talent, with a focus on fairness in AI operations. \n\nRetention Program Records: Records of retention programs aimed at keeping AI talent, noting how these programs support fairness. \n\nAI Skill Cultivation Documentation: Documentation on initiatives to cultivate AI skills within the organization, ensuring employees can contribute to fair AI operations. | Safety & Performance  Explanation: Deliverables would include an AI Talent Development Plan outlining the training and skill development initiatives and an AI Talent Retention Strategy detailing the approaches used to keep highly skilled AI professionals. Additionally, a Skills and Retention Progress Report could be generated, tracking the effectiveness of these programs. | Impact Explanation: Talent Development Program Outline: Detailed outline of the AI talent development programs, including training modules, skill development pathways, and career progression plans. \n \nEmployee Skill Assessment Records: Records of employee skill assessments before and after participation in the talent development programs.  \n\nRetention Strategy Documentation: Documentation of the strategies and initiatives implemented to retain top AI talent, including incentives, career development opportunities, and work environment enhancements."
    },
    {
        "control_name": "Differential Privacy Implementation",
        "category": "Privacy Enhancement Techniques",
        "description": "Implement differential privacy to protect the privacy of individual records in the AI system's dataset and prevent the transfer of malicious knowledge from the attacker's model to the target model.",
        "risk_level": "General",
        "framework": "EU AI Law: Article 10: Data and data governance\nArticle 15: Accuracy, robustness and cybersecurity\nRecital 70: Emphasizes the right to privacy and protection of personal data.\nArticle 78: Confidentiality\nAnnex III: High-Risk AI Systems, NIST 800-53: AC-16: Security and Privacy Attributes\nAC-22: Publicly Accessible Content\nAC-23: Data Mining Protection\nCM-3: Configuration Change Control\nCM-4: Impact Analysis\nCM-9: Configuration Management Plan\nSA-11: Developer Testing and Evaluation\nPL-4: Rules of Behavior\nPL-8: Security and Privacy Architectures\nSC-4: Information in Shared System Resources\nSC-7: Boundary Protection\nSC-28: Protection of Information at Rest\nSC-32: System Partitioning\nSI-2: Flaw Remediation\nSI-11: Error Handling\nSC-12: Cryptographic Key Establishment and Management\nPT-1: PII Processing and Transparency Policy and Procedures\nPT-7: Specific Categories of Personally Identifiable Information\nPM-21 Accounting of Disclosures\nPM-22: Personally Identifiable Information Quality Management, SCF: DCH-01.2: Sensitive / Regulated Data Protection - Apply differential privacy measures as part of protecting sensitive and regulated data used in AI models.\nDCH-06.4: Making Sensitive Data Unreadable In Storage - Utilize differential privacy strategies to render sensitive data unreadable, especially in AI training datasets.\nDCH-23.6: Differential Data Privacy - Implement differential privacy techniques in AI systems to enhance data privacy and security.\nDCH-23.7: Automated De-Identification of Sensitive Data - Automate the process of de-identifying sensitive data using differential privacy methods in AI systems.\nPRI-01.6: Security of Personal Data - Ensure the security of personal data in AI systems by implementing differential privacy techniques.",
        "explainability": "The Differential Privacy Implementation control involves integrating differential privacy measures into the design and implementation of AI systems. The rationale for this control is to enhance privacy protection by minimizing the disclosure of sensitive information while still allowing for meaningful analysis and insights.",
        "evidence": "Rationale Explanation: Differential Privacy Policy: Development of a comprehensive policy outlining the principles, algorithms, and implementation strategies for achieving differential privacy in AI systems. \n\nPrivacy Impact Assessments: Conducting privacy impact assessments to identify potential privacy risk and ensuring that the implementation of differential privacy measures aligns with established policies and regulatory requirements. \n\nTechnical Documentation: Creation of technical documentation detailing the integration of differential privacy mechanisms, algorithms used, and safeguards in place to protect user privacy. | Responsibility Explanation: Differential Privacy Implementation Plan: A plan for implementing differential privacy in AI systems. \n\nPrivacy Protection Protocol: A protocol for protecting privacy in training datasets. \n\nPrivacy Impact Assessment Reports: Reports assessing the impact of differential privacy implementation. | Data Explanation: Differential Privacy Implementation Plan: Documentation specifying the strategies and techniques for implementing differential privacy, with a focus on data-related aspects. \n\nPrivacy Impact Assessment: Documentation assessing the impact of differential privacy on data privacy and protection. | Fairness Explanation: Differential Privacy Implementation Plans: Plans that outline the application of differential privacy techniques to protect individual data. \n\nPrivacy Protection Fairness Metrics: Metrics used to evaluate the fairness of privacy protections provided by differential privacy. \n\nKnowledge Transfer Prevention Strategies: Strategies to prevent malicious knowledge transfer, ensuring they are fair and do not compromise individual privacy. | Safety & Performance  Explanation: Deliverables would include a Differential Privacy Framework Document outlining the specific methods and algorithms used for implementing differential privacy and a Data Privacy Impact Report assessing the effectiveness of these measures in protecting individual data privacy. An Attack Transfer Prevention Analysis might also be produced to evaluate the system's resistance to knowledge transfer attacks. | Impact Explanation: Differential Privacy Implementation Plans: Detailed plans outlining the implementation of differential privacy techniques, including algorithms used and data handling protocols.  \n\nPrivacy Protection Effectiveness Reports: Reports assessing the effectiveness of differential privacy measures in protecting individual data records.  \n\nSecurity Incident Logs: Logs documenting any security incidents related to data privacy, including potential attempts at malicious knowledge transfer."
    }
]